@ROUT ATL_dgemvT_sse3
#include "atlas_asm.h"
#ifndef ATL_GAS_x8664
   #error "This kernel requires x86-64 assembly!"
#endif
/*
 *                      %rdi        %rsi           %rdx          %rcx
 * void ATL_UGEMV(ATL_CINT M, ATL_CINT N, const TYPE *A, ATL_CINT lda,
 *                          %r8      %r9
 *                const TYPE *X, TYPE *Y)
 */

@NU 1
#define M %rdi
#define N %rsi
#define pA0 %rdx
#define lda %rcx
#define pX %r8
#define pY %r9
@NU !

@ROUT ATL_dgemvkT_sse3_4x1 ATL_dgemvkT_sse3_14x1 ATL_dgemvkT_sse3_10x1
#include "atlas_asm.h"
@ROUT ATL_dgemvkT_sse3_14x1
/*
 * This kernel assumes (lda%2 = 0) && X%16 == A%16
 */
#define II      %rdi
#define JJ      %rsi
#define pA5     %rdx
#define lda     %rcx
#define pX      %rbx
#define pY      %rbp
#define Mr      %rax  /* 0 or 1 of iteration left at end of vector loop? */
#define mlda    %r8 
#define mlda5   %r9
#define lda3    %r10
#define mlda3   %r11
#define lda5    %r12
#define II0     %r13
#define lda7    %r14
#define pAn     %r15

#define rX0     %xmm0
#define rA0     %xmm1
#define rY0     %xmm2
#define rY1     %xmm3
#define rY2     %xmm4
#define rY3     %xmm5
#define rY4     %xmm6
#define rY5     %xmm7
#define rY6     %xmm8
#define rY7     %xmm9
#define rY8     %xmm10
#define rY9     %xmm11
#define rY10    %xmm12
#define rY11    %xmm13
#define rY12    %xmm14
#define rY13    %xmm15

#define MOVAPD  movaps
#define MOVSD movsd
#ifdef BETAX
   #define BETAOFF -72
#endif
@ROUT ATL_dgemvkT_sse3_4x1 ATL_dgemvkT_sse3_14x1 ATL_dgemvkT_sse3_10x1
/*
 * Rename for backward compatibility
 */
#ifndef ATL_UGEMVT
   #if BETA0
      #define ATL_UGEMVT ATL_dgemvkT_b0
   #elif defined(BETAX)
      #define ATL_UGEMVT ATL_dgemvkT_bX
   #else
      #ifndef BETA1
         #define BETA1
      #endif
      #define ATL_UGEMVT ATL_dgemvkT_b1
   #endif
#endif
@ROUT ATL_dgemvkT_sse3_14x1
/*                      rdi,         rsi,             xmm0
void ATL_UGEMVT(const int M, const int N, const float alpha,
                           rdx            rcx              r8              r9
                const float *A, const int lda, const float *X, const int incX,
                           %xmm1     8(rsp)        16(rsp)
                const float beta, float *Y, const int incY)
*/
        .text
.global ATL_asmdecor(ATL_UGEMVT)
ATL_asmdecor(ATL_UGEMVT):
/*
 * Swap M & N for backwards compatibility
 */
   xchg %rdi, %rsi
/*
 * Save callee-saved registers, and load in-memory parameters to registers
 */
   movq %rbx, -8(%rsp)
   movq %rbp, -16(%rsp)
   movq %r12, -24(%rsp)
   movq %r13, -32(%rsp)
   movq %r14, -40(%rsp)
   movq %r15, -48(%rsp)

   movq 8(%rsp), pY             /* pY = Y */
   mov  %r8, pX
#ifdef BETAX
   unpcklpd  %xmm1, %xmm1       /* xmm1 = {beta, beta} */
   movapd %xmm1, BETAOFF(%rsp)  /* save beta for later use */
#endif
/*
 *  Compute the number of remainder iterations after vector loop complete
 */
    xor Mr, Mr                  /* Mr = 0 */
    mov $-1, lda3               /* lda = -1 */
    mov pX, II0                 /* II0 = X */
    lea (pX, II, 8), pX         /* pX pts to end of X */
    bt  $3, pX                  /* CF=1 if pX not aligned */
    cmovc lda3, Mr              /* Mr = (X ends aligned?) 0 : -1 */
    lea (pX, Mr, 8), pX         /* pts to end of aligned space */
    sub pX, II0                 /* II0 = -na*sizeof, na= aligned space elts */
/*
 * Compute derived values
 */
   shl $3, lda                  /* lda *= sizeof */
   lea  (lda,lda,4), mlda5      /* mlda5 = 5*lda */
   mov  mlda5, lda5
   add  mlda5, pA5              /* pA5 = pA0 + 5*lda */
   neg  mlda5                   /* mlda5 = -5*lda */
   mov  lda, mlda               /* mlda = lda */
   lea  (mlda5,lda,2), mlda3    /* mlda3 = -5*lda+2*lda = -3*lda */
   neg  mlda                    /* mlda = -lda */
   lea  (lda,lda,2), lda3       /* lda3 = 3*lda */
   lea  (lda3,lda,4), lda7      /* lda7 = 3*lda+4*lda = 7*lda */
   
   shl  $3, JJ                  /* JJ = N * sizeof */
   lea  (pY, JJ), pY            /* Y += N */
   neg  JJ                      /* JJ = -N*sizeof */

   NLOOP:
      lea (pA5, lda7,2), pAn
      mov       II0, II
/*
 *    First iteration peeled to handle misalignment and zeroing of Y regs
 */
/*
 *    If A is not 16-byte aligned, peel one iteration to force 16-byte alignment
 */
      bt $3, pA5
      jc  PEEL_SCALAR
/*
 *    This code entered when A already 16-byte aligned, peels 1st 2 scalar iters
 */
      MOVAPD (pX,II), rY0
      MOVAPD rY0, rY1
      mulpd  (pA5,mlda5), rY0
      MOVAPD rY1, rY2
      mulpd  (pA5,mlda,4), rY1
      MOVAPD rY2, rY3
      mulpd  (pA5,mlda3), rY2
      MOVAPD rY3, rY4
      mulpd  (pA5,mlda,2), rY3
      MOVAPD rY4, rY5
      mulpd  (pA5,mlda), rY4
      MOVAPD rY5, rY6
      mulpd  (pA5), rY5
      MOVAPD rY6, rY7
      mulpd  (pA5,lda), rY6
      MOVAPD rY7, rY8
      mulpd  (pA5,lda,2), rY7
      MOVAPD rY8, rY9
      mulpd  (pA5,lda3), rY8
      MOVAPD rY9, rY10
      mulpd  (pA5,lda,4), rY9
      MOVAPD rY10, rY11
      mulpd  (pA5,lda5), rY10
      MOVAPD rY11, rY12
      mulpd  (pA5,lda3,2), rY11
      MOVAPD rY12, rY13
      mulpd  (pA5,lda7), rY12
      mulpd  (pA5,lda,8), rY13
      add    $16, pA5
      MOVAPD 16(pX,II), rX0
      add    $32, II
      MLOOP:
         MOVAPD  (pA5,mlda5), rA0
         mulpd   rX0, rA0
         addpd   rA0, rY0
         MOVAPD  (pA5,mlda,4), rA0
         mulpd   rX0, rA0
         addpd   rA0, rY1
         MOVAPD  (pA5,mlda3), rA0
         mulpd   rX0, rA0
         addpd   rA0, rY2
         MOVAPD  (pA5,mlda,2), rA0
         mulpd   rX0, rA0
         addpd   rA0, rY3
         MOVAPD  (pA5,mlda), rA0
         mulpd   rX0, rA0
         addpd   rA0, rY4
         MOVAPD  (pA5), rA0
         mulpd   rX0, rA0
         addpd   rA0, rY5
         MOVAPD  (pA5,lda), rA0
         mulpd   rX0, rA0
         addpd   rA0, rY6
         MOVAPD  (pA5,lda,2), rA0
         mulpd   rX0, rA0
         addpd   rA0, rY7
         MOVAPD  (pA5,lda3), rA0
         mulpd   rX0, rA0
         addpd   rA0, rY8
         MOVAPD  (pA5,lda,4), rA0
         mulpd   rX0, rA0
         addpd   rA0, rY9
         MOVAPD  (pA5,lda5), rA0
         mulpd   rX0, rA0
         addpd   rA0, rY10
         MOVAPD  (pA5,lda3,2), rA0
         mulpd   rX0, rA0
         addpd   rA0, rY11
         MOVAPD  (pA5,lda7), rA0
         mulpd   rX0, rA0
         addpd   rA0, rY12
         MOVAPD  (pA5,lda,8), rA0
         mulpd   rX0, rA0
         addpd   rA0, rY13
         add    $16, pA5          /* pA5 += 16 */
         MOVAPD (pX,II), rX0
      add  $16,   II              /* II += 16 */
      jnz MLOOP
/*
 *    Last iteration peeled, to stop preload of rX0
 */
      MOVAPD  (pA5,mlda5), rA0
      mulpd   rX0, rA0
      addpd   rA0, rY0
      MOVAPD  (pA5,mlda,4), rA0
      mulpd   rX0, rA0
      addpd   rA0, rY1
      MOVAPD  (pA5,mlda3), rA0
      mulpd   rX0, rA0
      addpd   rA0, rY2
      MOVAPD  (pA5,mlda,2), rA0
      mulpd   rX0, rA0
      addpd   rA0, rY3
      MOVAPD  (pA5,mlda), rA0
      mulpd   rX0, rA0
      addpd   rA0, rY4
      MOVAPD  (pA5), rA0
      mulpd   rX0, rA0
      addpd   rA0, rY5
      MOVAPD  (pA5,lda), rA0
      mulpd   rX0, rA0
      addpd   rA0, rY6
      MOVAPD  (pA5,lda,2), rA0
      mulpd   rX0, rA0
      addpd   rA0, rY7
      MOVAPD  (pA5,lda3), rA0
      mulpd   rX0, rA0
      addpd   rA0, rY8
      MOVAPD  (pA5,lda,4), rA0
      mulpd   rX0, rA0
      addpd   rA0, rY9
      MOVAPD  (pA5,lda5), rA0
      mulpd   rX0, rA0
      addpd   rA0, rY10
      MOVAPD  (pA5,lda3,2), rA0
      mulpd   rX0, rA0
      addpd   rA0, rY11
      MOVAPD  (pA5,lda7), rA0
      mulpd   rX0, rA0
      addpd   rA0, rY12
      MOVAPD  (pA5,lda,8), rA0
      mulpd   rX0, rA0
      addpd   rA0, rY13
/*
 *    Handle N=1 cleanup if necessary
 */
      bt $0, Mr
      jc SCALAR_LAST_ITER
YSUM:
/*
 *    Sum up and apply Y/BETA
 */
      #ifdef BETAX
         MOVAPD BETAOFF(%rsp), rX0
         MOVAPD (pY,JJ), rA0
         mulpd  rX0, rA0
      #endif
      haddpd  rY1, rY0          /* rY0 = {Y1ab, Y0ab} */
      #ifdef BETA1
         addpd (pY,JJ), rY0
      #elif BETAX
         addpd rA0, rY0
         MOVAPD 16(pY,JJ), rA0
         mulpd  rX0, rA0
      #endif
      haddpd  rY3, rY2          /* rY2 = {Y3ab, Y2ab} */
      #ifdef BETA1
         addpd 16(pY,JJ), rY2
      #elif BETAX
         addpd rA0, rY2
         MOVAPD 32(pY,JJ), rA0
         mulpd  rX0, rA0
      #endif
      haddpd  rY5, rY4          /* rY4 = {Y5ab, Y4ab} */
      #ifdef BETA1
         addpd 32(pY,JJ), rY4
      #elif BETAX
         addpd rA0, rY4
         MOVAPD 48(pY,JJ), rA0
         mulpd  rX0, rA0
      #endif
      haddpd  rY7, rY6          /* rY6 = {Y7ab, Y6ab} */
      #ifdef BETA1
         addpd 48(pY,JJ), rY6
      #elif BETAX
         addpd rA0, rY6
         MOVAPD 64(pY,JJ), rA0
         mulpd  rX0, rA0
      #endif
      haddpd  rY9, rY8          /* rY8 = {Y9ab, Y8ab} */
      #ifdef BETA1
         addpd 64(pY,JJ), rY8
      #elif BETAX
         addpd rA0, rY8
         MOVAPD 80(pY,JJ), rA0
         mulpd  rX0, rA0
      #endif
      haddpd  rY11,rY10         /* rY10= {Y11ab, Y10ab} */
      #ifdef BETA1
         addpd 80(pY,JJ), rY10
      #elif BETAX
         addpd rA0, rY10
         MOVAPD 96(pY,JJ), rA0
         mulpd  rX0, rA0
      #endif
      haddpd  rY13,rY12         /* rY12= {Y13ab, Y12ab} */
      #ifdef BETA1
         addpd 96(pY,JJ), rY12
      #elif BETAX
         addpd rA0, rY12
      #endif
      MOVAPD    rY0, (pY,JJ)
         mov    pAn, pA5
      MOVAPD    rY2, 16(pY,JJ)
         mov    II0, II
      MOVAPD    rY4, 32(pY,JJ)
      MOVAPD    rY6, 48(pY,JJ)
   prefetchnta  112(pY,JJ)
      MOVAPD    rY8, 64(pY,JJ)
   prefetchnta  112+64(pY,JJ)
      MOVAPD    rY10, 80(pY,JJ)
      MOVAPD    rY12, 96(pY,JJ)
   add   $112, JJ
   jnz NLOOP

DONE:
   movq -8(%rsp), %rbx
   movq -16(%rsp), %rbp
   movq -24(%rsp), %r12
   movq -32(%rsp), %r13
   movq -40(%rsp), %r14
   movq -48(%rsp), %r15
   ret
/*
 *    Peel of one iteration of X loop to handle misalign of X & A
 */
PEEL_SCALAR:
      MOVSD  (pX,II), rY0
      MOVAPD rY0, rY1
      mulsd  (pA5,mlda5), rY0
      MOVAPD rY1, rY2
      mulsd  (pA5,mlda,4), rY1
      MOVAPD rY2, rY3
      mulsd  (pA5,mlda3), rY2
      MOVAPD rY3, rY4
      mulsd  (pA5,mlda,2), rY3
      MOVAPD rY4, rY5
      mulsd  (pA5,mlda), rY4
      MOVAPD rY5, rY6
      mulsd  (pA5), rY5
      MOVAPD rY6, rY7
      mulsd  (pA5,lda), rY6
      MOVAPD rY7, rY8
      mulsd  (pA5,lda,2), rY7
      MOVAPD rY8, rY9
      mulsd  (pA5,lda3), rY8
      MOVAPD rY9, rY10
      mulsd  (pA5,lda,4), rY9
      MOVAPD rY10, rY11
      mulsd  (pA5,lda5), rY10
      MOVAPD rY11, rY12
      mulsd  (pA5,lda3,2), rY11
      MOVAPD rY12, rY13
      mulsd  (pA5,lda7), rY12
      mulsd  (pA5,lda,8), rY13
         addq  $8, pA5
      MOVAPD 8(pX,II), rX0
      add    $24, II
      jmp    MLOOP
/*
 *    When our X+N was did not end with 16-byte alignment, we must do one
 *    extra scalar iteration
 */
SCALAR_LAST_ITER:
   movsd   (pX,II), rX0
   movsd   16(pA5,mlda5), rA0
   mulsd   rX0, rA0
   addsd   rA0, rY0
   movsd   16(pA5,mlda,4), rA0
   mulsd   rX0, rA0
   addsd   rA0, rY1
   movsd   16(pA5,mlda3), rA0
   mulsd   rX0, rA0
   addsd   rA0, rY2
   movsd   16(pA5,mlda,2), rA0
   mulsd   rX0, rA0
   addsd   rA0, rY3
   movsd   16(pA5,mlda), rA0
   mulsd   rX0, rA0
   addsd   rA0, rY4
   movsd   16(pA5), rA0
   mulsd   rX0, rA0
   addsd   rA0, rY5
   movsd   16(pA5,lda), rA0
   mulsd   rX0, rA0
   addsd   rA0, rY6
   movsd   16(pA5,lda,2), rA0
   mulsd   rX0, rA0
   addsd   rA0, rY7
   movsd   16(pA5,lda3), rA0
   mulsd   rX0, rA0
   addsd   rA0, rY8
   movsd   16(pA5,lda,4), rA0
   mulsd   rX0, rA0
   addsd   rA0, rY9
   movsd   16(pA5,lda5), rA0
   mulsd   rX0, rA0
   addsd   rA0, rY10
   movsd   16(pA5,lda3,2), rA0
   mulsd   rX0, rA0
   addsd   rA0, rY11
   movsd   16(pA5,lda7), rA0
   mulsd   rX0, rA0
   addsd   rA0, rY12
   movsd   16(pA5,lda,8), rA0
   mulsd   rX0, rA0
   addsd   rA0, rY13
   jmp     YSUM
@ROUT ATL_dgemvkT_sse3_4x1
/*
 * ASSUMES: M >= 6, M%2=0, N%4=0, A,X,Y all 16-byte aligned
 */

#define rX0     %xmm0
#define rA00    %xmm1
#define rA01    %xmm2
#define rA02    %xmm3
#define rA03    %xmm4
#define rY0     %xmm5
#define rY1     %xmm6
#define rY2     %xmm7
#define rY3     %xmm8
#define rBETA   %xmm9

#define pA0     %rdx
#define lda     %rcx
#define lda3    %rax
#define pX      %rbx
#define II      %rsi
#define pY      %rbp
#define JJ      %rdi
#define pAn     %r8
#define II0     %r9
#define Mr      %r10 /* 0 or 1 iteration left at end of vector loop? */
/*                      rdi,         rsi,             xmm0
void ATL_UGEMVT(const int M, const int N, const float alpha,
                           rdx            rcx              r8              r9
                const float *A, const int lda, const float *X, const int incX,
                           %xmm1     8(rsp)        16(rsp)
                const float beta, float *Y, const int incY)
*/

        .text
.global ATL_asmdecor(ATL_UGEMVT)
ATL_asmdecor(ATL_UGEMVT):
/*
 * Save callee-saved registers, and load in-memory parameters to registers
 */
   movq %rbx, -8(%rsp)
   movq %rbp, -16(%rsp)

   movq %r8, pX
   movq 8(%rsp), pY
   #ifdef BETAX
      movddup   %xmm1, rBETA
   #endif
/*
 *  Compute the number of remainder iterations after vector loop complete
 */
   xor Mr, Mr                  /* Mr = 0 */
   mov $-1, lda3               /* lda = -1 */
   mov pX, II0                 /* II0 = X */
   lea (pX, II, 8), pX         /* pX pts to end of X */
   bt  $3, pX                  /* CF=1 if pX not aligned */
   cmovc lda3, Mr              /* Mr = (X ends aligned?) 0 : -1 */
   lea (pX, Mr, 8), pX         /* pts to end of aligned space */
   sub pX, II0                 /* II0 = -na*sizeof, na= aligned space elts */
/*
 * Compute derived values
 */
   shl  $3, II                  /* II = M*sizeof */
   movq II, II0
   shl $3, lda                  /* lda *= sizeof */
   lea (lda,lda,2), lda3        /* lda3 = lda*3 */
   shl $3, JJ                   /* JJ = N*sizeof */
   lea (pY, JJ), pY             /* pY += N */
   NLOOP:
      lea (pA0, lda,4), pAn      /* pAn = pA0 + lda*4 */
/*
 *    Peel 1st iteration to zero Y, and then preload all A/B regs
 */
      movaps  (pX,II), rY0
      movaps  rY0, rY1
      mulpd   (pA0), rY0
      movaps  rY1, rY2
      mulpd   (pA0,lda), rY1
      movaps  rY2, rY3
      mulpd   (pA0,lda,2), rY2
      mulpd   (pA0,lda3), rY3

      movaps 16(pA0), rA00
      movaps 16(pA0,lda), rA01
      movaps 16(pA0,lda,2), rA02
      movaps 16(pA0,lda3), rA03
      movaps 16(pX), rX0
      add    $32, pA0
      add    $48, pX
   .align 16
   LOOPM:
      mulpd  rX0, rA00
      addpd  rA00, rY0
      movaps (pA0), rA00

      mulpd  rX0, rA01
      addpd  rA01, rY1
      movaps (pA0,lda), rA01

      mulpd  rX0, rA02
      addpd  rA01, rY2
      movaps (pA0,lda,2), rA02

      mulpd  rX0, rA03
      movaps -16(pX,II), rX0
      addpd  rA03, rY3
      movaps (pA0,lda3), rA03
      add    $16, pA0
      add    $16, II
      jnz    LOOPM
/*
 *    Drain the fetch pipe
 */
      mulpd  rX0, rA00
      addpd  rA00, rY0
      mulpd  rX0, rA01
      addpd  rA01, rY1
      mulpd  rX0, rA02
      addpd  rA01, rY2
      mulpd  rX0, rA03
      addpd  rA03, rY3
      haddpd rY1, rY0
      haddpd rY3, rY2
      #ifdef BETA1
         addps  (pY,JJ), rY0
         addps  16(pY,JJ), rY2
      #elif defined(BETAX)
         movaps (pY,JJ), rA00
         movaps 16(pY,JJ), rA01
         mulps  rBETA, rA00
         mulps  rBETA, rA01
         addps  rA00, rY0
         addps  rA01, rY2
      #endif
      movaps    rY0, (pY,JJ)
      movaps    rY2, 16(pY,JJ)
      movq      pAn, pA0
      movq      II0, II
   add       $16, JJ
   jnz  NLOOP

   movq -8(%rsp), %rbx
   movq -16(%rsp), %rbp
   ret
@ROUT ATL_sgerk4x4_av
#include "atlas_misc.h"

void Mjoin(PATL,ger1_a1_x1_yX)
   (const int M, const int N, const SCALAR alpha, const TYPE *X, const int incX,
    const TYPE *Y, const int incY, TYPE *pA0, const int lda)
{
   vector float y0, y1, y2, y3, x0, x1, x2, x3;
   vector float a00, a10, a20, a30, a01, a11, a21, a31;
   vector float a02, a12, a22, a32, a03, a13, a23, a33;
   vector float m0, m1, m2, m3;
@skip   vector unsigned char m0, m1, m2, m3;
@skip   const vector int izero = VECTOR_INIT(0,0,0,0);
   TYPE *pA1=pA0+lda, *pA2=pA1+lda, *pA3 = pA2+lda;
   const int lda4 = lda<<2, incY2=incY+incY, incY3 = incY2+incY, incY4=(incY<<2);
   int i, j;

   ATL_assert(incX == 1);
   ATL_assert(((size_t)X)%16 == 0);
   ATL_assert(((size_t)pA0)%16 == 0);
   vec_mtvscr((vector unsigned int){0,0,0,0});
   for (j=0; j < N; j += 4, pA0 += lda4, pA1 += lda4, pA2 += lda4, pA3 += lda4, 
                            Y += incY4)
   {
      a00 = vec_ld(0, pA0);
      a01 = vec_ld(0, pA1);
      a02 = vec_ld(0, pA2);
      a03 = vec_ld(0, pA3);
      a10 = vec_ld(0, pA0+4);
      a11 = vec_ld(0, pA1+4);
      a12 = vec_ld(0, pA2+4);
      a13 = vec_ld(0, pA3+4);
      a20 = vec_ld(0, pA0+8);
      a21 = vec_ld(0, pA1+8);
      a22 = vec_ld(0, pA2+8);
      a23 = vec_ld(0, pA3+8);
      a30 = vec_ld(0, pA0+12);
      a31 = vec_ld(0, pA1+12);
      a32 = vec_ld(0, pA2+12);
      a33 = vec_ld(0, pA3+12);
#if 1
      m0 = (vector float) vec_lvsl(0, Y);
      m1 = (vector float) vec_lvsl(0, Y+incY);
      m2 = (vector float) vec_lvsl(0, Y+incY2);
      m3 = (vector float) vec_lvsl(0, Y+incY3);
      y0 = vec_lde(0, Y);
      y1 = vec_lde(0, Y+incY);
      y2 = vec_lde(0, Y+incY2);
      y3 = vec_lde(0, Y+incY3);
      m0 = vec_splat(m0, 0);
      m1 = vec_splat(m1, 0);
      m2 = vec_splat(m2, 0);
      m3 = vec_splat(m3, 0);
      y0 = vec_perm(y0, y0, (vector unsigned char)m0);
      y1 = vec_perm(y1, y1, (vector unsigned char)m1);
      y2 = vec_perm(y2, y2, (vector unsigned char)m2);
      y3 = vec_perm(y3, y3, (vector unsigned char)m3);
#else
      y0 = vec_ld(0, Y);
      y1 = vec_splat(y0, 1);
      y2 = vec_splat(y0, 2);
      y3 = vec_splat(y0, 3);
      y0 = vec_splat(y0, 0);
#endif
      x0 = vec_ld(0, X);
      x1 = vec_ld(0, X+4);
      x2 = vec_ld(0, X+8);
      x3 = vec_ld(0, X+12);
      for (i=16; i < M; i += 16)
      {
         a00 = vec_madd(x0, y0, a00);
         a10 = vec_madd(x1, y0, a10);
         a20 = vec_madd(x2, y0, a20);
         a30 = vec_madd(x3, y0, a30);
         vec_stl(a00, 0, pA0+i-16);
         vec_stl(a10, 0, pA0+i-12);
         vec_stl(a20, 0, pA0+i-8);
         vec_stl(a30, 0, pA0+i-4);
         a00 = vec_ld(0, pA0+i);
         a10 = vec_ld(0, pA0+i+4);
         a20 = vec_ld(0, pA0+i+8);
         a30 = vec_ld(0, pA0+i+12);
         a01 = vec_madd(x0, y1, a01);
         a11 = vec_madd(x1, y1, a11);
         a21 = vec_madd(x2, y1, a21);
         a31 = vec_madd(x3, y1, a31);
         vec_stl(a01, 0, pA1+i-16);
         vec_stl(a11, 0, pA1+i-12);
         vec_stl(a21, 0, pA1+i-8);
         vec_stl(a31, 0, pA1+i-4);
         a01 = vec_ld(0, pA1+i);
         a11 = vec_ld(0, pA1+i+4);
         a21 = vec_ld(0, pA1+i+8);
         a31 = vec_ld(0, pA1+i+12);
         a02 = vec_madd(x0, y2, a02);
         a12 = vec_madd(x1, y2, a12);
         a22 = vec_madd(x2, y2, a22);
         a32 = vec_madd(x3, y2, a32);
         vec_stl(a02, 0, pA2+i-16);
         vec_stl(a12, 0, pA2+i-12);
         vec_stl(a22, 0, pA2+i-8);
         vec_stl(a32, 0, pA2+i-4);
         a02 = vec_ld(0, pA2+i);
         a12 = vec_ld(0, pA2+i+4);
         a22 = vec_ld(0, pA2+i+8);
         a32 = vec_ld(0, pA2+i+12);
         a03 = vec_madd(x0, y3, a03);
         a13 = vec_madd(x1, y3, a13);
         a23 = vec_madd(x2, y3, a23);
         a33 = vec_madd(x3, y3, a33);
         x0 = vec_ld(0, X+i);
         x1 = vec_ld(0, X+i+4);
         x2 = vec_ld(0, X+i+8);
         x3 = vec_ld(0, X+i+12);
         vec_stl(a03, 0, pA3+i-16);
         vec_stl(a13, 0, pA3+i-12);
         vec_stl(a23, 0, pA3+i-8);
         vec_stl(a33, 0, pA3+i-4);
         a03 = vec_ld(0, pA3+i);
         a13 = vec_ld(0, pA3+i+4);
         a23 = vec_ld(0, pA3+i+8);
         a33 = vec_ld(0, pA3+i+12);
      }
/*
 *    Drain load/use pipe
 */
      a00 = vec_madd(x0, y0, a00);
      a10 = vec_madd(x1, y0, a10);
      a20 = vec_madd(x2, y0, a20);
      a30 = vec_madd(x3, y0, a30);
      vec_stl(a00, 0, pA0+i-16);
      vec_stl(a10, 0, pA0+i-12);
      vec_stl(a20, 0, pA0+i-8);
      vec_stl(a30, 0, pA0+i-4);
      a01 = vec_madd(x0, y1, a01);
      a11 = vec_madd(x1, y1, a11);
      a21 = vec_madd(x2, y1, a21);
      a31 = vec_madd(x3, y1, a31);
      vec_stl(a01, 0, pA1+i-16);
      vec_stl(a11, 0, pA1+i-12);
      vec_stl(a21, 0, pA1+i-8);
      vec_stl(a31, 0, pA1+i-4);
      a02 = vec_madd(x0, y2, a02);
      a12 = vec_madd(x1, y2, a12);
      a22 = vec_madd(x2, y2, a22);
      a32 = vec_madd(x3, y2, a32);
      vec_stl(a02, 0, pA2+i-16);
      vec_stl(a12, 0, pA2+i-12);
      vec_stl(a22, 0 , pA2+i-8);
      vec_stl(a32, 0 , pA2+i-4);
      a03 = vec_madd(x0, y3, a03);
      a13 = vec_madd(x1, y3, a13);
      a23 = vec_madd(x2, y3, a23);
      a33 = vec_madd(x3, y3, a33);
      vec_stl(a03, 0, pA3+i-16);
      vec_stl(a13, 0, pA3+i-12);
      vec_stl(a23, 0, pA3+i-8);
      vec_stl(a33, 0, pA3+i-4);
   }
}
@ROUT ATL_sgemvk4x4_av
#include "atlas_misc.h"

#ifdef BETA0
void ATL_sgemvT_a1_x1_b0_y1
#elif defined(BETAX)
void ATL_sgemvT_a1_x1_bX_y1
#else
void ATL_sgemvT_a1_x1_b1_y1
#endif
   (ATL_CINT N, ATL_CINT M, const float alpha, const float *pA0, ATL_CINT lda,
    const float *X, ATL_CINT incX, const float beta, float *Y, ATL_CINT incY)
{
   int i, j;
   vector float vbeta = {beta, beta, beta, beta};
   vector float y0, y1, y2, y3, z0, z1, z2, z3, x0, x1, x2, x3, vzero;
   vector float a00, a10, a20, a30, a01, a11, a21, a31;
   vector float a02, a12, a22, a32, a03, a13, a23, a33;
   const TYPE *pA1=pA0+lda, *pA2=pA1+lda, *pA3 = pA2+lda;
   const int lda4 = lda<<2;

   vzero = vec_xor(vzero, vzero);
   for (j=0; j < N; j += 4, pA0 += lda4, pA1 += lda4, pA2 += lda4, pA3 += lda4)
   {
      a00 = vec_ld(0, pA0);
      a01 = vec_ld(0, pA1);
      a02 = vec_ld(0, pA2);
      a03 = vec_ld(0, pA3);
      a10 = vec_ld(0, pA0+4);
      a11 = vec_ld(0, pA1+4);
      a12 = vec_ld(0, pA2+4);
      a13 = vec_ld(0, pA3+4);
      a20 = vec_ld(0, pA0+8);
      a21 = vec_ld(0, pA1+8);
      a22 = vec_ld(0, pA2+8);
      a23 = vec_ld(0, pA3+8);
      a30 = vec_ld(0, pA0+12);
      a31 = vec_ld(0, pA1+12);
      a32 = vec_ld(0, pA2+12);
      a33 = vec_ld(0, pA3+12);
      x0 = vec_ld(0, X);
      x1 = vec_ld(0, X+4);
      x2 = vec_ld(0, X+8);
      x3 = vec_ld(0, X+12);
      y0 = vec_xor(y0, y0);
      y1 = y0;
      y2 = vec_xor(y2, y2);
      y3 = y0;
      for (i=16; i < M; i += 16)
      {
         y0 = vec_madd(x0, a00, y0);
         y1 = vec_madd(x0, a01, y1);
         y2 = vec_madd(x0, a02, y2);
         y3 = vec_madd(x0, a03, y3);
         a00 = vec_ld(0, pA0+i);
         a01 = vec_ld(0, pA1+i);
         a02 = vec_ld(0, pA2+i);
         a03 = vec_ld(0, pA3+i);
         z0 = vec_madd(x1, a10, z0);
         z1 = vec_madd(x1, a11, z1);
         z2 = vec_madd(x1, a12, z2);
         z3 = vec_madd(x1, a13, z3);
         a10 = vec_ld(0, pA0+i+4);
         a11 = vec_ld(0, pA1+i+4);
         a12 = vec_ld(0, pA2+i+4);
         a13 = vec_ld(0, pA3+i+4);
         y0 = vec_madd(x2, a20, y0);
         y1 = vec_madd(x2, a21, y1);
         y2 = vec_madd(x2, a22, y2);
         y3 = vec_madd(x2, a23, y3);
         a20 = vec_ld(0, pA0+i+8);
         a21 = vec_ld(0, pA1+i+8);
         a22 = vec_ld(0, pA2+i+8);
         a23 = vec_ld(0, pA3+i+8);
         z0 = vec_madd(x3, a30, z0);
         z1 = vec_madd(x3, a31, z1);
         z2 = vec_madd(x3, a32, z2);
         z3 = vec_madd(x3, a33, z3);
         x0 = vec_ld(0, X+i);
         x1 = vec_ld(0, X+i+4);
         x2 = vec_ld(0, X+i+8);
         x3 = vec_ld(0, X+i+12);
         a30 = vec_ld(0, pA0+i+12);
         a31 = vec_ld(0, pA1+i+12);
         a32 = vec_ld(0, pA2+i+12);
         a33 = vec_ld(0, pA3+i+12);
      }
/*
 *    Drain load/use pipeline
 */
      y0 = vec_madd(x0, a00, y0);
      y1 = vec_madd(x0, a01, y1);
      y2 = vec_madd(x0, a02, y2);
      y3 = vec_madd(x0, a03, y3);
      z0 = vec_madd(x1, a10, z0);
      z1 = vec_madd(x1, a11, z1);
      z2 = vec_madd(x1, a12, z2);
      z3 = vec_madd(x1, a13, z3);
      y0 = vec_madd(x2, a20, y0);
      y1 = vec_madd(x2, a21, y1);
      y2 = vec_madd(x2, a22, y2);
      y3 = vec_madd(x2, a23, y3);
      z0 = vec_madd(x3, a30, z0);
      z1 = vec_madd(x3, a31, z1);
      z2 = vec_madd(x3, a32, z2);
      z3 = vec_madd(x3, a33, z3);
      #if defined(BETA1) || defined(BETAX)
         a00 = vec_ld(0, Y+j);
         #ifdef BETAX
            a00 = vec_madd(a00, vbeta, vzero);
         #endif
      #endif
/*
 *    Add y += z, and reduce to scalars
 */
      y0 = vec_add(y0, z0);     /* y0 = {y0a, y0b, y0c, y0d} */
      y1 = vec_add(y1, z1);     /* y1 = {y1a, y1b, y1c, y1d} */
      y2 = vec_add(y2, z2);     /* y2 = {y2a, y2b, y2c, y2d} */
      y3 = vec_add(y3, z3);     /* y3 = {y3a, y3b, y3c, y3d} */

      z0 = vec_mergeh(y0, y2);  /* z0 = {y0a, y2a, y0b, y2b} */
      z2 = vec_mergel(y0, y2);  /* z2 = {y0c, y2c, y0d, y2d} */
      z1 = vec_mergeh(y1, y3);  /* z1 = {y1a, y3a, y1b, y3b} */
      z3 = vec_mergel(y1, y3);  /* z3 = {y1c, y3c, y1d, y3d} */

      y0 = vec_mergeh(y0, z1);  /* y0 = {y0a, y1a, y2a, y3a} */
      y2 = vec_mergel(z0, z1);  /* y2 = {y0b, y1b, y2b, y3b} */
      y1 = vec_mergeh(z2, z3);  /* y1 = {y0c, y1c, y2c, y3c} */
      y3 = vec_mergel(z2, z3);  /* y3 = {y0d, y1d, y2d, y3d} */

      y0 = vec_add(y0, y2);     /* y0 = {y0ab, y1ab, y2ab, y3ab} */
      y1 = vec_add(y1, y3);     /* y0 = {y0cd, y1cd, y2cd, y3cd} */
      y0 = vec_add(y0, y1);     /* y0 = {y0abcd, y1abcd, y2abcd, y3abcd} */
      #if defined(BETAX) || defined(BETA1)
         y0 = vec_add(y0, a00);
      #endif
      vec_stl(y0, 0, Y+j);
   }
}
@ROUT ATL_dgemvkT_sse3_10x1
/*
 * ASSUMES: N%10=0, A,X,Y 16-byte aligned
 */
#define rA0     %xmm0
#define rX0     %xmm1
#define rY0     %xmm2
#define rY1     %xmm3
#define rY2     %xmm4
#define rY3     %xmm5
#define rY4     %xmm6
#define rY5     %xmm7
#define rY6     %xmm8
#define rY7     %xmm9
#define rY8     %xmm10
#define rY9     %xmm11
#define rt0     %xmm12
#define rt1     %xmm13
#define rt2     %xmm14
#define rBETA   %xmm15

#define II      %rbp
#define pX      %rsi
#define pA0     %rdx
#define pA1     %rbx
#define pA2     %rax
#define pA3     %rdi
#define pA4     %rsi
#define pA5     %r8
#define pA6     %r9
#define pA7     %r10
#define pA8     %r12
#define pA9     %r13
#define JJ      %14
#define pY      %15

#define LDA10OFF 
#define IIOFF

   lea  (pA0, lda), pA1
   lea  (pA0, lda,2), pA2
   lea  (pA0, lda,4), pA4
   lea  (pA0, lda,8), pA8
   lea  (pA1, lda,2), pA3
   lea  (pA1, lda,4), pA5
   lea  (pA1, lda,8), pA9
   lea  (pA4, lda,2), pA6
   lea  (pA8, lda), pA9
   lea  (pA3, lda,4), pA7
   add  lda, lda        /* lda = 2*lda */
   lea  (lda,lda,4)     /* lda = 2*lda + 4*2*lda = 2*lda + 8*lda = 10*lda */
   NLOOP:
         movaps (pX,II), rX0
      .align 16
      MLOOP:
         movaps (pA0,II), rA0           // 0
         mulpd  rX0, rA0                // 4
         addpd  rA0, rY0                // 8

         movaps (pA1,II), rA0           // 12 : end of decode 0
         mulpd  rX0, rA0                // 0-16
         addpd  rA0, rY1                // 4-20

         movaps (pA2,II), rA0           // 8-24
         mulpd  rX0, rA0                // 12-28 ; end of decode 1
         addpd  rA0, rY2                // 0-32

         movaps (pA3,II), rA0           // 4-36
         mulpd  rX0, rA0                // 8-40
         addpd  rA0, rY3                // 12-44 ; end of decode 2

         movaps (pA4,II), rA0           // 0-48
         mulpd  rX0, rA0                // 4-52
         addpd  rA0, rY4                // 8-56

         movaps (pA5,II), rA0           // 12-60 ; end of decode 3
         mulpd  rX0, rA0                // 0-64
         addpd  rA0, rY5                // 4-68

         movaps (pA6,II), rA0           // 8-72
         mulpd  rX0, rA0                // 12-76 ; end of decode 4
         addpd  rA0, rY6                // 0-80 

         movaps (pA7,II), rA0           // 4-84 
         mulpd  rX0, rA0                // 8-88
         addpd  rA0, rY7                // 12-92  ; end of decode 5

         movaps (pA8,II), rA0           // 0-96
         mulpd  rX0, rA0                // 4-100
         addpd  rA0, rY8                // 8-104

         mulpd  (pA9,II), rX0           // 12-108 ; end of decode 6
         addpd  rX0, rY9                // 0-112

         movaps 16(pX,II), rX0          // 4-116
      add %eax, %esi                    // 6-118; II += 16
      jnz MLOOP                         // 8-120 ; end of decode 7

      add lda, pA0
      add lda, pA1
      add lda, pA2
      add lda, pA3
      add lda, pA4
      add lda, pA5
      add lda, pA6
      add lda, pA7
      add lda, pA8
      add lda, pA9
@ROUT ATL_zgerk_1x4_sse3 zgemv_1x4_sse2
@extract -b @(topd)/cw.inc lang=C -def cwdate 2009 -def cwdate 2010

#include <xmmintrin.h>
#include "atlas_misc.h"

@ROUT ATL_zgerk_1x4_sse3
@beginskip
#ifdef Conj_
void Mjoin(PATL,ger1c_a1_x1_yX)
#else
void Mjoin(PATL,ger1u_a1_x1_yX)
#endif
@endskip
#define ATL_USEPF
#ifndef PFDIST
   #define PFDIST 32
#endif
@define sf @@
@whiledef sf u
static void gerk_sse3@(sf)
   (ATL_CINT M0, ATL_CINT N, const TYPE *X, const TYPE *Y0, 
    TYPE *A0, ATL_CINT lda0)
/* 
 * N must be a multiple of 4
 */
{
   ATL_CINT lda=lda0+lda0, lda4 = lda<<2, M = M0+M0;
   TYPE *A1=A0+lda, *A2=A1+lda, *A3=A2+lda;
   register __m128d y0a, y0b, y1a, y1b, y2a, y2b, y3a, y3b, x0a, x0b, xn;
   register __m128d a00, a01, a02, a03;
   const __m128d vnone = {-1.0, -1.0};
   ATL_INT i, j;

   for (j=0; j < N; j += 4, A0 += lda4, A1 += lda4, A2 += lda4, A3 += lda4,
                            Y0 += 8)
   {
      a00 = _mm_load@(sf)_pd(A0);
      a01 = _mm_load@(sf)_pd(A1);
      a02 = _mm_load@(sf)_pd(A2);
      a03 = _mm_load@(sf)_pd(A3);
      x0a = _mm_load@(sf)_pd(X);             /* x0a = {Xi, Xr} */
@skip      x0b = _mm_shuffle_epi32((__m128i)x0a, $0b0100 1110);   /* {Yr, Xi} */
      x0b = (__m128d)_mm_shuffle_epi32((__m128i)x0a, 0x4E);   /* x0b = {Yr, Xi} */

      y0a = _mm_load1_pd(Y0);           /* y0a = {Yr, Yr} */
      y0b = _mm_load1_pd(Y0+1);         /* y0b = {Yi, Yi} */
      y0b = _mm_mul_sd(y0b, vnone);     /* y0b = {Yi,-Yi} */
      y1a = _mm_load1_pd(Y0+2);         /* y1a = {Yr, Yr} */
      y1b = _mm_load1_pd(Y0+3);         /* y1b = {Yi, Yi} */
      y1b = _mm_mul_sd(y1b, vnone);     /* y1b = {Yi,-Yi} */
      y2a = _mm_load1_pd(Y0+4);         /* y2a = {Yr, Yr} */
      y2b = _mm_load1_pd(Y0+5);         /* y2b = {Yi, Yi} */
      y2b = _mm_mul_sd(y2b, vnone);     /* y2b = {Yi,-Yi} */
      y3a = _mm_load1_pd(Y0+6);         /* y3a = {Yr, Yr} */
      y3b = _mm_load1_pd(Y0+7);         /* y3b = {Yi, Yi} */
      y3b = _mm_mul_sd(y3b, vnone);     /* y3b = {Yi,-Yi} */
      for (i=2; i < M; i += 2)
      {
/*
 *       Do Ar += xr*yr;  Ai += xi*yr; for all 4 columns of A
 */

         #ifdef ATL_USEPF
            _mm_prefetch(A0+PFDIST, _MM_HINT_T0);
         #endif
         xn = x0a;                    /* xn  = {Xi, Xr} */
         x0a = _mm_mul_pd(x0a, y0a);  /* x0a = {Xi*Yr, Xr*Yr} */
         a00 = _mm_add_pd(a00, x0a);  /* a00 = {Ai+Xi*Yr, Ar+Xr*Yr} */
         x0a = xn;
         #ifdef ATL_USEPF
            _mm_prefetch(A1+PFDIST, _MM_HINT_T0);
         #endif
         xn  = _mm_mul_pd(xn, y1a);   /* xn  = {Xi*Yr, Xr*Yr} */
         a01 = _mm_add_pd(a01, xn);   /* a01 = {Ai+Xi*Yr, Ar+Xr*Yr} */
         #ifdef ATL_USEPF
            _mm_prefetch(A2+PFDIST, _MM_HINT_T0);
         #endif
         xn = x0a;
         x0a = _mm_mul_pd(x0a, y2a);  /* x0a = {Xi*Yr, Xr*Yr} */
         a02 = _mm_add_pd(a02, x0a);  /* a02 = {Ai+Xi*Yr, Ar+Xr*Yr} */
         x0a = _mm_load@(sf)_pd(X+i);      /* x0a = {Xi, Xr} */
         #ifdef ATL_USEPF
            _mm_prefetch(A3+PFDIST, _MM_HINT_T0);
         #endif
         xn  = _mm_mul_pd(xn, y3a);   /* xn  = {Xi*Yr, Xr*Yr} */
         a03 = _mm_add_pd(a03, xn);   /* a02 = {Ai+Xi*Yr, Ar+Xr*Yr} */
/*
 *       Do Ar += xi*(-yi); Ai += xr*yi; for all 4 columns of A
 */
                                      /* y0b = {Yi,-Yi} */
         xn = x0b;                    /* xn  = {Xr, Xi} */
         x0b = _mm_mul_pd(x0b, y0b);  /* x0b = {Xr*Yi, -Xi*Yi} */
         a00 = _mm_add_pd(a00, x0b);  /* a00 = {Ai+Xi*Yr+Xr*Yi, Ar+Xr*Yr-Xi*Yi} */
         _mm_store@(sf)_pd(A0+i-2, a00);   /* *A0 = a00 */
         a00 = _mm_load@(sf)_pd(A0+i);
         x0b = xn;                    /* x0b = {Xr, Xi} */
         xn  = _mm_mul_pd(xn,  y1b);  /* xn  = {Xr*Yi, -Xi*Yi} */
         a01 = _mm_add_pd(a01, xn);   /* a01 = {Ai+Xi*Yr+Xr*Yi, Ar+Xr*Yr-Xi*Yi} */
         _mm_store@(sf)_pd(A1+i-2, a01);   /* *A1 = a01 */
         a01 = _mm_load@(sf)_pd(A1+i);
         xn = x0b;
         x0b = _mm_mul_pd(x0b, y2b);  /* x0b = {Xr*Yi, -Xi*Yi} */
         a02 = _mm_add_pd(a02, x0b);  /* a02 = {Ai+Xi*Yr+Xr*Yi, Ar+Xr*Yr-Xi*Yi} */
         _mm_store@(sf)_pd(A2+i-2, a02);   /* *A2 = a02 */
         a02 = _mm_load@(sf)_pd(A2+i);
         x0b = (__m128d)_mm_shuffle_epi32((__m128i)x0a, 0x4E);   /* x0b = {Yr, Xi} */
         xn  = _mm_mul_pd(xn,  y3b);  /* xn  = {Xr*Yi, -Xi*Yi} */
         a03 = _mm_add_pd(a03, xn);   /* a01 = {Ai+Xi*Yr+Xr*Yi, Ar+Xr*Yr-Xi*Yi} */
         _mm_store@(sf)_pd(A3+i-2, a03);   /* *A3 = a03 */
         a03 = _mm_load@(sf)_pd(A3+i);
      }
/*
 *    ===================
 *    Drain load/use pipe
 *    ===================
 */
/*
 *    Do Ar += xr*yr;  Ai += xi*yr; for all 4 columns of A
 */
      xn = x0a;                    /* xn  = {Xi, Xr} */
      x0a = _mm_mul_pd(x0a, y0a);  /* x0a = {Xi*Yr, Xr*Yr} */
      a00 = _mm_add_pd(a00, x0a);  /* a00 = {Ai+Xi*Yr, Ar+Xr*Yr} */
      x0a = xn;
      xn  = _mm_mul_pd(xn, y1a);   /* xn  = {Xi*Yr, Xr*Yr} */
      a01 = _mm_add_pd(a01, xn);   /* a01 = {Ai+Xi*Yr, Ar+Xr*Yr} */
      xn = x0a;
      x0a = _mm_mul_pd(x0a, y2a);  /* x0a = {Xi*Yr, Xr*Yr} */
      a02 = _mm_add_pd(a02, x0a);  /* a02 = {Ai+Xi*Yr, Ar+Xr*Yr} */
      xn  = _mm_mul_pd(xn, y3a);   /* xn  = {Xi*Yr, Xr*Yr} */
      a03 = _mm_add_pd(a03, xn);   /* a02 = {Ai+Xi*Yr, Ar+Xr*Yr} */
/*
 *    Do Ar += xi*(-yi); Ai += xr*yi; for all 4 columns of A
 */
                                   /* y0b = {Yi,-Yi} */
      xn = x0b;                    /* xn  = {Xr, Xi} */
      x0b = _mm_mul_pd(x0b, y0b);  /* x0b = {Xr*Yi, -Xi*Yi} */
      a00 = _mm_add_pd(a00, x0b);  /* a00 = {Ai+Xi*Yr+Xr*Yi, Ar+Xr*Yr-Xi*Yi} */
      _mm_store@(sf)_pd(A0+i-2, a00);   /* *A0 = a00 */
      x0b = xn;                    /* x0b = {Xr, Xi} */
      xn  = _mm_mul_pd(xn,  y1b);  /* xn  = {Xr*Yi, -Xi*Yi} */
      a01 = _mm_add_pd(a01, xn);   /* a01 = {Ai+Xi*Yr+Xr*Yi, Ar+Xr*Yr-Xi*Yi} */
      _mm_store@(sf)_pd(A1+i-2, a01);   /* *A1 = a01 */
      xn = x0b;
      x0b = _mm_mul_pd(x0b, y2b);  /* x0b = {Xr*Yi, -Xi*Yi} */
      a02 = _mm_add_pd(a02, x0b);  /* a02 = {Ai+Xi*Yr+Xr*Yi, Ar+Xr*Yr-Xi*Yi} */
      _mm_store@(sf)_pd(A2+i-2, a02);   /* *A2 = a02 */
      x0b = (__m128d)_mm_shuffle_epi32((__m128i)x0a, 0x4E);   /* x0b = {Yr, Xi} */
      xn  = _mm_mul_pd(xn,  y3b);  /* xn  = {Xr*Yi, -Xi*Yi} */
      a03 = _mm_add_pd(a03, xn);   /* a01 = {Ai+Xi*Yr+Xr*Yi, Ar+Xr*Yr-Xi*Yi} */
      _mm_store@(sf)_pd(A3+i-2, a03);   /* *A3 = a03 */
   }
}
@endwhile
void ATL_UGERK
   (ATL_CINT M, ATL_CINT N, const TYPE *X, const TYPE *Y, TYPE *A, ATL_CINT lda)
{
   size_t ia = (size_t) A, ix = (size_t) X;
   ATL_CINT N4 = (N>>2)<<2;
   const TYPE one[2] = {ATL_rone, ATL_rzero};

   if (N4)
   {
      if ((ia>>4)<<4 == ia && (ix>>4)<<4 == ix)
         gerk_sse3(M, N4, X, Y, A, lda);
      else
         gerk_sse3u(M, N4, X, Y, A, lda);
   }
   if (N4 != N)
      Mjoin(PATL,gerk_axpy)(M, N-N4, one, X, 1, Y+(N4 SHIFT), 1, 
                            A+N4*(lda SHIFT), lda);

}
@ROUT zgemv_1x4_sse2
#define _my_hadd_pd(dst, src) \
   __asm__ __volatile__ ("haddpd %2, %0" : "=x"(dst) : "0" (dst), "x"(src))

#ifdef Conj_
void Mjoin(Mjoin(ATL_zgemvC_a1_x1,BNM),_y1)
#else
void Mjoin(Mjoin(ATL_zgemvT_a1_x1,BNM),_y1)
#endif
   (ATL_CINT N, ATL_CINT M0, const TYPE *alpha, const TYPE *A0, ATL_CINT lda0, 
    const TYPE *X, ATL_CINT incX, const TYPE *beta, TYPE *Y, ATL_CINT incY)
{
   ATL_CINT lda=lda0+lda0, lda4 = lda<<2, M = M0+M0;
   ATL_INT i, j;
   const TYPE *A1=A0+lda, *A2=A1+lda, *A3=A2+lda;
   #ifdef BETAX
      const TYPE vbeta[2] __attribute((aligned(16))) = {*beta, beta[1]};
   #endif
    const register __m128d rscal = {ATL_rone, ATL_rnone};
   register __m128d y0r, y0i,  y1r, y1i,  y2r, y2i,  y3r, y3i,
                    x0, z0, xn,
                    a00, a01, a02, a03;
   ATL_assert(N%4 == 0);

   for (j=0; j < N; j += 4, A0+=lda4, A1+=lda4, A2+=lda4, A3+=lda4, Y += 8)
   {
      #ifdef BETAX
         xn = _mm_load_pd(vbeta);       /* xn = {Bi, Br} */
         y0r = _mm_load_pd(Y);          /* y0r = {Yi, Yr} */
         y0i = (__m128d)_mm_shuffle_epi32((__m128i)y0r, 0x4E);/* y0i={Yr, Yi} */
         y0r = _mm_mul_pd(y0r, xn);     /* y0r = {Yi*Bi, Yr*Br} */
         y0i = _mm_mul_pd(y0i, xn);     /* y0i = {Yr*Bi, Yi*Br} */

         y1r = _mm_load_pd(Y+2);        /* y1r = {Yi, Yr} */
         y1i = (__m128d)_mm_shuffle_epi32((__m128i)y1r, 0x4E);/* y1i={Yr, Yi} */
         y1r = _mm_mul_pd(y1r, xn);     /* y1r = {Yi*Bi, Yr*Br} */
         y1i = _mm_mul_pd(y1i, xn);     /* y1i = {Yr*Bi, Yi*Br} */

         y2r = _mm_load_pd(Y+4);        /* y2r = {Yi, Yr} */
         y2i = (__m128d)_mm_shuffle_epi32((__m128i)y2r, 0x4E);/* y2i={Yr, Yi} */
         y2r = _mm_mul_pd(y2r, xn);     /* y2r = {Yi*Bi, Yr*Br} */
         y2i = _mm_mul_pd(y2i, xn);     /* y2i = {Yr*Bi, Yi*Br} */

         y3r = _mm_load_pd(Y+6);        /* y3r = {Yi, Yr} */
         y3i = (__m128d)_mm_shuffle_epi32((__m128i)y3r, 0x4E);/* y3i={Yr, Yi} */
         y3r = _mm_mul_pd(y3r, xn);     /* y3r = {Yi*Bi, Yr*Br} */
         y3i = _mm_mul_pd(y3i, xn);     /* y3i = {Yr*Bi, Yi*Br} */
      #elif defined(BETA0)
         y0r = _mm_xor_pd(y0r, y0r);
         y0i = y0r;
         y1r = _mm_xor_pd(y1r, y1r);
         y1i = y1r;
         y2r = _mm_xor_pd(y2r, y2r);
         y2i = y2r;
         y3r = _mm_xor_pd(y3r, y3r);
         y3i = y3r;
      #else
         y0r = _mm_load_sd(Y);     /* y0r = {0, Yr} */
         y0i = _mm_load_sd(Y+1);   /* y0i = {0, Yi} */
         y1r = _mm_load_sd(Y+2);
         y1i = _mm_load_sd(Y+3);
         y2r = _mm_load_sd(Y+4);
         y2i = _mm_load_sd(Y+5);
         y3r = _mm_load_sd(Y+6);
         y3i = _mm_load_sd(Y+7);
         #ifdef BETAXI0
            xn = _mm_load_sd(beta);
            y0r = _mm_mul_sd(y0r, xn);
            y0i = _mm_mul_sd(y0i, xn);
            y1r = _mm_mul_sd(y1r, xn);
            y1i = _mm_mul_sd(y1i, xn);
            y2r = _mm_mul_sd(y2r, xn);
            y2i = _mm_mul_sd(y2i, xn);
            y3r = _mm_mul_sd(y3r, xn);
            y3i = _mm_mul_sd(y3i, xn);
         #endif
      #endif

      x0 = _mm_load_pd(X);      /* x0 = {Xi, Xr} */
      z0 = (__m128d)_mm_shuffle_epi32((__m128i)x0, 0x4E);   /* z0 = {Xr, Xi} */
      a00 = _mm_load_pd(A0);    /* a00 = {Ai, Ar} */
      a01 = _mm_load_pd(A1);    /* a01 = {Ai, Ar} */
      a02 = _mm_load_pd(A2);    /* a02 = {Ai, Ar} */
      a03 = _mm_load_pd(A3);    /* a03 = {Ai, Ar} */
      for (i=2; i < M; i += 2)
      {
         xn = x0;                       /* xn = {Xi, Xr} */
         x0 = _mm_mul_pd(x0, a00);      /* x0 = {Xi*Ai, Xr*Ar} */
         y0r = _mm_add_pd(y0r, x0);     /* y0r = {yr+Xi*Ai, yr+Xr*Ar} */
         x0 = xn;
         xn = _mm_mul_pd(xn, a01);      /* xn = {Xi*Ai, Xr*Ar} */
         y1r = _mm_add_pd(y1r, xn);     /* y1r = {yr+Xi*Ai, yr+Xr*Ar} */
         xn = x0;                       /* xn = {Xi, Xr} */
         x0 = _mm_mul_pd(x0, a02);      /* x0 = {Xi*Ai, Xr*Ar} */
         y2r = _mm_add_pd(y2r, x0);     /* y2r = {yr+Xi*Ai, yr+Xr*Ar} */
         x0 = _mm_load_pd(X+i);         /* x0 = {Xi, Xr} */
         xn = _mm_mul_pd(xn, a03);      /* xn = {Xi*Ai, Xr*Ar} */
         y3r = _mm_add_pd(y3r, xn);     /* y1r = {yr+Xi*Ai, yr+Xr*Ar} */

         a00 = _mm_mul_pd(a00, z0);     /* a00 = {Ai*Xr, Ar*Xi} */
         y0i = _mm_add_pd(y0i, a00);    /* y0i = {yi+Ai*xr, yi+Ar*Xi} */
         a00 = _mm_load_pd(A0+i);       /* a00 = {Ai, Ar} */
         a01 = _mm_mul_pd(a01, z0);     /* a01 = {Ai*Xr, Ar*Xi} */
         y1i = _mm_add_pd(y1i, a01);    /* y1i = {yi+Ai*xr, yi+Ar*Xi} */
         a01 = _mm_load_pd(A1+i);       /* a01 = {Ai, Ar} */
         a02 = _mm_mul_pd(a02, z0);     /* a02 = {Ai*Xr, Ar*Xi} */
         y2i = _mm_add_pd(y2i, a02);    /* y2i = {yi+Ai*xr, yi+Ar*Xi} */
         a02 = _mm_load_pd(A2+i);       /* a02 = {Ai, Ar} */
         a03 = _mm_mul_pd(a03, z0);     /* a03 = {Ai*Xr, Ar*Xi} */
         y3i = _mm_add_pd(y3i, a03);    /* y3i = {yi+Ai*xr, yi+Ar*Xi} */
         a03 = _mm_load_pd(A3+i);       /* a03 = {Ai, Ar} */
         z0 = (__m128d)_mm_shuffle_epi32((__m128i)x0, 0x4E);   /* z0 = {Xr, Xi} */
      }
/* 
 *    Drain load/use pipe
 */
      xn = x0;                       /* xn = {Xi, Xr} */
      x0 = _mm_mul_pd(x0, a00);      /* x0 = {Xi*Ai, Xr*Ar} */
      y0r = _mm_add_pd(y0r, x0);     /* y0r = {yr+Xi*Ai, yr+Xr*Ar} */
      x0 = xn;
      xn = _mm_mul_pd(xn, a01);      /* xn = {Xi*Ai, Xr*Ar} */
      y1r = _mm_add_pd(y1r, xn);     /* y1r = {yr+Xi*Ai, yr+Xr*Ar} */
      xn = x0;                       /* xn = {Xi, Xr} */
      x0 = _mm_mul_pd(x0, a02);      /* x0 = {Xi*Ai, Xr*Ar} */
      y2r = _mm_add_pd(y2r, x0);     /* y2r = {yr+Xi*Ai, yr+Xr*Ar} */
      xn = _mm_mul_pd(xn, a03);      /* xn = {Xi*Ai, Xr*Ar} */
      y3r = _mm_add_pd(y3r, xn);     /* y1r = {yr+Xi*Ai, yr+Xr*Ar} */

      a00 = _mm_mul_pd(a00, z0);     /* a00 = {Ai*Xr, Ar*Xi} */
      y0i = _mm_add_pd(y0i, a00);    /* y0i = {yi+Ai*xr, yi+Ar*Xi} */
      a01 = _mm_mul_pd(a01, z0);     /* a01 = {Ai*Xr, Ar*Xi} */
      y1i = _mm_add_pd(y1i, a01);    /* y1i = {yi+Ai*xr, yi+Ar*Xi} */
      a02 = _mm_mul_pd(a02, z0);     /* a02 = {Ai*Xr, Ar*Xi} */
      y2i = _mm_add_pd(y2i, a02);    /* y2i = {yi+Ai*xr, yi+Ar*Xi} */
      a03 = _mm_mul_pd(a03, z0);     /* a03 = {Ai*Xr, Ar*Xi} */
      y3i = _mm_add_pd(y3i, a03);    /* y3i = {yi+Ai*xr, yi+Ar*Xi} */

      y0r = _mm_mul_pd(y0r, rscal);
//      y0r = _mm_hadd_pd(y0r, y0i);
      _my_hadd_pd(y0r, y0i);
      _mm_store_pd(Y, y0r);
      y1r = _mm_mul_pd(y1r, rscal);
//      y1r = _mm_hadd_pd(y1r, y1i);
      _my_hadd_pd(y1r, y1i);
      _mm_store_pd(Y+2, y1r);
      y2r = _mm_mul_pd(y2r, rscal);
//      y2r = _mm_hadd_pd(y2r, y2i);
      _my_hadd_pd(y2r, y2i);
      _mm_store_pd(Y+4, y2r);
      y3r = _mm_mul_pd(y3r, rscal);
//      y3r = _mm_hadd_pd(y3r, y3i);
      _my_hadd_pd(y3r, y3i);
      _mm_store_pd(Y+6, y3r);
   }
}
@ROUT ATL_sgemvN_8x4_sse
@extract -punymacs -b @(topd)/cw.inc lang=C -def cwdate 2010
#include <xmmintrin.h>
#include <stdio.h>
#include "atlas_misc.h"

void ATL_UGEMV(ATL_CINT M, ATL_CINT N, const TYPE *A, ATL_CINT lda1,
               const TYPE *X, TYPE *Y)
{/* BEGIN GEMVN: nMU=1, MU=8, NU=4 */
   ATL_INT i, j;
   ATL_CINT MAp = ( (((((size_t)A)+15)>>4)<<4) - ((size_t)A) )/sizeof(TYPE);
   ATL_CINT MA=M-MAp;
   ATL_CINT M8=((MA/8)*8)+MAp, N4=((N/4)*4), lda2=lda1+lda1, lda3=lda2+lda1, lda4=lda3+lda1;
   __m128 y0, y1, y2, y3, y4, y5, y6, y7, x0, x1, x2, x3, a0_0, m0_0, a1_0, m1_0, a2_0, m2_0, a3_0, m3_0, a4_0, m4_0, a5_0, m5_0, a6_0, m6_0, a7_0, m7_0, a0_1, m0_1, a1_1, m1_1, a2_1, m2_1, a3_1, m3_1, a4_1, m4_1, a5_1, m5_1, a6_1, m6_1, a7_1, m7_1, a0_2, m0_2, a1_2, m1_2, a2_2, m2_2, a3_2, m3_2, a4_2, m4_2, a5_2, m5_2, a6_2, m6_2, a7_2, m7_2, a0_3, m0_3, a1_3, m1_3, a2_3, m2_3, a3_3, m3_3, a4_3, m4_3, a5_3, m5_3, a6_3, m6_3, a7_3, m7_3;

   for (j=0; j < N4; j += 4, A += lda4, X += 4)
   {/* BEGIN N-LOOP UR=4 */
      x0 = _mm_load1_ps(X);
      x1 = _mm_load1_ps(X+1);
      x2 = _mm_load1_ps(X+2);
      x3 = _mm_load1_ps(X+3);
      for (i=0; i < MAp; i++)
      {/* peel to force X/A alignment */
         y0 = _mm_load_ss(Y+i);
         a0_0 =_mm_load_ss(A+i);
         a0_0 = _mm_mul_ss(a0_0, x0);
         y0 = _mm_add_ss(y0, a0_0);
         a0_1 =_mm_load_ss(A+i+lda1);
         a0_1 = _mm_mul_ss(a0_1, x1);
         y0 = _mm_add_ss(y0, a0_1);
         a0_2 =_mm_load_ss(A+i+lda2);
         a0_2 = _mm_mul_ss(a0_2, x2);
         y0 = _mm_add_ss(y0, a0_2);
         a0_3 =_mm_load_ss(A+i+lda3);
         a0_3 = _mm_mul_ss(a0_3, x3);
         y0 = _mm_add_ss(y0, a0_3);
         _mm_store_ss(Y+i, y0);
      } /* end force-align peel */

      for (i=MAp; i < M8; i += 8)
      {/* ----- BEGIN M-LOOP BODY ----- */
         /* --- BEGIN MUxNU UNROLL 0 --- */
         y0 = _mm_load_ps(Y+i+0);
         a0_0 = _mm_load_ps(A+i+0);
         a0_0 = _mm_mul_ps(a0_0, x0);
         y0 = _mm_add_ps(y0, a0_0);
         y4 = _mm_load_ps(Y+i+4);
         a4_0 = _mm_load_ps(A+i+4);
         a4_0 = _mm_mul_ps(a4_0, x0);
         y4 = _mm_add_ps(y4, a4_0);

         a0_1 = _mm_load_ps(A+i+lda1);
         a0_1 = _mm_mul_ps(a0_1, x1);
         y0 = _mm_add_ps(y0, a0_1);
         a4_1 = _mm_load_ps(A+i+4+lda1);
         a4_1 = _mm_mul_ps(a4_1, x1);
         y4 = _mm_add_ps(y4, a4_1);

         a0_2 = _mm_load_ps(A+i+lda2);
         a0_2 = _mm_mul_ps(a0_2, x2);
         y0 = _mm_add_ps(y0, a0_2);
         a4_2 = _mm_load_ps(A+i+4+lda2);
         a4_2 = _mm_mul_ps(a4_2, x2);
         y4 = _mm_add_ps(y4, a4_2);

         a0_3 = _mm_load_ps(A+i+lda3);
         a0_3 = _mm_mul_ps(a0_3, x3);
         y0 = _mm_add_ps(y0, a0_3);
         a4_3 = _mm_load_ps(A+i+4+lda3);
         a4_3 = _mm_mul_ps(a4_3, x3);
         y4 = _mm_add_ps(y4, a4_3);

         _mm_store_ps(Y+i, y0);
         _mm_store_ps(Y+i+4, y4);
         /* --- END MUxNU UNROLL 0 --- */
      }/* ----- END M-LOOP BODY ----- */
      if (M != M8)
      {/* ----- BEGIN VECTOR UNROLL M CLEANUP ----- */
         for (i=M8; i < M; i++)
         {/* ----- BEGIN SCALAR M CLEANUP ----- */
         y0 = _mm_load_ss(Y+i);
         a0_0 =_mm_load_ss(A+i);
         a0_0 = _mm_mul_ss(a0_0, x0);
         y0 = _mm_add_ss(y0, a0_0);
         a0_1 =_mm_load_ss(A+i+lda1);
         a0_1 = _mm_mul_ss(a0_1, x1);
         y0 = _mm_add_ss(y0, a0_1);
         a0_2 =_mm_load_ss(A+i+lda2);
         a0_2 = _mm_mul_ss(a0_2, x2);
         y0 = _mm_add_ss(y0, a0_2);
         a0_3 =_mm_load_ss(A+i+lda3);
         a0_3 = _mm_mul_ss(a0_3, x3);
         y0 = _mm_add_ss(y0, a0_3);
         _mm_store_ss(Y+i, y0);
         }/* ----- END SCALAR M CLEANUP ----- */
      }/* ----- END VECTOR UNROLL M CLEANUP ----- */
   }/* END N-LOOP UR=4 */

   for (j=N4; j < N; j += 1, A += lda1, X++)
   {/* BEGIN N-LOOP UR=1 */
      x0 = _mm_load1_ps(X);
      for (i=0; i < MAp; i++)
      {/* peel to force X/A alignment */
         y0 = _mm_load_ss(Y+i);
         a0_0 =_mm_load_ss(A+i);
         a0_0 = _mm_mul_ss(a0_0, x0);
         y0 = _mm_add_ss(y0, a0_0);
         _mm_store_ss(Y+i, y0);
      } /* end force-align peel */

      for (i=MAp; i < M8; i += 8)
      {/* ----- BEGIN M-LOOP BODY ----- */
         /* --- BEGIN MUxNU UNROLL 0 --- */
         y0 = _mm_load_ps(Y+i+0);
         a0_0 = _mm_load_ps(A+i+0);
         a0_0 = _mm_mul_ps(a0_0, x0);
         y0 = _mm_add_ps(y0, a0_0);
         y4 = _mm_load_ps(Y+i+4);
         a4_0 = _mm_load_ps(A+i+4);
         a4_0 = _mm_mul_ps(a4_0, x0);
         y4 = _mm_add_ps(y4, a4_0);
         _mm_store_ps(Y+i, y0);
         _mm_store_ps(Y+i+4, y4);
         /* --- END MUxNU UNROLL 0 --- */
      }/* ----- END M-LOOP BODY ----- */
      if (M != M8)
      {/* ----- BEGIN VECTOR UNROLL M CLEANUP ----- */
         for (i=M8; i < M; i++)
         {/* ----- BEGIN SCALAR M CLEANUP ----- */
         y0 = _mm_load_ss(Y+i);
         a0_0 =_mm_load_ss(A+i);
         a0_0 = _mm_mul_ss(a0_0, x0);
         y0 = _mm_add_ss(y0, a0_0);
         _mm_store_ss(Y+i, y0);
         }/* ----- END SCALAR M CLEANUP ----- */
      }/* ----- END VECTOR UNROLL M CLEANUP ----- */
   }/* END N-LOOP UR=1 */
}/* END GER: nMU=1, MU=8, NU=4 */
#ifdef MA
   #undef MA
#endif
#ifdef MAp
   #undef MAp
#endif
@ROUT ATL_cgerk_8x4_sse3
@extract -punymacs -b @(topd)/cw.inc lang=C -def cwdate 2010
#include "atlas_asm.h"
/*
 * This file does a 1x4 unrolled r1_sse with these params:
 *    CL=8, ORDER=clmajor
 */
#ifndef ATL_GAS_x8664
   #error "This kernel requires x86-64 assembly!"
#endif
/*
 * Integer register assignment
 */
#define M       %rdi
#define N       %rsi
#define pA0     %r8
#define lda     %rax
#define pX      %rdx
#define pY      %r9
#define II      %rbx
#define pX0     %r11
#define Mr      %rcx
#define incAXm  %r10
#define incII   %r15
#define incAn   %r14
#define lda3    %r12
#define Ma      %r13
/*
 * SSE register assignment
 */
#define rXr     %xmm0
#define rXi     %xmm1
#define ra0     %xmm2
#define rA0     %xmm3
#define rY0      %xmm4
#define rYh0     %xmm5
#define rY1      %xmm6
#define rYh1     %xmm7
#define rY2      %xmm8
#define rYh2     %xmm9
#define rY3      %xmm10
#define rYh3     %xmm11
#define NONEPONEOFF -72
#define rneg %xmm15

/*
 * macros
 */
#ifndef MOVA
   #define MOVA movaps
#endif
#define movapd movaps
#define movupd movups
#define xorpd xorps
#define addpd addps
#define mulpd mulps
#define addsd addss
#define mulsd mulss
#define movsd movss
#define haddpd haddps
/*
 * Define macros controlling prefetch
 */
#ifndef PFDIST
   #define PFDIST 256
#endif
#ifndef PFADIST
   #define PFADIST 0
#endif
#ifndef PFYDIST
   #define PFYDIST 64
#endif
#ifndef PFXDIST
   #define PFXDIST 64
#endif
#ifndef PFIY
   #define PFIY prefetchnta
#endif
#ifndef PFIA
   #ifdef ATL_3DNow
      #define PFIA prefetchw
   #else
      #define PFIA prefetcht0
   #endif
#endif
#if PFADIST == 0                /* flag for no prefetch */
   #define prefA(mem)
#else
   #define prefA(mem) PFIA mem
#endif
#if PFYDIST == 0                /* flag for no prefetch */
   #define prefY(mem)
#else
   #define prefY(mem) PFIY mem
#endif
#if PFXDIST == 0                /* flag for no prefetch */
   #define prefX(mem)
#else
   #define prefX(mem) PFIX mem
#endif
.text
/*
 *                      %rdi        %rsi           %rdx          %rcx
 * void ATL_UGERK(ATL_CINT M, ATL_CINT N, const TYPE *X, const TYPE *Y,
 *                    %r8      %r9
 *                TYPE *A, ATL_CINT lda)
 */
.text
.global ATL_asmdecor(ATL_UGERK)
ALIGN64
ATL_asmdecor(ATL_UGERK):

/*
 * Save callee-saved iregs
 */
   movq %rbp, -8(%rsp)
   movq %rbx, -16(%rsp)
   movq %r12, -24(%rsp)
   movq %r13, -32(%rsp)
   movq %r14, -40(%rsp)
   movq %r15, -48(%rsp)
/*
 * Compute if we peel or not.  This kernel asserts that lda is a multiple
 * of 16 bytes, and that A is at least 8-byte aligned, which means we must
 * either peel A on iteration or none, depending on alignment
 *
 * Compute M = (M/MU)*MU, Mr = M - (M/MU)*MU
 * NOTE: Mr is %rcx reg, so we can use jcx to go to cleanup loop
 */
   mov  %r9, lda        /* move lda to assigned register, rax */
   mov  %rcx, pY        /* move pY to assigned register, r9 */
   mov  $1, Mr          /* setup assignment to peel */
   xor  Ma, Ma          /* default to no peel */
   test $0xF, pA0       /* 0 if 16-byte aligned */
   cmovnz Mr, Ma        /* if nonzero, say need 1 iteration peel */
   sub  Ma, M
   mov  M, Mr           /* Mr = M */
   shr $3, M            /* M = M / MU */
   shl $3, M            /* M = (M/MU)*MU */
   sub M, Mr            /* Mr = M - (M/MU)*MU */
/*
 * Construct nonepone = {1.0,-1.0,1.0,-1.0}
 */
   finit
   fld1                                 /* ST =  1.0 */
   fldz                                 /* ST =  0.0 1.0 */
   fsub %st(1), %st                     /* ST = -1.0 1.0 */
   fsts NONEPONEOFF(%rsp)               /* ST= -1.0 1.0 */
   fstps NONEPONEOFF+8(%rsp)            /* ST=1.0 */
   fsts NONEPONEOFF+4(%rsp)             /* ST=1.0 */
   fstps NONEPONEOFF+12(%rsp)          /* ST=NULL, mem={1.0, -1.0, 1.0, -1.0}*/
   movaps NONEPONEOFF(%rsp), rneg
/*
 * Setup constants
 */
   mov lda, incAn       /* incAn = lda */
   sub M, incAn         /* incAn = lda - (M/MU)*MU */
   sub Ma, incAn        
   sub Mr, incAn        /* incAn = lda - M */
   shl $3, incAn        /* incAn = (lda-M)*sizeof */
   shl $3, lda          /* lda *= sizeof */
   sub $-128, pA0       /* code compaction by using signed 1-byte offsets */
   sub $-128, pX        /* code compaction by using signed 1-byte offsets */
   mov pX, pX0          /* save for restore after M loops */
   mov $-64, incAXm     /* code comp: use reg rather than constant */
   lea (lda, lda,2), lda3       /* lda3 = 3*lda */
   lea (incAn, lda3), incAn     /* incAn = (4*lda-M)*sizeof */
   mov $8*1, incII      /* code comp: use reg rather than constant */
   mov M, II

   ALIGN32
   LOOPN:
      movlps 0*8(pY), rY0   /* rY0 = {xx,xx, Y0i, Y0r} */
      movlhps rY0, rY0      /* rY0 = {Y0i, Y0r, Y0i, Y0r} */
      pshufd $0x11, rY0, rYh0 /* rYh0 = {Y0r, Y0i, Y0r, Y0i} */
      mulps rneg, rYh0  /* rYh0 = {Y0r,-Y0i, Y0r,-Y0i} */
      movlps 1*8(pY), rY1   /* rY1 = {xx,xx, Y1i, Y1r} */
      movlhps rY1, rY1      /* rY1 = {Y1i, Y1r, Y1i, Y1r} */
      pshufd $0x11, rY1, rYh1 /* rYh1 = {Y1r, Y1i, Y1r, Y1i} */
      mulps rneg, rYh1  /* rYh1 = {Y1r,-Y1i, Y1r,-Y1i} */
      movlps 2*8(pY), rY2   /* rY2 = {xx,xx, Y2i, Y2r} */
      movlhps rY2, rY2      /* rY2 = {Y2i, Y2r, Y2i, Y2r} */
      pshufd $0x11, rY2, rYh2 /* rYh2 = {Y2r, Y2i, Y2r, Y2i} */
      mulps rneg, rYh2  /* rYh2 = {Y2r,-Y2i, Y2r,-Y2i} */
      movlps 3*8(pY), rY3   /* rY3 = {xx,xx, Y3i, Y3r} */
      movlhps rY3, rY3      /* rY3 = {Y3i, Y3r, Y3i, Y3r} */
      pshufd $0x11, rY3, rYh3 /* rYh3 = {Y3r, Y3i, Y3r, Y3i} */
      mulps rneg, rYh3  /* rYh3 = {Y3r,-Y3i, Y3r,-Y3i} */
/*
 *    If no peeled iteration, start M-loop, else do peeled iteration
 */
      bt $0, Ma
      jnc LOOPM
         movlps -128(pX), rXi           /* rXr = {0, 0, X0i, X0r} */
         pshufd $0xE0, rXi, rXr         /* rXr = {0, 0, X0r, X0r} */
         movaps rY0, rA0                /* rA0 = {Y0i, Y0r, Y0i, Y0r} */
         mulps  rXr, rA0                /* rA0 = {0, 0, X0r*Y0i, X0r*Y0r} */
         movlps -128(pA0), ra0
         addps ra0, rA0
         shufps $0xE5, rXi, rXi         /* rXi = {0, 0, X0i, X0i} */
         movaps rYh0, ra0               /* ra0 = {Y0r, -Y0i, Y0r,-Y0i} */
         mulps  rXi, ra0                /* ra0 = {0, 0, X0i*Y0r,-X0i*Y0i} */
         addps  ra0, rA0
         movlps rA0, -128(pA0)
         movlps -128(pX), rXi           /* rXr = {0, 0, X0i, X0r} */
         pshufd $0xE0, rXi, rXr         /* rXr = {0, 0, X0r, X0r} */
         movaps rY1, rA0                /* rA0 = {Y1i, Y1r, Y1i, Y1r} */
         mulps  rXr, rA0                /* rA0 = {0, 0, X0r*Y1i, X0r*Y1r} */
         movlps -128(pA0,lda), ra0
         addps ra0, rA0
         shufps $0xE5, rXi, rXi         /* rXi = {0, 0, X0i, X0i} */
         movaps rYh1, ra0               /* ra0 = {Y1r, -Y1i, Y1r,-Y1i} */
         mulps  rXi, ra0                /* ra0 = {0, 0, X0i*Y0r,-X0i*Y0i} */
         addps  ra0, rA0
         movlps rA0, -128(pA0,lda)
         movlps -128(pX), rXi           /* rXr = {0, 0, X0i, X0r} */
         pshufd $0xE0, rXi, rXr         /* rXr = {0, 0, X0r, X0r} */
         movaps rY2, rA0                /* rA0 = {Y2i, Y2r, Y2i, Y2r} */
         mulps  rXr, rA0                /* rA0 = {0, 0, X0r*Y2i, X0r*Y2r} */
         movlps -128(pA0,lda,2), ra0
         addps ra0, rA0
         shufps $0xE5, rXi, rXi         /* rXi = {0, 0, X0i, X0i} */
         movaps rYh2, ra0               /* ra0 = {Y2r, -Y2i, Y2r,-Y2i} */
         mulps  rXi, ra0                /* ra0 = {0, 0, X0i*Y0r,-X0i*Y0i} */
         addps  ra0, rA0
         movlps rA0, -128(pA0,lda,2)
         movlps -128(pX), rXi           /* rXr = {0, 0, X0i, X0r} */
         pshufd $0xE0, rXi, rXr         /* rXr = {0, 0, X0r, X0r} */
         movaps rY3, rA0                /* rA0 = {Y3i, Y3r, Y3i, Y3r} */
         mulps  rXr, rA0                /* rA0 = {0, 0, X0r*Y3i, X0r*Y3r} */
         movlps -128(pA0,lda3), ra0
         addps ra0, rA0
         shufps $0xE5, rXi, rXi         /* rXi = {0, 0, X0i, X0i} */
         movaps rYh3, ra0               /* ra0 = {Y3r, -Y3i, Y3r,-Y3i} */
         mulps  rXi, ra0                /* ra0 = {0, 0, X0i*Y0r,-X0i*Y0i} */
         addps  ra0, rA0
         movlps rA0, -128(pA0,lda3)
         add $8, pX
         add $8, pA0
      LOOPM:
         movsldup 0-128(pX), rXr /* rXr = { X1r, X1r, X0r, X0r} */
         movaps rY0, rA0   /* rA0 = {Y0i, Y0r,Y0i, Y0r} */
         mulps  rXr, rA0   /* rA0 = {X1r*Y0i,X1r*Y0r,X0r*Y0i,X0r*Y0r} */
         addps  0-128(pA0), rA0
         movshdup 0-128(pX), rXi /* rXi = {X1i, X1i, X0i, X0i} */
         movaps rYh0, ra0  /* ra0 = {Y0r,-Y0i,Y0r,-Y0i} */
         mulps  rXi, ra0   /* ra0 = {X1i*Y0r,-X1i*Y0i,X0i*Y0r,-X0i*Y0i} */
         addps  ra0, rA0
         MOVA   rA0, 0-128(pA0)
         prefA(PFADIST+0(pA0))
         prefA(PFADIST+0(pA0,lda))
         movaps rY1, rA0   /* rA0 = {Y1i, Y1r,Y1i, Y1r} */
         mulps  rXr, rA0   /* rA0 = {X1r*Y1i,X1r*Y1r,X0r*Y1i,X0r*Y1r} */
         addps  0-128(pA0,lda), rA0
         movaps rYh1, ra0  /* ra0 = {Y1r,-Y1i,Y1r,-Y1i} */
         mulps  rXi, ra0   /* ra0 = {X1i*Y0r,-X1i*Y0i,X0i*Y0r,-X0i*Y0i} */
         addps  ra0, rA0
         MOVA   rA0, 0-128(pA0,lda)
         prefA(PFADIST+0(pA0,lda,2))
         movaps rY2, rA0   /* rA0 = {Y2i, Y2r,Y2i, Y2r} */
         mulps  rXr, rA0   /* rA0 = {X1r*Y2i,X1r*Y2r,X0r*Y2i,X0r*Y2r} */
         addps  0-128(pA0,lda,2), rA0
         movaps rYh2, ra0  /* ra0 = {Y2r,-Y2i,Y2r,-Y2i} */
         mulps  rXi, ra0   /* ra0 = {X1i*Y0r,-X1i*Y0i,X0i*Y0r,-X0i*Y0i} */
         addps  ra0, rA0
         MOVA   rA0, 0-128(pA0,lda,2)
         prefA(PFADIST+0(pA0,lda3))
         movaps rY3, rA0   /* rA0 = {Y3i, Y3r,Y3i, Y3r} */
         mulps  rXr, rA0   /* rA0 = {X1r*Y3i,X1r*Y3r,X0r*Y3i,X0r*Y3r} */
         addps  0-128(pA0,lda3), rA0
         movaps rYh3, ra0  /* ra0 = {Y3r,-Y3i,Y3r,-Y3i} */
         mulps  rXi, ra0   /* ra0 = {X1i*Y0r,-X1i*Y0i,X0i*Y0r,-X0i*Y0i} */
         addps  ra0, rA0
         MOVA   rA0, 0-128(pA0,lda3)

         movsldup 16-128(pX), rXr /* rXr = { X1r, X1r, X0r, X0r} */
         movaps rY0, rA0   /* rA0 = {Y0i, Y0r,Y0i, Y0r} */
         mulps  rXr, rA0   /* rA0 = {X1r*Y0i,X1r*Y0r,X0r*Y0i,X0r*Y0r} */
         addps  16-128(pA0), rA0
         movshdup 16-128(pX), rXi /* rXi = {X1i, X1i, X0i, X0i} */
         movaps rYh0, ra0  /* ra0 = {Y0r,-Y0i,Y0r,-Y0i} */
         mulps  rXi, ra0   /* ra0 = {X1i*Y0r,-X1i*Y0i,X0i*Y0r,-X0i*Y0i} */
         addps  ra0, rA0
         MOVA   rA0, 16-128(pA0)
         movaps rY1, rA0   /* rA0 = {Y1i, Y1r,Y1i, Y1r} */
         mulps  rXr, rA0   /* rA0 = {X1r*Y1i,X1r*Y1r,X0r*Y1i,X0r*Y1r} */
         addps  16-128(pA0,lda), rA0
         movaps rYh1, ra0  /* ra0 = {Y1r,-Y1i,Y1r,-Y1i} */
         mulps  rXi, ra0   /* ra0 = {X1i*Y0r,-X1i*Y0i,X0i*Y0r,-X0i*Y0i} */
         addps  ra0, rA0
         MOVA   rA0, 16-128(pA0,lda)
         movaps rY2, rA0   /* rA0 = {Y2i, Y2r,Y2i, Y2r} */
         mulps  rXr, rA0   /* rA0 = {X1r*Y2i,X1r*Y2r,X0r*Y2i,X0r*Y2r} */
         addps  16-128(pA0,lda,2), rA0
         movaps rYh2, ra0  /* ra0 = {Y2r,-Y2i,Y2r,-Y2i} */
         mulps  rXi, ra0   /* ra0 = {X1i*Y0r,-X1i*Y0i,X0i*Y0r,-X0i*Y0i} */
         addps  ra0, rA0
         MOVA   rA0, 16-128(pA0,lda,2)
         movaps rY3, rA0   /* rA0 = {Y3i, Y3r,Y3i, Y3r} */
         mulps  rXr, rA0   /* rA0 = {X1r*Y3i,X1r*Y3r,X0r*Y3i,X0r*Y3r} */
         addps  16-128(pA0,lda3), rA0
         movaps rYh3, ra0  /* ra0 = {Y3r,-Y3i,Y3r,-Y3i} */
         mulps  rXi, ra0   /* ra0 = {X1i*Y0r,-X1i*Y0i,X0i*Y0r,-X0i*Y0i} */
         addps  ra0, rA0
         MOVA   rA0, 16-128(pA0,lda3)

         movsldup 32-128(pX), rXr /* rXr = { X1r, X1r, X0r, X0r} */
         movaps rY0, rA0   /* rA0 = {Y0i, Y0r,Y0i, Y0r} */
         mulps  rXr, rA0   /* rA0 = {X1r*Y0i,X1r*Y0r,X0r*Y0i,X0r*Y0r} */
         addps  32-128(pA0), rA0
         movshdup 32-128(pX), rXi /* rXi = {X1i, X1i, X0i, X0i} */
         movaps rYh0, ra0  /* ra0 = {Y0r,-Y0i,Y0r,-Y0i} */
         mulps  rXi, ra0   /* ra0 = {X1i*Y0r,-X1i*Y0i,X0i*Y0r,-X0i*Y0i} */
         addps  ra0, rA0
         MOVA   rA0, 32-128(pA0)
         movaps rY1, rA0   /* rA0 = {Y1i, Y1r,Y1i, Y1r} */
         mulps  rXr, rA0   /* rA0 = {X1r*Y1i,X1r*Y1r,X0r*Y1i,X0r*Y1r} */
         addps  32-128(pA0,lda), rA0
         movaps rYh1, ra0  /* ra0 = {Y1r,-Y1i,Y1r,-Y1i} */
         mulps  rXi, ra0   /* ra0 = {X1i*Y0r,-X1i*Y0i,X0i*Y0r,-X0i*Y0i} */
         addps  ra0, rA0
         MOVA   rA0, 32-128(pA0,lda)
         movaps rY2, rA0   /* rA0 = {Y2i, Y2r,Y2i, Y2r} */
         mulps  rXr, rA0   /* rA0 = {X1r*Y2i,X1r*Y2r,X0r*Y2i,X0r*Y2r} */
         addps  32-128(pA0,lda,2), rA0
         movaps rYh2, ra0  /* ra0 = {Y2r,-Y2i,Y2r,-Y2i} */
         mulps  rXi, ra0   /* ra0 = {X1i*Y0r,-X1i*Y0i,X0i*Y0r,-X0i*Y0i} */
         addps  ra0, rA0
         MOVA   rA0, 32-128(pA0,lda,2)
         movaps rY3, rA0   /* rA0 = {Y3i, Y3r,Y3i, Y3r} */
         mulps  rXr, rA0   /* rA0 = {X1r*Y3i,X1r*Y3r,X0r*Y3i,X0r*Y3r} */
         addps  32-128(pA0,lda3), rA0
         movaps rYh3, ra0  /* ra0 = {Y3r,-Y3i,Y3r,-Y3i} */
         mulps  rXi, ra0   /* ra0 = {X1i*Y0r,-X1i*Y0i,X0i*Y0r,-X0i*Y0i} */
         addps  ra0, rA0
         MOVA   rA0, 32-128(pA0,lda3)

         movsldup 48-128(pX), rXr /* rXr = { X1r, X1r, X0r, X0r} */
         movaps rY0, rA0   /* rA0 = {Y0i, Y0r,Y0i, Y0r} */
         mulps  rXr, rA0   /* rA0 = {X1r*Y0i,X1r*Y0r,X0r*Y0i,X0r*Y0r} */
         addps  48-128(pA0), rA0
         movshdup 48-128(pX), rXi /* rXi = {X1i, X1i, X0i, X0i} */
         movaps rYh0, ra0  /* ra0 = {Y0r,-Y0i,Y0r,-Y0i} */
         mulps  rXi, ra0   /* ra0 = {X1i*Y0r,-X1i*Y0i,X0i*Y0r,-X0i*Y0i} */
         addps  ra0, rA0
         MOVA   rA0, 48-128(pA0)
         movaps rY1, rA0   /* rA0 = {Y1i, Y1r,Y1i, Y1r} */
         mulps  rXr, rA0   /* rA0 = {X1r*Y1i,X1r*Y1r,X0r*Y1i,X0r*Y1r} */
         addps  48-128(pA0,lda), rA0
         movaps rYh1, ra0  /* ra0 = {Y1r,-Y1i,Y1r,-Y1i} */
         mulps  rXi, ra0   /* ra0 = {X1i*Y0r,-X1i*Y0i,X0i*Y0r,-X0i*Y0i} */
         addps  ra0, rA0
         MOVA   rA0, 48-128(pA0,lda)
         movaps rY2, rA0   /* rA0 = {Y2i, Y2r,Y2i, Y2r} */
         mulps  rXr, rA0   /* rA0 = {X1r*Y2i,X1r*Y2r,X0r*Y2i,X0r*Y2r} */
         addps  48-128(pA0,lda,2), rA0
         movaps rYh2, ra0  /* ra0 = {Y2r,-Y2i,Y2r,-Y2i} */
         mulps  rXi, ra0   /* ra0 = {X1i*Y0r,-X1i*Y0i,X0i*Y0r,-X0i*Y0i} */
         addps  ra0, rA0
         MOVA   rA0, 48-128(pA0,lda,2)
         movaps rY3, rA0   /* rA0 = {Y3i, Y3r,Y3i, Y3r} */
         mulps  rXr, rA0   /* rA0 = {X1r*Y3i,X1r*Y3r,X0r*Y3i,X0r*Y3r} */
         addps  48-128(pA0,lda3), rA0
         movaps rYh3, ra0  /* ra0 = {Y3r,-Y3i,Y3r,-Y3i} */
         mulps  rXi, ra0   /* ra0 = {X1i*Y0r,-X1i*Y0i,X0i*Y0r,-X0i*Y0i} */
         addps  ra0, rA0
         MOVA   rA0, 48-128(pA0,lda3)

         sub incAXm, pX
         sub incAXm, pA0
      sub incII, II
      jnz LOOPM

      cmp $0, Mr
      jz  MCLEANED

      mov Mr, II
      xorps rXr, rXr
      movaps rXr, rXi
      xorps ra0, ra0
      LOOPMCU:
         movlps -128(pX), rXi           /* rXr = {0, 0, X0i, X0r} */
         pshufd $0xE0, rXi, rXr         /* rXr = {0, 0, X0r, X0r} */
         movaps rY0, rA0                /* rA0 = {Y0i, Y0r, Y0i, Y0r} */
         mulps  rXr, rA0                /* rA0 = {0, 0, X0r*Y0i, X0r*Y0r} */
         movlps -128(pA0), ra0
         addps ra0, rA0
         shufps $0xE5, rXi, rXi         /* rXi = {0, 0, X0i, X0i} */
         movaps rYh0, ra0               /* ra0 = {Y0r, -Y0i, Y0r,-Y0i} */
         mulps  rXi, ra0                /* ra0 = {0, 0, X0i*Y0r,-X0i*Y0i} */
         addps  ra0, rA0
         movlps rA0, -128(pA0)
         movlps -128(pX), rXi           /* rXr = {0, 0, X0i, X0r} */
         pshufd $0xE0, rXi, rXr         /* rXr = {0, 0, X0r, X0r} */
         movaps rY1, rA0                /* rA0 = {Y1i, Y1r, Y1i, Y1r} */
         mulps  rXr, rA0                /* rA0 = {0, 0, X0r*Y1i, X0r*Y1r} */
         movlps -128(pA0,lda), ra0
         addps ra0, rA0
         shufps $0xE5, rXi, rXi         /* rXi = {0, 0, X0i, X0i} */
         movaps rYh1, ra0               /* ra0 = {Y1r, -Y1i, Y1r,-Y1i} */
         mulps  rXi, ra0                /* ra0 = {0, 0, X0i*Y0r,-X0i*Y0i} */
         addps  ra0, rA0
         movlps rA0, -128(pA0,lda)
         movlps -128(pX), rXi           /* rXr = {0, 0, X0i, X0r} */
         pshufd $0xE0, rXi, rXr         /* rXr = {0, 0, X0r, X0r} */
         movaps rY2, rA0                /* rA0 = {Y2i, Y2r, Y2i, Y2r} */
         mulps  rXr, rA0                /* rA0 = {0, 0, X0r*Y2i, X0r*Y2r} */
         movlps -128(pA0,lda,2), ra0
         addps ra0, rA0
         shufps $0xE5, rXi, rXi         /* rXi = {0, 0, X0i, X0i} */
         movaps rYh2, ra0               /* ra0 = {Y2r, -Y2i, Y2r,-Y2i} */
         mulps  rXi, ra0                /* ra0 = {0, 0, X0i*Y0r,-X0i*Y0i} */
         addps  ra0, rA0
         movlps rA0, -128(pA0,lda,2)
         movlps -128(pX), rXi           /* rXr = {0, 0, X0i, X0r} */
         pshufd $0xE0, rXi, rXr         /* rXr = {0, 0, X0r, X0r} */
         movaps rY3, rA0                /* rA0 = {Y3i, Y3r, Y3i, Y3r} */
         mulps  rXr, rA0                /* rA0 = {0, 0, X0r*Y3i, X0r*Y3r} */
         movlps -128(pA0,lda3), ra0
         addps ra0, rA0
         shufps $0xE5, rXi, rXi         /* rXi = {0, 0, X0i, X0i} */
         movaps rYh3, ra0               /* ra0 = {Y3r, -Y3i, Y3r,-Y3i} */
         mulps  rXi, ra0                /* ra0 = {0, 0, X0i*Y0r,-X0i*Y0i} */
         addps  ra0, rA0
         movlps rA0, -128(pA0,lda3)
         add $8, pX
         add $8, pA0
      dec II
      jnz LOOPMCU

MCLEANED:
      prefY(4*8+PFYDIST(pY))
      add $4*8, pY
      add incAn, pA0
      mov pX0, pX
      mov M, II
   sub $4, N
   jnz LOOPN
/*
 * EPILOGUE: restore registers and return
 */
   movq -8(%rsp), %rbp
   movq -16(%rsp), %rbx
   movq -24(%rsp), %r12
   movq -32(%rsp), %r13
   movq -40(%rsp), %r14
   movq -48(%rsp), %r15
   ret
@ROUT ATL_sgerk_8x4_sse
@extract -punymacs -b @(topd)/cw.inc lang=C -def cwdate 2010
#include <xmmintrin.h>
#include <stdio.h>
#include "atlas_misc.h"

void ATL_UGERK
   (ATL_CINT M, ATL_CINT N, const TYPE *X, const TYPE *Y,
    TYPE *A, ATL_CINT lda1)
{/* BEGIN GER: nMU=1, MU=8, NU=4 */
   ATL_INT i, j;
   ATL_CINT MAp = ( (((((size_t)A)+15)>>4)<<4) - ((size_t)A) )/sizeof(TYPE);
   ATL_CINT MA=M-MAp;
   ATL_CINT M8=((MA/8)*8)+MAp, N4=((N/4)*4), lda2=lda1+lda1, lda3=lda2+lda1, lda4=lda3+lda1;
   __m128 x0, x1, x2, x3, x4, x5, x6, x7, y0, y1, y2, y3, a0_0, m0_0, a1_0, m1_0, a2_0, m2_0, a3_0, m3_0, a4_0, m4_0, a5_0, m5_0, a6_0, m6_0, a7_0, m7_0, a0_1, m0_1, a1_1, m1_1, a2_1, m2_1, a3_1, m3_1, a4_1, m4_1, a5_1, m5_1, a6_1, m6_1, a7_1, m7_1, a0_2, m0_2, a1_2, m1_2, a2_2, m2_2, a3_2, m3_2, a4_2, m4_2, a5_2, m5_2, a6_2, m6_2, a7_2, m7_2, a0_3, m0_3, a1_3, m1_3, a2_3, m2_3, a3_3, m3_3, a4_3, m4_3, a5_3, m5_3, a6_3, m6_3, a7_3, m7_3;

   for (j=0; j < N4; j += 4, A += lda4, Y += 4)
   {/* BEGIN N-LOOP UR=4 */
      y0 = _mm_load1_ps(Y);
      y1 = _mm_load1_ps(Y+1);
      y2 = _mm_load1_ps(Y+2);
      y3 = _mm_load1_ps(Y+3);
      for (i=0; i < MAp; i++)
      {/* peel to force X/A alignment */
         x0 = _mm_load_ss(X+i+0);
         a0_0 = _mm_load_ss(A+i+0);
         m0_0 = _mm_mul_ss(x0, y0);
         a0_0 = _mm_add_ss(a0_0, m0_0);
         _mm_store_ss(A+i+0, a0_0);
         a0_1 = _mm_load_ss(A+i+0+lda1);
         m0_1 = _mm_mul_ss(x0, y1);
         a0_1 = _mm_add_ss(a0_1, m0_1);
         _mm_store_ss(A+i+0+lda1, a0_1);
         a0_2 = _mm_load_ss(A+i+0+lda2);
         m0_2 = _mm_mul_ss(x0, y2);
         a0_2 = _mm_add_ss(a0_2, m0_2);
         _mm_store_ss(A+i+0+lda2, a0_2);
         a0_3 = _mm_load_ss(A+i+0+lda3);
         m0_3 = _mm_mul_ss(x0, y3);
         a0_3 = _mm_add_ss(a0_3, m0_3);
         _mm_store_ss(A+i+0+lda3, a0_3);
      } /* end force-align peel */

      for (i=MAp; i < M8; i += 8)
      {/* ----- BEGIN M-LOOP BODY ----- */
         /* --- BEGIN MUxNU UNROLL 0 --- */
         x0 = _mm_load_ps(X+i+0);
         a0_0 = _mm_load_ps(A+i+0);
         m0_0 = _mm_mul_ps(x0, y0);
         a0_0 = _mm_add_ps(a0_0, m0_0);
         _mm_store_ps(A+i+0, a0_0);
         x4 = _mm_load_ps(X+i+4);
         a4_0 = _mm_load_ps(A+i+4);
         m4_0 = _mm_mul_ps(x4, y0);
         a4_0 = _mm_add_ps(a4_0, m4_0);
         _mm_store_ps(A+i+4, a4_0);
         a0_1 = _mm_load_ps(A+i+0+lda1);
         m0_1 = _mm_mul_ps(x0, y1);
         a0_1 = _mm_add_ps(a0_1, m0_1);
         _mm_store_ps(A+i+0+lda1, a0_1);
         a4_1 = _mm_load_ps(A+i+4+lda1);
         m4_1 = _mm_mul_ps(x4, y1);
         a4_1 = _mm_add_ps(a4_1, m4_1);
         _mm_store_ps(A+i+4+lda1, a4_1);
         a0_2 = _mm_load_ps(A+i+0+lda2);
         m0_2 = _mm_mul_ps(x0, y2);
         a0_2 = _mm_add_ps(a0_2, m0_2);
         _mm_store_ps(A+i+0+lda2, a0_2);
         a4_2 = _mm_load_ps(A+i+4+lda2);
         m4_2 = _mm_mul_ps(x4, y2);
         a4_2 = _mm_add_ps(a4_2, m4_2);
         _mm_store_ps(A+i+4+lda2, a4_2);
         a0_3 = _mm_load_ps(A+i+0+lda3);
         m0_3 = _mm_mul_ps(x0, y3);
         a0_3 = _mm_add_ps(a0_3, m0_3);
         _mm_store_ps(A+i+0+lda3, a0_3);
         a4_3 = _mm_load_ps(A+i+4+lda3);
         m4_3 = _mm_mul_ps(x4, y3);
         a4_3 = _mm_add_ps(a4_3, m4_3);
         _mm_store_ps(A+i+4+lda3, a4_3);
         /* --- END MUxNU UNROLL 0 --- */
      }/* ----- END M-LOOP BODY ----- */
      if (M != M8)
      {/* ----- BEGIN VECTOR UNROLL M CLEANUP ----- */

         for (i=M8; i < M; i++)
         {/* ----- BEGIN SCALAR M CLEANUP ----- */
            x0 = _mm_load_ss(X+i+0);
            a0_0 = _mm_load_ss(A+i+0);
            m0_0 = _mm_mul_ss(x0, y0);
            a0_0 = _mm_add_ss(a0_0, m0_0);
            _mm_store_ss(A+i+0, a0_0);
            a0_1 = _mm_load_ss(A+i+0+lda1);
            m0_1 = _mm_mul_ss(x0, y1);
            a0_1 = _mm_add_ss(a0_1, m0_1);
            _mm_store_ss(A+i+0+lda1, a0_1);
            a0_2 = _mm_load_ss(A+i+0+lda2);
            m0_2 = _mm_mul_ss(x0, y2);
            a0_2 = _mm_add_ss(a0_2, m0_2);
            _mm_store_ss(A+i+0+lda2, a0_2);
            a0_3 = _mm_load_ss(A+i+0+lda3);
            m0_3 = _mm_mul_ss(x0, y3);
            a0_3 = _mm_add_ss(a0_3, m0_3);
            _mm_store_ss(A+i+0+lda3, a0_3);
         }/* ----- END SCALAR M CLEANUP ----- */
      }/* ----- END VECTOR UNROLL M CLEANUP ----- */
   }/* END N-LOOP UR=4 */

   for (j=N4; j < N; j += 1, A += lda1, Y++)
   {/* BEGIN N-LOOP UR=1 */
      y0 = _mm_load1_ps(Y);
      for (i=0; i < MAp; i++)
      {/* peel to force X/A alignment */
         x0 = _mm_load_ss(X+i+0);
         a0_0 = _mm_load_ss(A+i+0);
         m0_0 = _mm_mul_ss(x0, y0);
         a0_0 = _mm_add_ss(a0_0, m0_0);
         _mm_store_ss(A+i+0, a0_0);
      } /* end force-align peel */

      for (i=MAp; i < M8; i += 8)
      {/* ----- BEGIN M-LOOP BODY ----- */
         /* --- BEGIN MUxNU UNROLL 0 --- */
         x0 = _mm_load_ps(X+i+0);
         a0_0 = _mm_load_ps(A+i+0);
         m0_0 = _mm_mul_ps(x0, y0);
         a0_0 = _mm_add_ps(a0_0, m0_0);
         _mm_store_ps(A+i+0, a0_0);
         x4 = _mm_load_ps(X+i+4);
         a4_0 = _mm_load_ps(A+i+4);
         m4_0 = _mm_mul_ps(x4, y0);
         a4_0 = _mm_add_ps(a4_0, m4_0);
         _mm_store_ps(A+i+4, a4_0);
         /* --- END MUxNU UNROLL 0 --- */
      }/* ----- END M-LOOP BODY ----- */
      if (M != M8)
      {/* ----- BEGIN VECTOR UNROLL M CLEANUP ----- */
         for (i=M8; i < M; i++)
         {/* ----- BEGIN SCALAR M CLEANUP ----- */
            x0 = _mm_load_ss(X+i+0);
            a0_0 = _mm_load_ss(A+i+0);
            m0_0 = _mm_mul_ss(x0, y0);
            a0_0 = _mm_add_ss(a0_0, m0_0);
            _mm_store_ss(A+i+0, a0_0);
         }/* ----- END SCALAR M CLEANUP ----- */
      }/* ----- END VECTOR UNROLL M CLEANUP ----- */
   }/* END N-LOOP UR=1 */
}/* END GER: nMU=1, MU=8, NU=4 */
#ifdef MA
   #undef MA
#endif
#ifdef MAp
   #undef MAp
#endif
@ROUT ATL_dgerk_4x8_sse
@extract -punymacs -b @(topd)/cw.inc lang=C -def cwdate 2009 -def cwdate 2010
#include <xmmintrin.h>
#include <stdio.h>
#include "atlas_misc.h"

void ATL_UGERK
   (ATL_CINT M, ATL_CINT N, const TYPE *X, const TYPE *Y, 
    TYPE *A, ATL_CINT lda1)
{/* BEGIN GER: nMU=1, MU=8, NU=4 */
   ATL_INT i, j;
// ATL_CINT MAp = ( (((((size_t)A)+15)>>4)<<4) - (((((size_t)A))>>4)<<4) )/sizeof(TYPE);
   ATL_CINT MAp = ( ((size_t)A)&(15) ) / sizeof(TYPE);
   ATL_CINT MA=M-MAp;
   ATL_CINT M8=((MA/8)*8)+MAp, M2=((MA>>1)<<1)+MAp, N4=((N/4)*4), lda2=lda1+lda1, lda3=lda2+lda1, lda4=lda3+lda1;
   __m128d x0, x1, x2, x3, x4, x5, x6, x7, y0, y1, y2, y3, a0_0, m0_0, a1_0, m1_0, a2_0, m2_0, a3_0, m3_0, a4_0, m4_0, a5_0, m5_0, a6_0, m6_0, a7_0, m7_0, a0_1, m0_1, a1_1, m1_1, a2_1, m2_1, a3_1, m3_1, a4_1, m4_1, a5_1, m5_1, a6_1, m6_1, a7_1, m7_1, a0_2, m0_2, a1_2, m1_2, a2_2, m2_2, a3_2, m3_2, a4_2, m4_2, a5_2, m5_2, a6_2, m6_2, a7_2, m7_2, a0_3, m0_3, a1_3, m1_3, a2_3, m2_3, a3_3, m3_3, a4_3, m4_3, a5_3, m5_3, a6_3, m6_3, a7_3, m7_3;

   for (j=0; j < N4; j += 4, A += lda4, Y += 4)
   {/* BEGIN N-LOOP UR=4 */
      y0 = _mm_load1_pd(Y);
      y1 = _mm_load1_pd(Y+1);
      y2 = _mm_load1_pd(Y+2);
      y3 = _mm_load1_pd(Y+3);
      if (MAp)
      {/* peel to force X/A alignment */
         i=0;                                /* bug fix. */
         x0 = _mm_load_sd(X+i+0);
         a0_0 = _mm_load_sd(A+i+0);
         m0_0 = _mm_mul_sd(x0, y0);
         a0_0 = _mm_add_sd(a0_0, m0_0);
         _mm_store_sd(A+i+0, a0_0);
         a0_1 = _mm_load_sd(A+i+0+lda1);
         m0_1 = _mm_mul_sd(x0, y1);
         a0_1 = _mm_add_sd(a0_1, m0_1);
         _mm_store_sd(A+i+0+lda1, a0_1);
         a0_2 = _mm_load_sd(A+i+0+lda2);
         m0_2 = _mm_mul_sd(x0, y2);
         a0_2 = _mm_add_sd(a0_2, m0_2);
         _mm_store_sd(A+i+0+lda2, a0_2);
         a0_3 = _mm_load_sd(A+i+0+lda3);
         m0_3 = _mm_mul_sd(x0, y3);
         a0_3 = _mm_add_sd(a0_3, m0_3);
         _mm_store_sd(A+i+0+lda3, a0_3);
      } /* end force-align peel */

      for (i=MAp; i < M8; i += 8)
      {/* ----- BEGIN M-LOOP BODY ----- */
         /* --- BEGIN MUxNU UNROLL 0 --- */
         x0 = _mm_load_pd(X+i+0);
         a0_0 = _mm_load_pd(A+i+0);
         m0_0 = _mm_mul_pd(x0, y0);
         a0_0 = _mm_add_pd(a0_0, m0_0);
         _mm_store_pd(A+i+0, a0_0);
         x2 = _mm_load_pd(X+i+2);
         a2_0 = _mm_load_pd(A+i+2);
         m2_0 = _mm_mul_pd(x2, y0);
         a2_0 = _mm_add_pd(a2_0, m2_0);
         _mm_store_pd(A+i+2, a2_0);
         x4 = _mm_load_pd(X+i+4);
         a4_0 = _mm_load_pd(A+i+4);
         m4_0 = _mm_mul_pd(x4, y0);
         a4_0 = _mm_add_pd(a4_0, m4_0);
         _mm_store_pd(A+i+4, a4_0);
         x6 = _mm_load_pd(X+i+6);
         a6_0 = _mm_load_pd(A+i+6);
         m6_0 = _mm_mul_pd(x6, y0);
         a6_0 = _mm_add_pd(a6_0, m6_0);
         _mm_store_pd(A+i+6, a6_0);
         a0_1 = _mm_load_pd(A+i+0+lda1);
         m0_1 = _mm_mul_pd(x0, y1);
         a0_1 = _mm_add_pd(a0_1, m0_1);
         _mm_store_pd(A+i+0+lda1, a0_1);
         a2_1 = _mm_load_pd(A+i+2+lda1);
         m2_1 = _mm_mul_pd(x2, y1);
         a2_1 = _mm_add_pd(a2_1, m2_1);
         _mm_store_pd(A+i+2+lda1, a2_1);
         a4_1 = _mm_load_pd(A+i+4+lda1);
         m4_1 = _mm_mul_pd(x4, y1);
         a4_1 = _mm_add_pd(a4_1, m4_1);
         _mm_store_pd(A+i+4+lda1, a4_1);
         a6_1 = _mm_load_pd(A+i+6+lda1);
         m6_1 = _mm_mul_pd(x6, y1);
         a6_1 = _mm_add_pd(a6_1, m6_1);
         _mm_store_pd(A+i+6+lda1, a6_1);
         a0_2 = _mm_load_pd(A+i+0+lda2);
         m0_2 = _mm_mul_pd(x0, y2);
         a0_2 = _mm_add_pd(a0_2, m0_2);
         _mm_store_pd(A+i+0+lda2, a0_2);
         a2_2 = _mm_load_pd(A+i+2+lda2);
         m2_2 = _mm_mul_pd(x2, y2);
         a2_2 = _mm_add_pd(a2_2, m2_2);
         _mm_store_pd(A+i+2+lda2, a2_2);
         a4_2 = _mm_load_pd(A+i+4+lda2);
         m4_2 = _mm_mul_pd(x4, y2);
         a4_2 = _mm_add_pd(a4_2, m4_2);
         _mm_store_pd(A+i+4+lda2, a4_2);
         a6_2 = _mm_load_pd(A+i+6+lda2);
         m6_2 = _mm_mul_pd(x6, y2);
         a6_2 = _mm_add_pd(a6_2, m6_2);
         _mm_store_pd(A+i+6+lda2, a6_2);
         a0_3 = _mm_load_pd(A+i+0+lda3);
         m0_3 = _mm_mul_pd(x0, y3);
         a0_3 = _mm_add_pd(a0_3, m0_3);
         _mm_store_pd(A+i+0+lda3, a0_3);
         a2_3 = _mm_load_pd(A+i+2+lda3);
         m2_3 = _mm_mul_pd(x2, y3);
         a2_3 = _mm_add_pd(a2_3, m2_3);
         _mm_store_pd(A+i+2+lda3, a2_3);
         a4_3 = _mm_load_pd(A+i+4+lda3);
         m4_3 = _mm_mul_pd(x4, y3);
         a4_3 = _mm_add_pd(a4_3, m4_3);
         _mm_store_pd(A+i+4+lda3, a4_3);
         a6_3 = _mm_load_pd(A+i+6+lda3);
         m6_3 = _mm_mul_pd(x6, y3);
         a6_3 = _mm_add_pd(a6_3, m6_3);
         _mm_store_pd(A+i+6+lda3, a6_3);
         /* --- END MUxNU UNROLL 0 --- */
      }/* ----- END M-LOOP BODY ----- */
      if (M != M8)
      {/* ----- BEGIN VECTOR UNROLL M CLEANUP ----- */

         for (i=M8; i < M2; i += 2)
         {/* ----- BEGIN M-LOOP BODY ----- */
            /* --- BEGIN MUxNU UNROLL 0 --- */
            x0 = _mm_load_pd(X+i+0);
            a0_0 = _mm_load_pd(A+i+0);
            m0_0 = _mm_mul_pd(x0, y0);
            a0_0 = _mm_add_pd(a0_0, m0_0);
            _mm_store_pd(A+i+0, a0_0);
            a0_1 = _mm_load_pd(A+i+0+lda1);
            m0_1 = _mm_mul_pd(x0, y1);
            a0_1 = _mm_add_pd(a0_1, m0_1);
            _mm_store_pd(A+i+0+lda1, a0_1);
            a0_2 = _mm_load_pd(A+i+0+lda2);
            m0_2 = _mm_mul_pd(x0, y2);
            a0_2 = _mm_add_pd(a0_2, m0_2);
            _mm_store_pd(A+i+0+lda2, a0_2);
            a0_3 = _mm_load_pd(A+i+0+lda3);
            m0_3 = _mm_mul_pd(x0, y3);
            a0_3 = _mm_add_pd(a0_3, m0_3);
            _mm_store_pd(A+i+0+lda3, a0_3);
            /* --- END MUxNU UNROLL 0 --- */
         }/* ----- END M-LOOP BODY ----- */
         if (M != M2)
         {/* ----- BEGIN SCALAR M CLEANUP ----- */
            x0 = _mm_load_sd(X+i+0);
            a0_0 = _mm_load_sd(A+i+0);
            m0_0 = _mm_mul_sd(x0, y0);
            a0_0 = _mm_add_sd(a0_0, m0_0);
            _mm_store_sd(A+i+0, a0_0);
            a0_1 = _mm_load_sd(A+i+0+lda1);
            m0_1 = _mm_mul_sd(x0, y1);
            a0_1 = _mm_add_sd(a0_1, m0_1);
            _mm_store_sd(A+i+0+lda1, a0_1);
            a0_2 = _mm_load_sd(A+i+0+lda2);
            m0_2 = _mm_mul_sd(x0, y2);
            a0_2 = _mm_add_sd(a0_2, m0_2);
            _mm_store_sd(A+i+0+lda2, a0_2);
            a0_3 = _mm_load_sd(A+i+0+lda3);
            m0_3 = _mm_mul_sd(x0, y3);
            a0_3 = _mm_add_sd(a0_3, m0_3);
            _mm_store_sd(A+i+0+lda3, a0_3);
         }/* ----- END SCALAR M CLEANUP ----- */
      }/* ----- END VECTOR UNROLL M CLEANUP ----- */
   }/* END N-LOOP UR=4 */

   for (j=N4; j < N; j += 1, A += lda1, Y++)
   {/* BEGIN N-LOOP UR=1 */
      y0 = _mm_load1_pd(Y);
      if (MAp)
      {/* peel to force X/A alignment */
         i=0;                                /* bug fix. */
         x0 = _mm_load_sd(X+i+0);
         a0_0 = _mm_load_sd(A+i+0);
         m0_0 = _mm_mul_sd(x0, y0);
         a0_0 = _mm_add_sd(a0_0, m0_0);
         _mm_store_sd(A+i+0, a0_0);
      } /* end force-align peel */

      for (i=MAp; i < M8; i += 8)
      {/* ----- BEGIN M-LOOP BODY ----- */
         /* --- BEGIN MUxNU UNROLL 0 --- */
         x0 = _mm_load_pd(X+i+0);
         a0_0 = _mm_load_pd(A+i+0);
         m0_0 = _mm_mul_pd(x0, y0);
         a0_0 = _mm_add_pd(a0_0, m0_0);
         _mm_store_pd(A+i+0, a0_0);
         x2 = _mm_load_pd(X+i+2);
         a2_0 = _mm_load_pd(A+i+2);
         m2_0 = _mm_mul_pd(x2, y0);
         a2_0 = _mm_add_pd(a2_0, m2_0);
         _mm_store_pd(A+i+2, a2_0);
         x4 = _mm_load_pd(X+i+4);
         a4_0 = _mm_load_pd(A+i+4);
         m4_0 = _mm_mul_pd(x4, y0);
         a4_0 = _mm_add_pd(a4_0, m4_0);
         _mm_store_pd(A+i+4, a4_0);
         x6 = _mm_load_pd(X+i+6);
         a6_0 = _mm_load_pd(A+i+6);
         m6_0 = _mm_mul_pd(x6, y0);
         a6_0 = _mm_add_pd(a6_0, m6_0);
         _mm_store_pd(A+i+6, a6_0);
         /* --- END MUxNU UNROLL 0 --- */
      }/* ----- END M-LOOP BODY ----- */
      if (M != M8)
      {/* ----- BEGIN VECTOR UNROLL M CLEANUP ----- */

         for (i=M8; i < M2; i += 2)
         {/* ----- BEGIN M-LOOP BODY ----- */
            /* --- BEGIN MUxNU UNROLL 0 --- */
            x0 = _mm_load_pd(X+i+0);
            a0_0 = _mm_load_pd(A+i+0);
            m0_0 = _mm_mul_pd(x0, y0);
            a0_0 = _mm_add_pd(a0_0, m0_0);
            _mm_store_pd(A+i+0, a0_0);
            /* --- END MUxNU UNROLL 0 --- */
         }/* ----- END M-LOOP BODY ----- */
         if (M != M2)
         {/* ----- BEGIN SCALAR M CLEANUP ----- */
            x0 = _mm_load_sd(X+i+0);
            a0_0 = _mm_load_sd(A+i+0);
            m0_0 = _mm_mul_sd(x0, y0);
            a0_0 = _mm_add_sd(a0_0, m0_0);
            _mm_store_sd(A+i+0, a0_0);
         }/* ----- END SCALAR M CLEANUP ----- */
      }/* ----- END VECTOR UNROLL M CLEANUP ----- */
   }/* END N-LOOP UR=1 */
}/* END GER: nMU=1, MU=8, NU=4 */
#ifdef MA
   #undef MA
#endif
#ifdef MAp
   #undef MAp
#endif
@ROUT ATL_cgerk_2x1p
@multidef cwdate 1999 2009 2010
@extract -punymacs -b @(topd)/cw.inc lang=C
#include "atlas_misc.h"
#include "atlas_lvl2.h"
#include "atlas_prefetch.h"

void ATL_UGERK
   (ATL_CINT M, ATL_CINT N, const TYPE *X, const TYPE *Y, TYPE *A, ATL_CINT lda)
{
   ATL_CINT mr = M - ((M>>1)<<1);
   ATL_CINT incA = ((lda-M+mr)<<1), lda2=lda+lda;
   const TYPE *stY = Y + N+N, *stX = X + ((M>>1)<<2), *x;
   register TYPE ry, iy, rx, ix, ra, ia;

   if ( N > 0)
   {
      if (M > 1)
      {
         do
         {
            #ifdef Conj_
               ry = *Y;
               iy = -Y[1];
            #else
               ry = *Y;
               iy = Y[1];
            #endif
            x = X;
            do
            {
               rx = *x; ix = x[1];
               ra = *A; ia = A[1];
               ra += rx * ry; ATL_pfl1W(A+16);
               ia += rx * iy;
               ra -= ix * iy;
               ia += ix * ry;
               *A = ra;
               A[1] = ia;
               rx = x[2]; ix = x[3]; x += 4;
               ra = A[2]; ia = A[3];
               ra += rx * ry;
               ia += rx * iy;
               ra -= ix * iy;
               ia += ix * ry;
               A[2] = ra;
               A[3] = ia; A += 4;
            }
            while (x != stX);
            if (!mr) goto L1;
            else
            {
               rx = *x; ix = x[1];
               ra = *A; ia = A[1];
               ra += rx * ry;
               ra -= ix * iy;
               ia += rx * iy;
               ia += ix * ry;
               *A = ra;
               A[1] = ia;
            }
L1:         Y += 2;
            A += incA;
         }
         while (Y != stY);
      }
      else if (M == 1) 
      {
         #ifdef Conj_
            rx = *X; ix = X[1];
            do
            {
               ry = *Y; iy = Y[1];
               ra = *A; ia = A[1];
               ra += rx * ry;
               ia -= rx * iy;
               ra += ix * iy;
               ia += ix * ry;
               *A = ra;
               A[1] = ia;
               Y += 2;
               A += lda2;
            }
            while (Y != stY);
         #else
            Mjoin(PATL,axpy)(N, X, Y, 1, A, lda);
         #endif
      }
   }
}
@ROUT ATL_cgerk_axpy
@multidef cwdate 1999 2009 2010
@extract -punymacs -b @(topd)/cw.inc lang=C
#include "atlas_misc.h"
#include "atlas_lvl2.h"

void ATL_UGERK(ATL_CINT M, ATL_CINT N, const TYPE *X, const TYPE *Y, 
               TYPE *A, ATL_CINT lda)
{
   ATL_CINT lda2 = lda<<1;
   const TYPE *stY = Y + N+N;
   TYPE y[2];
   do
   {
      *y = *Y;
      #ifdef Conj_
         y[1] = -Y[1];
      #else
         y[1] = Y[1];
      #endif
      Mjoin(PATL,axpy)(M, y, X, 1, A, 1);
      Y += 2;
      A += lda2;
   }
   while (Y != stY);
}
@ROUT ATL_gerk_axpy ATL_gerk_4x4_1 ATL_gerk_8x4_0 ATL_gerk_1x4_0
@extract -punymacs -b @(topd)/cw.inc lang=C -def cwdate 1999 -def cwdate 2009
#include "atlas_misc.h"
#include "atlas_lvl2.h"
@ROUT ATL_gerk_axpy `#include "atlas_prefetch.h"`

void ATL_UGERK
   (ATL_CINT M, ATL_CINT N, const TYPE *X, const TYPE *Y, TYPE *A, ATL_CINT lda)
{
@ROUT ATL_gerk_1x4_0
   ATL_INT i, j;
   const TYPE *x;
   TYPE *A0 = A, *A1 = A + lda, *A2 = A1 + lda, *A3 = A2 + lda;
   ATL_CINT N4 = (N>>2)<<2, incAn = (lda<<2) - M + 1;
   register TYPE m0, m1, m2, m3, x0, y0, y1, y2, y3;

   if (M > 8)
   {
      for (j=N4; j; j -= 4)
      {
         y0 = *Y;  
         y1 = Y[1]; 
         y2 = Y[2]; 
         y3 = Y[3];
         Y += 4; 
         x = X;
         x0 = *X; x = X + 1;
         m0 = y0 * x0;
         m1 = y1 * x0;
         m2 = y2 * x0;
         m3 = y3 * x0;
         for (i=M-1; i; i--)
         {
            x0 = *x++;
            *A0++ += m0; m0 = y0 * x0;
            *A1++ += m1; m1 = y1 * x0;
            *A2++ += m2; m2 = y2 * x0;
            *A3++ += m3; m3 = y3 * x0;
         }
         *A0 += m0; A0 += incAn;
         *A1 += m1; A1 += incAn;
         *A2 += m2; A2 += incAn;
         *A3 += m3; A3 += incAn;
      }
      if (N-N4) 
         Mjoin(PATL,gerk_axpy)(M, N-N4, ATL_rone, X, 1, Y, 1, A0, lda);
   }
   else 
      Mjoin(PATL,gerk_Mlt16)(M, N, ATL_rone, X, 1, Y, 1, A, lda);
@ROUT ATL_gerk_8x4_0
   int i, j;
   const TYPE *x;
   TYPE *A0 = A, *A1 = A + lda, *A2 = A1 + lda, *A3 = A2 + lda;
   ATL_CINT M8 = ((M-1)>>3)<<3, mr = M-M8-1;
   ATL_CINT N4 = (N>>2)<<2, incAn = (lda<<2) - M + 1;
   register TYPE m0, m1, m2, m3, x0, y0, y1, y2, y3;

   if (M8)
   {
      for (j=N4; j; j -= 4)
      {
         y0 = *Y; y1 = Y[1]; y2 = Y[2]; y3 = Y[3]; 
         x0 = *X; x = X + 1;
         m0 = y0 * x0; Y += 4; 
         m1 = y1 * x0; 
         m2 = y2 * x0; 
         m3 = y3 * x0; 
         for (i=M8; i; i -= 8)
         {
            x0 = *x;
            *A0 += m0; m0 = y0 * x0;
            *A1 += m1; m1 = y1 * x0;
            *A2 += m2; m2 = y2 * x0;
            *A3 += m3; m3 = y3 * x0;
            x0 = x[1];
            A0[1] += m0; m0 = y0 * x0;
            A1[1] += m1; m1 = y1 * x0;
            A2[1] += m2; m2 = y2 * x0;
            A3[1] += m3; m3 = y3 * x0;
            x0 = x[2];
            A0[2] += m0; m0 = y0 * x0;
            A1[2] += m1; m1 = y1 * x0;
            A2[2] += m2; m2 = y2 * x0;
            A3[2] += m3; m3 = y3 * x0;
            x0 = x[3];
            A0[3] += m0; m0 = y0 * x0;
            A1[3] += m1; m1 = y1 * x0;
            A2[3] += m2; m2 = y2 * x0;
            A3[3] += m3; m3 = y3 * x0;
            x0 = x[4];
            A0[4] += m0; m0 = y0 * x0;
            A1[4] += m1; m1 = y1 * x0;
            A2[4] += m2; m2 = y2 * x0;
            A3[4] += m3; m3 = y3 * x0;
            x0 = x[5];
            A0[5] += m0; m0 = y0 * x0;
            A1[5] += m1; m1 = y1 * x0;
            A2[5] += m2; m2 = y2 * x0;
            A3[5] += m3; m3 = y3 * x0;
            x0 = x[6];
            A0[6] += m0; m0 = y0 * x0;
            A1[6] += m1; m1 = y1 * x0;
            A2[6] += m2; m2 = y2 * x0;
            A3[6] += m3; m3 = y3 * x0;
            x0 = x[7]; x += 8;
            A0[7] += m0; m0 = y0 * x0; A0 += 8;
            A1[7] += m1; m1 = y1 * x0; A1 += 8;
            A2[7] += m2; m2 = y2 * x0; A2 += 8;
            A3[7] += m3; m3 = y3 * x0; A3 += 8;
         }
         if (mr)
         {
            for (i=mr; i; i--)
            {
               x0 = *x++;
               *A0++ += m0; m0 = y0 * x0;
               *A1++ += m1; m1 = y1 * x0;
               *A2++ += m2; m2 = y2 * x0;
               *A3++ += m3; m3 = y3 * x0;
            }
         }
         *A0 += m0; A0 += incAn;
         *A1 += m1; A1 += incAn;
         *A2 += m2; A2 += incAn;
         *A3 += m3; A3 += incAn;
      }
      if (N-N4)
         Mjoin(PATL,gerk_axpy)(M, N-N4, ATL_rone, X, 1, Y, 1, A0, lda);
   }
   else
      Mjoin(PATL,gerk_Mlt16)(M, N, ATL_rone, X, 1, Y, 1, A, lda);
@ROUT ATL_gerk_4x4_1
   ATL_INT i, j;
   const TYPE *x;
   TYPE *A0 = A, *A1 = A + lda, *A2 = A1 + lda, *A3 = A2 + lda;
   ATL_CINT M4 = (M>>2)<<2, N4 = (N>>2)<<2, incAn = (lda<<2) - M4;
   register TYPE x0, x1, x2, x3, y0, y1, y2, y3;

   if (M4)
   {
      for (j=N4; j; j -= 4)
      {
         y0 = *Y;
         y1 = Y[1];
         y2 = Y[2];
         y3 = Y[3];  Y  += 4;
         x = X;
         for (i=M4; i; i -= 4)
         {
            x0 = *x; x1 = x[1]; x2 = x[2]; x3 = x[3];
            *A0 += y0 * x0;
            x += 4;
            *A1 += y1 * x0;
            *A2 += y2 * x0;
            *A3 += y3 * x0;
            A0[1] += y0 * x1;
            A1[1] += y1 * x1;
            A2[1] += y2 * x1;
            A3[1] += y3 * x1;
            A0[2] += y0 * x2;
            A1[2] += y1 * x2;
            A2[2] += y2 * x2;
            A3[2] += y3 * x2;
            A0[3] += y0 * x3; A0 += 4;
            A1[3] += y1 * x3; A1 += 4;
            A2[3] += y2 * x3; A2 += 4;
            A3[3] += y3 * x3; A3 += 4;
         }
         switch(M-M4)
         {
         case 1:
            x0 = *x;
            *A0 += y0 * x0;
            *A1 += y1 * x0;
            *A2 += y2 * x0;
            *A3 += y3 * x0;
            break;
         case 2:
            x0 = *x; x1 = x[1];
            *A0 += y0 * x0;
            *A1 += y1 * x0;
            *A2 += y2 * x0;
            *A3 += y3 * x0;
            A0[1] += y0 * x1;
            A1[1] += y1 * x1;
            A2[1] += y2 * x1;
            A3[1] += y3 * x1;
            break;
         case 3:
            x0 = *x; x1 = x[1]; x2 = x[2];
            *A0 += y0 * x0;
            *A1 += y1 * x0;
            *A2 += y2 * x0;
            *A3 += y3 * x0;
            A0[1] += y0 * x1;
            A1[1] += y1 * x1;
            A2[1] += y2 * x1;
            A3[1] += y3 * x1;
            A0[2] += y0 * x2;
            A1[2] += y1 * x2;
            A2[2] += y2 * x2;
            A3[2] += y3 * x2;
            break;
         }
         A0 += incAn;
         A1 += incAn;
         A2 += incAn;
         A3 += incAn;
      }
      if (N-N4)
         Mjoin(PATL,gerk_axpy)(M, N-N4, ATL_rone, X, 1, Y, 1, A0, lda);
   }
   else
      Mjoin(PATL,gerk_Mlt16)(M, N, ATL_rone, X, 1, Y, 1, A, lda);
@ROUT ATL_gerk_axpy
   const TYPE *stY = Y + N;
@beginskip
   #ifdef ATL_AltiVec
      int cwrd = ATL_MulBySize(N)>>4;
      if (cwrd >= 64) cwrd = ATL_GetCtrl(512, (cwrd+31)>>5, 0);
      else cwrd = ATL_GetCtrl(64, (cwrd+3)>>2, 4);
   #endif
@endskip
   if (M > 8)
   {
      do
      {
@beginskip
         #ifdef ATL_AltiVec
            ATL_pfavW(A, cwrd, 0);
         #endif
@endskip
         Mjoin(PATL,axpy)(M, *Y, X, 1, A, 1);
         Y++;
         A += lda;
      }
      while (Y != stY);
   }
   else
      Mjoin(PATL,gerk_Mlt16)(M, N, ATL_rone, X, 1, Y, 1, A, lda);
@ROUT ATL_gerk_axpy ATL_gerk_4x4_1 ATL_gerk_8x4_0 ATL_gerk_1x4_0
}
@ROUT ATL_ger2k_1x1_1
#include "atlas_misc.h"

void ATL_UGER2K
   (ATL_CINT M, ATL_CINT N, const TYPE *X0, const TYPE *Y0, 
    const TYPE *X1, const TYPE *Y1, TYPE *A, ATL_CINT lda)
{
   register TYPE y0, y1;
   register ATL_INT i, j;
   
   for (j=0; j < N; j++)
   {
      y0 = Y0[j];
      y1 = Y1[j];
      for (i=0; i < M; i++)
         A[i+j*lda] += X0[i] * y0 + X1[i] * y1;
   }
}
@ROUT ATL_cger2k_1x1_1
#include "atlas_misc.h"

void ATL_UGER2K
   (ATL_CINT M, ATL_CINT N, const TYPE *X0, const TYPE *Y0, 
    const TYPE *X1, const TYPE *Y1, TYPE *A, ATL_CINT lda)
{
   const TYPE *x0, *x1;
   register TYPE y0r, y0i, y1r, y1i, x0r, x0i, x1r, x1i;
   register ATL_INT i, j;
   const ATL_INT incA = (lda-M)<<1;
   
   for (j=0; j < N; j++, A += incA)
   {
      y0r = *Y0;
      y0i = Y0[1];
      Y0 += 2;
      y1r = *Y1;
      y1i = Y1[1];
      Y1 += 2;
      x0 = X0;
      x1 = X1;
      for (i=0; i < M; i++)
      {
         x0r = *x0;
         x0i = x0[1];
         x0 += 2;
         x1r = *x1;
         x1i = x1[1];
         x1 += 2;
         *A += x0r*y0r - x0i*y0i + x1r*y1r - x1i*y1i;
         A[1] += x0r*y0i + x0i*y0r + x1r*y1i + x1i*y1r;
         A += 2;
      }
   }
}
@ROUT ATL_zger2k_rk2_avx
#include "atlas_misc.h"
#include "atlas_prefetch.h"
#include <immintrin.h>

#if defined(__GNUC__) || \
    (defined(__STDC_VERSION__) && (__STDC_VERSION__/100 >= 1999))
   #define ATL_SINLINE static inline
#else
   #define ATL_SINLINE static
#endif
/*
 * Subtract off x0 & x1 contribution to all remaining equations using a
 * rank-2 update with mu=2, nu=3, ku=2.  This version is for 16 AVX regs.
 * nu is the # of RHS, ku is the number of equations solved, and mu is 
 * unrolled only to enable vectorization & software pipelining of load/use.
 * Loop order is MKN, so that B is kept completely in registers, and
 * C and A are streamed in (and out, for C) from cache during the operation.
 */
ATL_SINLINE void ATL_rk2(ATL_CINT M, const TYPE *pA0, const TYPE *pA1,
                         const TYPE *pB0, const TYPE pB1, 
                         TYPE *C, ATL_CINT ldc0) 
{
   ATL_CINT ldc=ldc0+ldc0;
   TYPE *pC0 = C, *pC1 = C+ldc, *pC2 = C+((ldc)<<1);
   ATL_CINT MM =  (M & 2) ? M-2 : M-4; 
   int i; 
   register __m256d rB00, iB00, rB01, iB01, rB02, iB02;
   register __m256d C00, C01, C02; 
   register __m256d C20, C21, C22; 
   register __m256d A, a;

   rB00 = _mm256_set_pd(*pB1, *pB0, *pB1, *pB0);  
                                                /* rB10 rB00 rB10 rB00 */
   iB00 = _mm256_set_pd(pB1[1], pB0[1], pB1[1], pB0[1]);
                                                /* iB10 iB00 iB10 iB00 */
   rB01 = _mm256_set_pd(pB1[2], pB0[2], pB1[2], pB0[2]);  
                                                /* rB11 rB01 rB11 rB01 */
   iB01 = _mm256_set_pd(pB1[3], pB0[3], pB1[3], pB0[3]);
                                                /* iB11 iB01 iB11 iB01 */
   rB02 = _mm256_set_pd(pB1[4], pB0[4], pB1[4], pB0[4]);  
                                                /* rB12 rB02 rB12 rB02 */
   iB02 = _mm256_set_pd(pB1[5], pB0[5], pB1[5], pB0[5]);
                                                /* iB12 iB02 iB12 iB02 */
   C00  = _mm256_load_pd(pC0);                  /* iC10 rC10 iC00 rC00 */
   C01  = _mm256_load_pd(pC1);                  /* iC11 rC11 iC01 rC01 */
   C02  = _mm256_load_pd(pC2);                  /* iC12 rC12 iC02 rC02 */

   A    = _mm256_load_pd(pA0);                  /* iA10 rA10 iA00 rA00 */
   a    = _mm256_shuffle_pd(A, A, 0x5);         /* rA10 iA10 rA00 iA00 */

   for (i=0; i < MM; i += 4, pA0 += 8, pA1 += 8, pC0 += 8, pC1 += 8, pC2 += 8)
   {                                     /* rB00 = rB10 rB00 rB10 rB00 */
                                         /* iB00 = iB10 iB00 iB10 iB00 */
      register __m256d m, b;
/*
 *    Do M=K=0 calcs
 */
      b = _mm256_unpacklo_pd(rB00, rB00);       /* rB00 rB00 rB00 rB00 */
      m    = _mm256_mul_pd(A, b);
      C00 = _mm256_add_pd(m, C00);  C20 = _mm256_load_pd(pC0+4);
      b = _mm256_unpacklo_pd(rB01, rB01);       /* rB01 rB01 rB01 rB01 */
      m    = _mm256_mul_pd(A, b);
      C01 = _mm256_add_pd(m, C01);  C21 = _mm256_load_pd(pC1+4);
      b = _mm256_unpacklo_pd(rB02, rB02);       /* rB02 rB02 rB02 rB02 */
      m    = _mm256_mul_pd(A, b);
      C02 = _mm256_add_pd(m, C02);  A = _mm256_load_pd(pA1);

      b = _mm256_unpacklo_pd(iB00, iB00);       /* iB00 iB00 iB00 iB00 */
      m    = _mm256_mul_pd(a, b);
      C00 = _mm256_addsub_pd(C00, m);  C22 = _mm256_load_pd(pC2+4);
      b = _mm256_unpacklo_pd(iB01, iB01);
      m    = _mm256_mul_pd(a, b);
      C01 = _mm256_addsub_pd(C01, m);
      b = _mm256_unpacklo_pd(iB02, iB02);
      m    = _mm256_mul_pd(a, b);
      C02 = _mm256_addsub_pd(C02, m); a    = _mm256_shuffle_pd(A, A, 0x5);
/*
 *    Do M=0, K=1 calcs
 */
      b = _mm256_unpackhi_pd(rB00, rB00);       /* rB10 rB10 rB10 rB10 */
      m    = _mm256_mul_pd(A, b);
      C00 = _mm256_add_pd(m, C00);
      b = _mm256_unpackhi_pd(rB01, rB01);       /* rB11 rB11 rB11 rB11 */
      m    = _mm256_mul_pd(A, b);
      C01 = _mm256_add_pd(m, C01);
      b = _mm256_unpackhi_pd(rB02, rB02);       /* rB12 rB12 rB12 rB12 */
      m    = _mm256_mul_pd(A, b);
      C02 = _mm256_add_pd(m, C02);  A = _mm256_load_pd(pA0+4);

      b = _mm256_unpackhi_pd(iB00, iB00);       /* iB10 iB10 iB10 iB10 */
      m    = _mm256_mul_pd(a, b);
      C00 = _mm256_addsub_pd(C00, m); _mm256_store_pd(pC0, C00);
      b = _mm256_unpackhi_pd(iB01, iB01);  
      m    = _mm256_mul_pd(a, b);
      C01 = _mm256_addsub_pd(C01, m); _mm256_store_pd(pC1, C01);
      b = _mm256_unpackhi_pd(iB02, iB02); 
      m    = _mm256_mul_pd(a, b);
      C02 = _mm256_addsub_pd(C02, m); _mm256_store_pd(pC2, C02);
/*
 *    Do M=2, K=0 calcs
 */
      b = _mm256_unpacklo_pd(rB00, rB00);       /* rB00 rB00 rB00 rB00 */
      m    = _mm256_mul_pd(A, b);
      C20 = _mm256_add_pd(m, C20);  a    = _mm256_shuffle_pd(A, A, 0x5);
      b = _mm256_unpacklo_pd(rB01, rB01);
      m    = _mm256_mul_pd(A, b);
      C21 = _mm256_add_pd(m, C21);  C00 = _mm256_load_pd(pC0+8);
      b = _mm256_unpacklo_pd(rB02, rB02); 
      m    = _mm256_mul_pd(A, b);
      C22 = _mm256_add_pd(m, C22);  A = _mm256_load_pd(pA1+4);

      b = _mm256_unpacklo_pd(iB00, iB00);       /* iB00 iB00 iB00 iB00 */
      m    = _mm256_mul_pd(a, b);
      C20 = _mm256_addsub_pd(C20, m);  C01 = _mm256_load_pd(pC1+8);
      b = _mm256_unpacklo_pd(iB01, iB01);
      m    = _mm256_mul_pd(a, b);
      C21 = _mm256_addsub_pd(C21, m);  C02 = _mm256_load_pd(pC2+8);
      b = _mm256_unpacklo_pd(iB02, iB02);
      m    = _mm256_mul_pd(a, b);
      C22 = _mm256_addsub_pd(C22, m);   a    = _mm256_shuffle_pd(A, A, 0x5);
/*
 *    M=2, K=1 calcs
 */
      b = _mm256_unpackhi_pd(rB00, rB00);       /* rB10 rB10 rB10 rB10 */
      m    = _mm256_mul_pd(A, b);
      C20 = _mm256_add_pd(m, C20);
      b = _mm256_unpackhi_pd(rB01, rB01);
      m    = _mm256_mul_pd(A, b);
      C21 = _mm256_add_pd(m, C21);
      b = _mm256_unpackhi_pd(rB02, rB02);
      m    = _mm256_mul_pd(A, b);
      C22 = _mm256_add_pd(m, C22);   A = _mm256_load_pd(pA0+8);

      b = _mm256_unpackhi_pd(iB00, iB00);       /* iB10 iB10 iB10 iB10 */
      m    = _mm256_mul_pd(a, b);
      C20 = _mm256_addsub_pd(C20, m); _mm256_store_pd(pC0+4, C20);
      b = _mm256_unpackhi_pd(iB01, iB01);
      m    = _mm256_mul_pd(a, b);
      C21 = _mm256_addsub_pd(C21, m); _mm256_store_pd(pC1+4, C21);
      b = _mm256_unpackhi_pd(iB02, iB02);
      m    = _mm256_mul_pd(a, b);
      C22 = _mm256_addsub_pd(C22, m); _mm256_store_pd(pC2+4, C22);   
      a    = _mm256_shuffle_pd(A, A, 0x5);
   }
/*
 * Drain pipes
 */
   {
      register __m256d m, b;
/*
 *    Do M=K=0 calcs
 */
      b = _mm256_unpacklo_pd(rB00, rB00);       /* rB00 rB00 rB00 rB00 */
      m    = _mm256_mul_pd(A, b);
      C00 = _mm256_add_pd(m, C00);  
      b = _mm256_unpacklo_pd(rB01, rB01);       /* rB01 rB01 rB01 rB01 */
      m    = _mm256_mul_pd(A, b);
      C01 = _mm256_add_pd(m, C01);  
      b = _mm256_unpacklo_pd(rB02, rB02);       /* rB02 rB02 rB02 rB02 */
      m    = _mm256_mul_pd(A, b);
      C02 = _mm256_add_pd(m, C02);  A = _mm256_load_pd(pA1);

      b = _mm256_unpacklo_pd(iB00, iB00);       /* iB00 iB00 iB00 iB00 */
      m    = _mm256_mul_pd(a, b);
      C00 = _mm256_addsub_pd(C00, m);
      b = _mm256_unpacklo_pd(iB01, iB01);
      m    = _mm256_mul_pd(a, b);
      C01 = _mm256_addsub_pd(C01, m);
      b = _mm256_unpacklo_pd(iB02, iB02);
      m    = _mm256_mul_pd(a, b);
      C02 = _mm256_addsub_pd(C02, m); a    = _mm256_shuffle_pd(A, A, 0x5);
/*
 *    Do M=0, K=1 calcs
 */
      b = _mm256_unpackhi_pd(rB00, rB00);       /* rB10 rB10 rB10 rB10 */
      m    = _mm256_mul_pd(A, b);
      C00 = _mm256_add_pd(m, C00);
      b = _mm256_unpackhi_pd(rB01, rB01);       /* rB11 rB11 rB11 rB11 */
      m    = _mm256_mul_pd(A, b);
      C01 = _mm256_add_pd(m, C01);
      b = _mm256_unpackhi_pd(rB02, rB02);       /* rB12 rB12 rB12 rB12 */
      m    = _mm256_mul_pd(A, b);
      C02 = _mm256_add_pd(m, C02);

      b = _mm256_unpackhi_pd(iB00, iB00);       /* iB10 iB10 iB10 iB10 */
      m    = _mm256_mul_pd(a, b);
      C00 = _mm256_addsub_pd(C00, m); _mm256_store_pd(pC0, C00);
      b = _mm256_unpackhi_pd(iB01, iB01);  
      m    = _mm256_mul_pd(a, b);
      C01 = _mm256_addsub_pd(C01, m); _mm256_store_pd(pC1, C01);
      b = _mm256_unpackhi_pd(iB02, iB02); 
      m    = _mm256_mul_pd(a, b);
      C02 = _mm256_addsub_pd(C02, m); _mm256_store_pd(pC2, C02);
      if (!(M&2))
      {  
         A = _mm256_load_pd(pA0+4);
         C20 = _mm256_load_pd(pC0+4);
         C21 = _mm256_load_pd(pC1+4);
         C22 = _mm256_load_pd(pC2+4);
         a    = _mm256_shuffle_pd(A, A, 0x5);
/*
 *       Do M=2, K=0 calcs
 */
         b = _mm256_unpacklo_pd(rB00, rB00);       /* rB00 rB00 rB00 rB00 */
         m    = _mm256_mul_pd(A, b);
         C20 = _mm256_add_pd(m, C20);  a    = _mm256_shuffle_pd(A, A, 0x5);
         b = _mm256_unpacklo_pd(rB01, rB01);
         m    = _mm256_mul_pd(A, b);
         C21 = _mm256_add_pd(m, C21);
         b = _mm256_unpacklo_pd(rB02, rB02); 
         m    = _mm256_mul_pd(A, b);
         C22 = _mm256_add_pd(m, C22);  A = _mm256_load_pd(pA1+4);

         b = _mm256_unpacklo_pd(iB00, iB00);       /* iB00 iB00 iB00 iB00 */
         m    = _mm256_mul_pd(a, b);
         C20 = _mm256_addsub_pd(C20, m);
         b = _mm256_unpacklo_pd(iB01, iB01);
         m    = _mm256_mul_pd(a, b);
         C21 = _mm256_addsub_pd(C21, m);
         b = _mm256_unpacklo_pd(iB02, iB02);
         m    = _mm256_mul_pd(a, b);
         C22 = _mm256_addsub_pd(C22, m);   a    = _mm256_shuffle_pd(A, A, 0x5);
/*
 *       M=2, K=1 calcs
 */
         b = _mm256_unpackhi_pd(rB00, rB00);       /* rB10 rB10 rB10 rB10 */
         m    = _mm256_mul_pd(A, b);
         C20 = _mm256_add_pd(m, C20);
         b = _mm256_unpackhi_pd(rB01, rB01);
         m    = _mm256_mul_pd(A, b);
         C21 = _mm256_add_pd(m, C21);
         b = _mm256_unpackhi_pd(rB02, rB02);
         m    = _mm256_mul_pd(A, b);
         C22 = _mm256_add_pd(m, C22);  
   
         b = _mm256_unpackhi_pd(iB00, iB00);       /* iB10 iB10 iB10 iB10 */
         m    = _mm256_mul_pd(a, b);
         C20 = _mm256_addsub_pd(C20, m); _mm256_store_pd(pC0+4, C20);
         b = _mm256_unpackhi_pd(iB01, iB01);
         m    = _mm256_mul_pd(a, b);
         C21 = _mm256_addsub_pd(C21, m); _mm256_store_pd(pC1+4, C21);
         b = _mm256_unpackhi_pd(iB02, iB02);
         m    = _mm256_mul_pd(a, b);
         C22 = _mm256_addsub_pd(C22, m); _mm256_store_pd(pC2+4, C22);   
      }
   }
}
@ROUT ATL_zger2k_rk2_sse3
@extract -punymacs -b @(topd)/cw.inc lang=C -def cwdate 2011
#include "atlas_misc.h"
#include "atlas_prefetch.h"
#include <xmmintrin.h>
#include <pmmintrin.h>

#if defined(__GNUC__) || \
    (defined(__STDC_VERSION__) && (__STDC_VERSION__/100 >= 1999))
   #define ATL_SINLINE static inline
#else
   #define ATL_SINLINE static
#endif

/*
 * Subtract off x0 & x1 contribution to all remaining equations using a
 * rank-2 update with mu=2, nu=3, ku=2.  This version is for 16 SSE regs.
 * nu is the # of RHS, ku is the number of equations solved, and mu is 
 * unrolled only to enable vectorization & software pipelining of load/use.
 * Loop order is MKN, so that B is kept completely in registers, and
 * C and A are streamed in (and out, for C) from cache during the operation.
 */
ATL_SINLINE void ATL_rk2(ATL_CINT M, const TYPE *pA0, const TYPE *pA1,
                         const TYPE *pB0, const TYPE *pB1, 
                         TYPE *C, ATL_CINT ldc0) 
{
   ATL_CINT ldc=ldc0+ldc0;
   TYPE *pC0 = C, *pC1 = C+ldc, *pC2 = C+((ldc)<<1);
   ATL_INT i;
   ATL_CINT MM = (M&1) ? M-1 : M-2;
   register __m128d B00, B10, B01, B11, B02, B12;
   register __m128d C00, C01, C02, C10, C11, C12;
   register __m128d A, a;

   B00 = _mm_load_pd(pB0);
   B10 = _mm_load_pd(pB1);
   B01 = _mm_load_pd(pB0+2);
   B11 = _mm_load_pd(pB1+2);
   B02 = _mm_load_pd(pB0+4);
   B12 = _mm_load_pd(pB1+4);    	/* iB12, rB12 */


   C00 = _mm_load_pd(pC0);
   C01 = _mm_load_pd(pC1);
   C02 = _mm_load_pd(pC2);
   A = _mm_load_pd(pA0);                		/* iA00, rA00 */
   for (i=0; i < MM; i += 2, pA0 += 4, pA1 += 4, pC0 += 4, pC1 += 4, pC2 += 4)
   {
      register __m128d b;
/*
 *    K=0, M=[0,1], apply real components of B0x
 */
      b = _mm_movedup_pd(B00);			/* rB00,      rB00 */
      b = _mm_mul_pd(b, A);                     /* iA00*rB00, rA00*rB00 */
      C00 = _mm_add_pd(C00, b);
         a = (__m128d)_mm_shuffle_epi32((__m128i)A, 0x4E);  	/* rA00, iA00 */
      b = _mm_movedup_pd(B01);	
      b = _mm_mul_pd(b, A);    
      C01 = _mm_add_pd(C01, b);
         C10 = _mm_load_pd(pC0+2);
      b = _mm_movedup_pd(B02);	
      b = _mm_mul_pd(b, A);    
      C02 = _mm_add_pd(C02, b);
         A = _mm_load_pd(pA1);                		/* iA01, rA01 */
/*
 *    K=0, M=0, apply imaginary components of B0x
 */
      b = (__m128d)_mm_shuffle_epi32((__m128i)B00, 0xEE); /* iB00, iB00 */
      b = _mm_mul_pd(b, a);                     /* rA00*iB00, iA00*iB00 */
      C00 = _mm_addsub_pd(C00, b);
         C11 = _mm_load_pd(pC1+2);
      b = (__m128d)_mm_shuffle_epi32((__m128i)B01, 0xEE); 
      b = _mm_mul_pd(b, a);                     
      C01 = _mm_addsub_pd(C01, b);
         C12 = _mm_load_pd(pC2+2);
      b = (__m128d)_mm_shuffle_epi32((__m128i)B02, 0xEE); 
      b = _mm_mul_pd(b, a);                     
      C02 = _mm_addsub_pd(C02, b);
/*
 *    K=1, M=0, apply real components of B1x
 */
      b = _mm_movedup_pd(B10);			/* rB10,      rB10 */
      b = _mm_mul_pd(b, A);                     /* iA01*rB10, rA01*rB10 */
      C00 = _mm_add_pd(C00, b);
      a = (__m128d)_mm_shuffle_epi32((__m128i)A, 0x4E);  	/* rA01, iA01 */
      b = _mm_movedup_pd(B11);	
      b = _mm_mul_pd(b, A);    
      C01 = _mm_add_pd(C01, b);
      b = _mm_movedup_pd(B12);	
      b = _mm_mul_pd(b, A);    
      C02 = _mm_add_pd(C02, b);
         A = _mm_load_pd(pA0+2);                /* iA10, rA10 */
/*
 *    K=1, M=0, apply imaginary components of B1x
 */
      b = (__m128d)_mm_shuffle_epi32((__m128i)B10, 0xEE); /* iB10, iB10 */
      b = _mm_mul_pd(b, a);                     /* rA01*iB10, iA01*iB10 */
      C00 = _mm_addsub_pd(C00, b);
         _mm_store_pd(pC0, C00);   
      b = (__m128d)_mm_shuffle_epi32((__m128i)B11, 0xEE); 
      b = _mm_mul_pd(b, a);                     
      C01 = _mm_addsub_pd(C01, b);
         _mm_store_pd(pC1, C01);   
      b = (__m128d)_mm_shuffle_epi32((__m128i)B12, 0xEE); 
      b = _mm_mul_pd(b, a);                     
      C02 = _mm_addsub_pd(C02, b);
         _mm_store_pd(pC2, C02);   
/*
 *    K=0, M=1, apply real components of B0x
 */
      b = _mm_movedup_pd(B00);			/* rB00,      rB00 */
      b = _mm_mul_pd(b, A);                     /* iA10*rB00, rA10*rB00 */
      C10 = _mm_add_pd(C10, b);
         a = (__m128d)_mm_shuffle_epi32((__m128i)A, 0x4E);  	/* rA10, iA10 */
      b = _mm_movedup_pd(B01);	
      b = _mm_mul_pd(b, A);    
      C11 = _mm_add_pd(C11, b);
         C00 = _mm_load_pd(pC0+4);
      b = _mm_movedup_pd(B02);	
      b = _mm_mul_pd(b, A);    
      C12 = _mm_add_pd(C12, b);
         A = _mm_load_pd(pA1+2);               		/* iA11, rA11 */
/*
 *    K=0, M=1, apply imaginary components of B0x
 */
      b = (__m128d)_mm_shuffle_epi32((__m128i)B00, 0xEE); /* iB00, iB00 */
      b = _mm_mul_pd(b, a);                     /* rA10*iB00, iA10*iB00 */
      C10 = _mm_addsub_pd(C10, b);
         C01 = _mm_load_pd(pC1+4);
      b = (__m128d)_mm_shuffle_epi32((__m128i)B01, 0xEE); 
      b = _mm_mul_pd(b, a);                     
      C11 = _mm_addsub_pd(C11, b);
         C02 = _mm_load_pd(pC2+4);
      b = (__m128d)_mm_shuffle_epi32((__m128i)B02, 0xEE); 
      b = _mm_mul_pd(b, a);                     
      C12 = _mm_addsub_pd(C12, b);
/*
 *    K=1, M=1, apply real components of B1x
 */
      b = _mm_movedup_pd(B10);			/* rB10,      rB10 */
      b = _mm_mul_pd(b, A);                     /* iA11*rB10, rA11*rB10 */
      C10 = _mm_add_pd(C10, b);
      a = (__m128d)_mm_shuffle_epi32((__m128i)A, 0x4E);  	/* rA11, iA11 */
      b = _mm_movedup_pd(B11);	
      b = _mm_mul_pd(b, A);    
      C11 = _mm_add_pd(C11, b);
      b = _mm_movedup_pd(B12);	
      b = _mm_mul_pd(b, A);    
      C12 = _mm_add_pd(C12, b);
         A = _mm_load_pd(pA0+4);               		/* iA20, rA20 */
/*
 *    K=1, M=1, apply imaginary components of B1x
 */
      b = (__m128d)_mm_shuffle_epi32((__m128i)B10, 0xEE); /* iB10, iB10 */
      b = _mm_mul_pd(b, a);                     /* rA11*iB10, iA11*iB10 */
      C10 = _mm_addsub_pd(C10, b);
         _mm_store_pd(pC0+2, C10);   
      b = (__m128d)_mm_shuffle_epi32((__m128i)B11, 0xEE); 
      b = _mm_mul_pd(b, a);                     
      C11 = _mm_addsub_pd(C11, b);
         _mm_store_pd(pC1+2, C11);   
      b = (__m128d)_mm_shuffle_epi32((__m128i)B12, 0xEE); 
      b = _mm_mul_pd(b, a);                     
      C12 = _mm_addsub_pd(C12, b);
         _mm_store_pd(pC2+2, C12);   
   }
/*
 * Drain pipes
 */
   {
      register __m128d b;
/*
 *    K=0, M=[0,1], apply real components of B0x
 */
      b = _mm_movedup_pd(B00);			/* rB00,      rB00 */
      b = _mm_mul_pd(b, A);                     /* iA00*rB00, rA00*rB00 */
      C00 = _mm_add_pd(C00, b);
         a = (__m128d)_mm_shuffle_epi32((__m128i)A, 0x4E);  	/* rA00, iA00 */
      b = _mm_movedup_pd(B01);	
      b = _mm_mul_pd(b, A);    
      C01 = _mm_add_pd(C01, b);
      b = _mm_movedup_pd(B02);	
      b = _mm_mul_pd(b, A);    
      C02 = _mm_add_pd(C02, b);
         A = _mm_load_pd(pA1);                		/* iA01, rA01 */
/*
 *    K=0, M=0, apply imaginary components of B0x
 */
      b = (__m128d)_mm_shuffle_epi32((__m128i)B00, 0xEE); /* iB00, iB00 */
      b = _mm_mul_pd(b, a);                     /* rA00*iB00, iA00*iB00 */
      C00 = _mm_addsub_pd(C00, b);
      b = (__m128d)_mm_shuffle_epi32((__m128i)B01, 0xEE); 
      b = _mm_mul_pd(b, a);                     
      C01 = _mm_addsub_pd(C01, b);
      b = (__m128d)_mm_shuffle_epi32((__m128i)B02, 0xEE); 
      b = _mm_mul_pd(b, a);                     
      C02 = _mm_addsub_pd(C02, b);
/*
 *    K=1, M=0, apply real components of B1x
 */
      b = _mm_movedup_pd(B10);			/* rB10,      rB10 */
      b = _mm_mul_pd(b, A);                     /* iA01*rB10, rA01*rB10 */
      C00 = _mm_add_pd(C00, b);
      a = (__m128d)_mm_shuffle_epi32((__m128i)A, 0x4E);  	/* rA01, iA01 */
      b = _mm_movedup_pd(B11);	
      b = _mm_mul_pd(b, A);    
      C01 = _mm_add_pd(C01, b);
      b = _mm_movedup_pd(B12);	
      b = _mm_mul_pd(b, A);    
      C02 = _mm_add_pd(C02, b);
/*
 *    K=1, M=0, apply imaginary components of B1x
 */
      b = (__m128d)_mm_shuffle_epi32((__m128i)B10, 0xEE); /* iB10, iB10 */
      b = _mm_mul_pd(b, a);                     /* rA01*iB10, iA01*iB10 */
      C00 = _mm_addsub_pd(C00, b);
         _mm_store_pd(pC0, C00);   
      b = (__m128d)_mm_shuffle_epi32((__m128i)B11, 0xEE); 
      b = _mm_mul_pd(b, a);                     
      C01 = _mm_addsub_pd(C01, b);
         _mm_store_pd(pC1, C01);   
      b = (__m128d)_mm_shuffle_epi32((__m128i)B12, 0xEE); 
      b = _mm_mul_pd(b, a);                     
      C02 = _mm_addsub_pd(C02, b);
         _mm_store_pd(pC2, C02);   
      if (!(M&1))
      {
         C10 = _mm_load_pd(pC0+2);
         C11 = _mm_load_pd(pC1+2);
         C12 = _mm_load_pd(pC2+2);
         A = _mm_load_pd(pA0+2);                /* iA10, rA10 */
/*
 *       K=0, M=1, apply real components of B0x
 */
         b = _mm_movedup_pd(B00);			/* rB00,      rB00 */
         b = _mm_mul_pd(b, A);                     /* iA10*rB00, rA10*rB00 */
         C10 = _mm_add_pd(C10, b);
            a = (__m128d)_mm_shuffle_epi32((__m128i)A, 0x4E);  	/* rA10, iA10 */
         b = _mm_movedup_pd(B01);	
         b = _mm_mul_pd(b, A);    
         C11 = _mm_add_pd(C11, b);
         b = _mm_movedup_pd(B02);	
         b = _mm_mul_pd(b, A);    
         C12 = _mm_add_pd(C12, b);
            A = _mm_load_pd(pA1+2);               		/* iA11, rA11 */
/*
 *       K=0, M=1, apply imaginary components of B0x
 */
         b = (__m128d)_mm_shuffle_epi32((__m128i)B00, 0xEE); /* iB00, iB00 */
         b = _mm_mul_pd(b, a);                     /* rA10*iB00, iA10*iB00 */
         C10 = _mm_addsub_pd(C10, b);
         b = (__m128d)_mm_shuffle_epi32((__m128i)B01, 0xEE); 
         b = _mm_mul_pd(b, a);                     
         C11 = _mm_addsub_pd(C11, b);
         b = (__m128d)_mm_shuffle_epi32((__m128i)B02, 0xEE); 
         b = _mm_mul_pd(b, a);                     
         C12 = _mm_addsub_pd(C12, b);
/*
 *       K=1, M=1, apply real components of B1x
 */
         b = _mm_movedup_pd(B10);			/* rB10,      rB10 */
         b = _mm_mul_pd(b, A);                     /* iA11*rB10, rA11*rB10 */
         C10 = _mm_add_pd(C10, b);
         a = (__m128d)_mm_shuffle_epi32((__m128i)A, 0x4E);  	/* rA11, iA11 */
         b = _mm_movedup_pd(B11);	
         b = _mm_mul_pd(b, A);    
         C11 = _mm_add_pd(C11, b);
         b = _mm_movedup_pd(B12);	
         b = _mm_mul_pd(b, A);    
         C12 = _mm_add_pd(C12, b);
/*
 *       K=1, M=1, apply imaginary components of B1x
 */
         b = (__m128d)_mm_shuffle_epi32((__m128i)B10, 0xEE); /* iB10, iB10 */
         b = _mm_mul_pd(b, a);                     /* rA11*iB10, iA11*iB10 */
         C10 = _mm_addsub_pd(C10, b);
            _mm_store_pd(pC0+2, C10);   
         b = (__m128d)_mm_shuffle_epi32((__m128i)B11, 0xEE); 
         b = _mm_mul_pd(b, a);                     
         C11 = _mm_addsub_pd(C11, b);
            _mm_store_pd(pC1+2, C11);   
         b = (__m128d)_mm_shuffle_epi32((__m128i)B12, 0xEE); 
         b = _mm_mul_pd(b, a);                     
         C12 = _mm_addsub_pd(C12, b);
            _mm_store_pd(pC2+2, C12);   
      }
   }
}
@ROUT ATL_zger2k_rk2_sse3 ATL_zger2k_rk2_avx
void ATL_UGER2K
   (ATL_CINT M, ATL_CINT N, const TYPE *X0, const TYPE *Y0,
    const TYPE *X1, const TYPE *Y1, TYPE *A, ATL_CINT lda)
{
   const TYPE *x0, *x1;
   register ATL_INT i, j;
   ATL_CINT incA = lda+lda + (lda<<2);

   assert((N/3)*3 == N);
   for (j=0; j < N; j += 3, A += incA, Y0 += 6, Y1 += 6)
      ATL_rk2(M, X0, X1, Y0, Y1, A, lda);
}
@ROUT ATL_dger2k_2x2_sse3
@extract -b @(topd)/cw.inc lang=C -def cwdate 2010

#include <xmmintrin.h>
#include "atlas_misc.h"
#ifdef ATL_USEREAL
   #ifdef ATL_ALIGNED
      #define vloada(addr_) ( (__m128d)_mm_load_ps((float*)(addr_)) )
      #define vstorea(addr_, r_) _mm_store_ps((float*)(addr_), (__m128)(r_))
   #else
      #define vloada(addr_) ( (__m128d)_mm_loadu_ps((void*)(addr_)) )
      #define vstorea(addr_, r_) _mm_storeu_ps((float*)(addr_), (__m128)(r_))
   #endif
   #define vload(addr_) ( (__m128d)_mm_load_ps((float*)(addr_)) )
   #define sload(addr_) ( (__m128d)_mm_load_ss((float*)(addr_)) )
   #define vmovh2l(vsrc_) ( (__m128d)_mm_movehl_ps((__m128)(vsrc_), (__m128)(vsrc_)) )
#else
   #ifdef ATL_ALIGNED
      #define vloada(addr_) _mm_load_pd((addr_))
      #define vstorea(addr_, r_) _mm_store_pd((addr_), (r_))
   #else
      #define vloada(addr_) _mm_loadu_pd(addr_)
      #define vstorea(addr_, r_) _mm_storeu_pd((addr_), (r_))
   #endif
   #define vload(addr_) _mm_load_pd((addr_))
   #define sload(addr_) _mm_load_sd((addr_))
   #define vmovh2l(vsrc_) _mm_unpackhi_pd((vsrc_), (vsrc_) )
#endif
#define sstore(addr_, r_) _mm_store_sd((addr_), (r_))
#ifdef GCC_FINALLY_HAS_SSE
   #define vdup0(src_) _mm_movedup_pd(src_);
#else
   #define vdup0(src_) ( (__m128d) _mm_shuffle_epi32((__m128i)(src_), 0x44) )
#endif


void ATL_UGER2K
   (ATL_CINT M, ATL_CINT N, const TYPE *X, const TYPE *Y, const TYPE *W, 
    const TYPE *Z, TYPE *A, ATL_CINT lda)
/*
 * A += xy + wz
 */
{
   register __m128d a00, a01, p0, p1;
   register __m128d x0_x1, w0_w1, y0_y0, z0_z0, y1_y1, z1_z1;
   ATL_CINT M2 = (M>>1)<<1, N2 = (N>>1)<<1, incA = lda+lda;
   TYPE *A0 = A, *A1 = A + lda;
   ATL_INT i, j;

   for (j=0; j < N2; j += 2, Y += 2, Z += 2, A0 += incA, A1 += incA)
   {
      y1_y1 = vload(Y);                    /* y1_y1 = {y1, y0} */
      y0_y0 = vdup0(y1_y1);                /* y0_y0 = {y0, y0} */
      y1_y1 = vmovh2l(y1_y1);              /* y1_y1 = {y1, y1} */

      z1_z1 = vload(Z);                    /* z1_z1 = {z1, z0} */
      z0_z0 = vdup0(z1_z1);                /* z0_z0 = {z0, z0} */
      z1_z1 = vmovh2l(z1_z1);              /* z1_z1 = {z1, z1} */
      for (i=0; i < M2; i += 2)
      {
         x0_x1 = vload(X+i);             /* x0_x1 = {x1, x0} */
         p0 = _mm_mul_pd(x0_x1, y0_y0);  /* p0    = {x1*y0, x0*y0} */
         a00 = vloada(A0+i);             /* a00 = {a10, a00} */
         p1 = _mm_mul_pd(x0_x1, y1_y1);  /* p1    = {x1*y1, x0*y1} */
         a00 = _mm_add_pd(a00, p0);      /* a00   = {a10+x1*y0,a00+x0*y0} */
         a01 = vloada(A1+i);             /* a01   = {a11, a01} */
         a01 = _mm_add_pd(a01, p1);      /* a01   = {a11+x1*y1,a10+x0*y1} */
         w0_w1 = vload(W+i);             /* w0_w1 = {w1, w0} */
         p0 = _mm_mul_pd(w0_w1, z0_z0);  /* p0    = {w1*z0, w0*z0} */
         a00 = _mm_add_pd(a00, p0); /* a00={a10+x1*y0+w1*z0,a00+x0*y0+w0*z0} */
         vstorea(A0+i, a00);
         p1 = _mm_mul_pd(w0_w1, z1_z1);  /* p1    = {w1*z1, w0*z1} */
         a01 = _mm_add_pd(a01, p1); /* a01={a11+x1*y1+w1*z1,a10+x0*y1+w0*z1} */
         vstorea(A1+i, a01);
      }
      if (M2 != M)
      {
         x0_x1 = sload(X+i);             /* x0_x1 = {XX, x0} */
         p0 = _mm_mul_sd(x0_x1, y0_y0);  /* p0    = {XXXXX, x0*y0} */
         a00 = sload(A0+i);              /* a00 = {XXX, a00} */
         p1 = _mm_mul_sd(x0_x1, y1_y1);  /* p1    = {XXXXX, x0*y1} */
         a00 = _mm_add_sd(a00, p0);      /* a00   = {XXXXXXXXX,a00+x0*y0} */
         a01 = sload(A1+i);              /* a01   = {XXX, a01} */
         a01 = _mm_add_pd(a01, p1);      /* a01   = {a11+x1*y1,a10+x0*y1} */
         w0_w1 = sload(W+i);             /* w0_w1 = {XX, w0} */
         p0 = _mm_mul_sd(w0_w1, z0_z0);  /* p0    = {XXXXX, w0*z0} */
         a00 = _mm_add_sd(a00, p0); /* a00={XXXXXXXXXXXXXXX,a00+x0*y0+w0*z0} */
         sstore(A0+i, a00);
         p1 = _mm_mul_sd(w0_w1, z1_z1);  /* p1    = {XXXXX, w0*z1} */
         a01 = _mm_add_sd(a01, p1); /* a01={XXXXXXXXXXXXXXX,a10+x0*y1+w0*z1} */
         sstore(A1+i, a01);
      }
   }
}
@ROUT ATL_zger2k_2x1_sse3
@extract -b @(topd)/cw.inc lang=c -define date 2010
#include "atlas_asm.h"
#ifndef ATL_GAS_x8664
   #error "This kernel requires x86-64 assembly!"
#endif
/*
 * Register usage
 */
#define M       %rdi  /* already in */
#define N       %rsi  /* already in */
#define II      %rax  /* loaded in loop */
#define pX      %rdx  /* already in */
#define pA0     %r11  /* 56(%rsp) */
#define pA1     %rbx  /* computed */
#define pY      %rcx  /* already in */
#define pZ      %r9   /* already in */
#define lda     %r10  /* 16(%rsp) */
#define pW      %r8   /* already in */

#define a0      %xmm0
#define x0      %xmm1
#define revx0   %xmm2
#define a1      %xmm3
#define A0      %xmm4
#define y0r     %xmm5
#define y0i     %xmm6
#define y1r     %xmm7
#define y1i     %xmm8
#define z0r     %xmm9
#define z0i     %xmm10
#define z1r     %xmm11
#define z1i     %xmm12
#define vposneg %xmm15

#define movapd movaps
/*
void ATL_UGER2K
          %rdi        %rsi           %rdx           %rcx
   (ATL_CINT M, ATL_CINT N, const TYPE *X, const TYPE *Y,
              %r8            %r9  8(%rsp)       16(%rsp)
    const TYPE *W, const TYPE *Z, TYPE *A, ATL_CINT lda);
*/
.text
.global ATL_asmdecor(ATL_UGER2K)
ALIGN64
ATL_asmdecor(ATL_UGER2K):

/* 
 * construct vector that has -1.0 in low word, and 1.0 in high word
 */
   fld1               /* ST = 1.0 */
   fstl -16(%rsp)
   fchs               /* ST = -1.0 */
   fstpl  -24(%rsp)
   movapd -24(%rsp), vposneg      /* vposneg = {1.0, -1.0} */
/*
 * Save callee-saved regs
 */
   movq %rbx, -8(%rsp)
/*
 * Load & compute all integer variables
 */
   movslq 16(%rsp), lda
   shl  $4, lda         /* lda *= sizeof */
   movq 8(%rsp), pA0
   lea (pA0, lda), pA1   /* pA1 = pA0 + lda */

   lea -2(M,M), M              /* M = 2(M-1) */
//   add M, M                     /* M = 2*M */
   lea (pX, M, 8), pX           /* pX += 2*M */
   lea (pW, M, 8), pW           /* pW += 2*M */
   lea (pA0, M, 8), pA0         /* pA0 += 2*M */
   lea (pA0, lda), pA1          /* pA1 next column over */
   neg M                        /* M = -M */
/*
 * We assume N is a multiple of 2 for this loop
 */
   LOOPN:
      movddup (pY), y0r         /* y0r = {y0r, y0r} */
      movddup 8(pY), y0i        /* y0i = {y0i, y0i} */
      mulpd   vposneg, y0i      /* y0i = {y0i,-y0i} */
      movddup 16(pY), y1r       /* y1r = {y1r, y1r} */
      movddup 24(pY), y1i       /* y1i = {y1i, y1i} */
      mulpd   vposneg, y1i      /* y0i = {y1i,-y1i} */
         movapd (pX,M,8), x0           /* x0 = {x0i, x0r} */
      add     $32, pY
#
      movddup (pZ), z0r         /* z0r = {z0r, z0r} */
      movddup 8(pZ), z0i        /* z0i = {z0i, z0i} */
      mulpd   vposneg, z0i      /* z0i = {z0i,-z0i} */
      movddup 16(pZ), z1r       /* z1r = {z1r, z1r} */
      movddup 24(pZ), z1i       /* z1i = {z1i, z1i} */
      mulpd   vposneg, z1i      /* z0i = {z1i,-z1i} */
      add     $32, pZ
         pshufd $0x4E, x0, revx0        /* revx0 = {x0r, x0i} */
      mov M, II

ALIGN32
      LOOPM:
/*
 *       Reuse X to compute rank-1 update x*y for 2 columns of A
 */
         movapd x0, a0          /* a0 = {x0i, x0r} */
         mulpd  y0r, a0         /* a0 = {x0i*y0r, x0r*y0r} */
         addpd  (pA0,II,8), a0  /* a0 = {a0i+x0i*y0r, a0r+x0r*y0r} */

         movapd revx0, A0      /* A0 = {x0r, x0i} */
         mulpd  y0i, A0        /* A0 = {x0r*y0i, -x0i*y0i} */
         addpd  A0, a0;     /* a0 = {a0i+x0i*y0r+x0r*y0i,a0r+x0r*y0r-x0i*x0i} */

         movapd x0, a1          /* a1 = {x0i, x0r} */
                movapd (pW,II,8), x0    /* x0 = {w0i, w0r} */
         mulpd  y1r, a1        /* a1 = {x0i*y1r, x0r*y1r} */
         addpd  (pA1,II,8), a1 /* a1 = {A1i+x0i*y1r, A1r+x0r*y1r} */
         mulpd  y1i, revx0     /* rx0= {x0r*y1i, -x0i*y1i} */
         addpd  revx0, a1   /* a1={A1i+x0i*y1r+x0r*y1i, A1r+x0r*y1r-x0i*y1i} */
                pshufd $0x4E, x0, revx0        /* revx0 = {w0r, w0i} */
/*
 *       Reuse W to compute rank-1 update w*z for 2 columns of A to complete
 *       the rank-2 update of these two column elements
 */

         movapd x0, A0         /* A0 = {w0i, w0r} */
         mulpd  z0r, A0        /* A0 = {w0i*z0r, w0r*z0r} */
         addpd  A0, a0         /* a0 = {a0i+w0i*z0r, a0r+w0r*z0r} */

         movapd revx0, A0      /* A0 = {w0r, w0i} */
         mulpd  z0i, A0        /* A0 = {w0r*z0i, -w0i*z0i} */
         addpd  A0, a0         /* a0 = completed rank-2 update */
         movapd a0, (pA0,II,8) /* store completed rank-2 update */

         mulpd z1r, x0         /* x0 = {w0i*z1r, w0r*z1r} */
         addpd x0, a1          /* a1 = {a1i+w0i*z1r, a1r+w0r*z1r} */
                movapd 16(pX,II,8), x0           /* x0 = {x0i, x0r} */

         mulpd z1i, revx0      /* revx0={w0r*z1i, -w0i*z1i} */
         addpd revx0, a1       /* completed rank-2 update for a1 */
                pshufd $0x4E, x0, revx0        /* revx0 = {x0r, x0i} */
         movapd a1, (pA1,II,8)

      add       $2, II
      jnz LOOPM
/*    
 *    ==================
 *    Drain X fetch pipe
 *    ==================
 */
/*
 *    Reuse X to compute rank-1 update x*y for 2 columns of A
 */
      movapd x0, a0          /* a0 = {x0i, x0r} */
      mulpd  y0r, a0         /* a0 = {x0i*y0r, x0r*y0r} */
      addpd  (pA0,II,8), a0  /* a0 = {a0i+x0i*y0r, a0r+x0r*y0r} */

      movapd revx0, A0      /* A0 = {x0r, x0i} */
      mulpd  y0i, A0        /* A0 = {x0r*y0i, -x0i*y0i} */
      addpd  A0, a0;     /* a0 = {a0i+x0i*y0r+x0r*y0i,a0r+x0r*y0r-x0i*x0i} */

      movapd x0, a1          /* a1 = {x0i, x0r} */
             movapd (pW,II,8), x0    /* x0 = {w0i, w0r} */
      mulpd  y1r, a1        /* a1 = {x0i*y1r, x0r*y1r} */
      addpd  (pA1,II,8), a1 /* a1 = {A1i+x0i*y1r, A1r+x0r*y1r} */
      mulpd  y1i, revx0     /* rx0= {x0r*y1i, -x0i*y1i} */
      addpd  revx0, a1   /* a1={A1i+x0i*y1r+x0r*y1i, A1r+x0r*y1r-x0i*y1i} */
             pshufd $0x4E, x0, revx0        /* revx0 = {w0r, w0i} */
/*
 *    Reuse W to compute rank-1 update w*z for 2 columns of A to complete
 *    the rank-2 update of these two column elements
 */

      movapd x0, A0         /* A0 = {w0i, w0r} */
      mulpd  z0r, A0        /* A0 = {w0i*z0r, w0r*z0r} */
      addpd  A0, a0         /* a0 = {a0i+w0i*z0r, a0r+w0r*z0r} */

      movapd revx0, A0      /* A0 = {w0r, w0i} */
      mulpd  z0i, A0        /* A0 = {w0r*z0i, -w0i*z0i} */
      addpd  A0, a0         /* a0 = completed rank-2 update */
      movapd a0, (pA0,II,8) /* store completed rank-2 update */

      mulpd z1r, x0         /* x0 = {w0i*z1r, w0r*z1r} */
      addpd x0, a1          /* a1 = {a1i+w0i*z1r, a1r+w0r*z1r} */

      mulpd z1i, revx0      /* revx0={w0r*z1i, -w0i*z1i} */
      addpd revx0, a1       /* completed rank-2 update for a1 */
      movapd a1, (pA1,II,8)
      lea (pA0, lda, 2), pA0
      lea (pA1, lda, 2), pA1

   sub $2, N
   jnz LOOPN

/*
 * EPILOGUE: restore registers and return
 */
   movq -8(%rsp), %rbx
#if 0
   movq %r13, -32(%rsp), %r13
   movq %r14, -40(%rsp), %r14
   movq %r15, -48(%rsp), %r15
#endif
   ret
@ROUT ATL_cger2k_2x1_sse3
@extract -b @(topd)/cw.inc lang=c -define date 2010
#include "atlas_asm.h"
#ifndef ATL_GAS_x8664
   #error "This kernel requires x86-64 assembly!"
#endif
/*
 * Register usage
 */
#define M       %rdi  /* already in */
#define N       %rsi  /* already in */
#define II      %rax  /* loaded in loop */
#define pX      %rcx  /* already in */
#define pA0     %rdx  /* 56(%rsp) */
#define pA1     %rbx  /* computed */
#define pY      %r9   /* already in */
#define incY    %r8   /* 8(%rsp) */
#define pZ      %rbp  /* 40(%rsp) */
#define lda     %r10  /* 64(%rsp) */
#define pW      %r11  /* 24(%rsp) */
#define incZ    %r12  /* 48(%rsp) */

#define a0      %xmm0
#define x0      %xmm1
#define revx0   %xmm2
#define a1      %xmm3
#define A0      %xmm4
#define y0r     %xmm5
#define y0i     %xmm6
#define y1r     %xmm7
#define y1i     %xmm8
#define z0r     %xmm9
#define z0i     %xmm10
#define z1r     %xmm11
#define z1i     %xmm12
#define vposneg %xmm15

#define movapd movaps
/*
void ATL_UGER2K
          %rdi        %rsi               %rdx           %rcx
   (ATL_CINT M, ATL_CINT N, const TYPE *alpha0, const TYPE *X,
              %r8            %r9         8(%rsp)          16(%rsp) 
    ATL_CINT incX, const TYPE *Y, ATL_CINT incY, const TYPE *beta,
          24(%rsp)       32(%rsp)       40(%rsp)   48(%rsp)
    const TYPE *W, ATL_CINT incW, const TYPE *Z, ATL_CINT incZ,
    56(%rsp)     64(%rsp)
    TYPE *A, ATL_CINT lda)
*/
.text
.global ATL_asmdecor(ATL_UGER2K)
ALIGN64
ATL_asmdecor(ATL_UGER2K):

/* 
 * construct vector that has -1.0 in low word, and 1.0 in high word
 */
   fld1               /* ST = 1.0 */
   fst  -12(%rsp)
   fst  -20(%rsp)
   fchs               /* ST = -1.0 */
   fst    -16(%rsp)
   fstp   -24(%rsp)
   movaps -24(%rsp), vposneg      /* vposneg = {1.0, -1.0, 1.0, -1.0} */
/*
 * Save callee-saved iregs 
 */
   movq %rbp, -8(%rsp)
   movq %rbx, -16(%rsp)
   movq %r12, -24(%rsp)
#if 0
   movq %r13, -32(%rsp)
   movq %r14, -40(%rsp)
   movq %r15, -48(%rsp)
#endif
/*
 * Load & compute all integer variables
 */
   movslq 64(%rsp), lda
   shl  $3, lda         /* lda *= sizeof */
   movq 56(%rsp), pA0
   lea (pA0, lda), pA1   /* pA1 = pA0 + lda */
   movslq 8(%rsp), incY
   shl $3, incY          /* incY *= sizeof */
   movq 40(%rsp), pZ
   movq 24(%rsp), pW
   movslq 48(%rsp), incZ
   shl $3, incZ          /* incZ *= sizeof */

   lea -2(M,M), M              /* M = 2(M-1) */
//   add M, M                     /* M = 2*M */
   lea (pX, M, 8), pX           /* pX += 2*M */
   lea (pW, M, 8), pW           /* pW += 2*M */
   lea (pA0, M, 8), pA0         /* pA0 += 2*M */
   lea (pA0, lda), pA1          /* pA1 next column over */
   neg M                        /* M = -M */
/*
 * We assume N is a multiple of 4 for this loop
 */
   LOOPN:
      movsldup (pY), y0r        /* y0r = {y1r, y1r, y0r, y0r} */
      movshdup (pY), y0i        /* y0i = {y1i, y1i, y0i, y0i} */
      mulpd   vposneg, y0i      /* y0i = {y1i,-y1i, y0i,-y0i} */
      movsldup 16(pY), y1r      /* y1r = {y3r, y3r, y2r, y2r} */
      movshdup 16(pY), y1i      /* y1i = {y3i, y3i, y2i, y2i} */
      mulpd   vposneg, y1i      /* y0i = {y3i,-y3i, y2i,-y2i} */
      movapd (pX,M,8), x0       /* x0 = {x1i, x1r, x0i, x0r} */
      add  $32, pY
#
      movsldup (pZ), z0r        /* z0r = {z1r, z1r, z0r, z0r} */
      movshdup (pZ), z0i        /* z0i = {z1i, z1i, z0i, z0i} */
      mulpd   vposneg, z0i      /* z0i = {z1i,-z1i, z0i,-z0i} */
      movsldup 16(pZ), z1r      /* z1r = {z3r, z3r, z2r, z2r} */
      movshdup 16(pZ), z1i      /* z1i = {z3i, z3i, z2i, z2i} */
      mulpd   vposneg, z1i      /* z0i = {z3i,-z3i, z2i,-z2i} */
      add  $32, pZ
      pshufd $0xB1, x0, revx0   /* revx0 = {x1r, x1i, x0r, x0i */
      mov M, II

ALIGN32
      LOOPM:
/*
 *       Reuse X to compute rank-1 update x*y for 4 columns of A
 */
         movaps x0, a0        /* a0 = {x1i, x1r, x0i, x0r} */
         mulps  y0r, a0       /* a0 = {x1i*y1r, x1r*x1r, x0i*y0r, x0r*y0r} */
         addps (pA0,II,8),a0  /* a0 = {   ....    a0i+x0i*y0r,a0r+x0r*y0r} */

         movaps revx0, A0  /* A0 = {x1r, x1i, x0r, x0i} */
         mulps  y0i, A0    /* A0 = {x1r*y1i,-x1i*y1i, x0r*y0i, -x0i*y0i} */
         addps  A0, a0;    /* a0 = {a0i+x0i*y0r+x0r*y0i,a0r+x0r*y0r-x0i*x0i} */

         movaps x0, a1 /* a1 = {x1i, x1r, x0i, x0r} */
                movaps (pW,II,8), x0    /* x0 = {w1i, w1r, w0i, w0r} */
         mulps  y1r, a1        /* a1 = {x1i*y3r, x1r*y3r, x0i*y2r, x0r*y2r} */
         addps  (pA1,II,8), a1 /* a1 = {..., A1i+x0i*y1r, A1r+x0r*y1r} */
HERE HERE HERE
         mulps  y1i, revx0     /* rx0= {x0r*y1i, -x0i*y1i} */
         addps  revx0, a1   /* a1={A1i+x0i*y1r+x0r*y1i, A1r+x0r*y1r-x0i*y1i} */
                pshufd $0x4E, x0, revx0        /* revx0 = {w0r, w0i} */
/*
 *       Reuse W to compute rank-1 update w*z for 2 columns of A to complete
 *       the rank-2 update of these two column elements
 */

         movaps x0, A0         /* A0 = {w0i, w0r} */
         mulps  z0r, A0        /* A0 = {w0i*z0r, w0r*z0r} */
         addps  A0, a0         /* a0 = {a0i+w0i*z0r, a0r+w0r*z0r} */

         movaps revx0, A0      /* A0 = {w0r, w0i} */
         mulps  z0i, A0        /* A0 = {w0r*z0i, -w0i*z0i} */
         addps  A0, a0         /* a0 = completed rank-2 update */
         movaps a0, (pA0,II,8) /* store completed rank-2 update */

         mulps z1r, x0         /* x0 = {w0i*z1r, w0r*z1r} */
         addps x0, a1          /* a1 = {a1i+w0i*z1r, a1r+w0r*z1r} */
                movaps 16(pX,II,8), x0           /* x0 = {x0i, x0r} */

         mulps z1i, revx0      /* revx0={w0r*z1i, -w0i*z1i} */
         addps revx0, a1       /* completed rank-2 update for a1 */
                pshufd $0x4E, x0, revx0        /* revx0 = {x0r, x0i} */
         movaps a1, (pA1,II,8)

      add       $4, II
      jnz LOOPM
/*    
 *    ==================
 *    Drain X fetch pipe
 *    ==================
 */
/*
 *    Reuse X to compute rank-1 update x*y for 2 columns of A
 */
      movapd x0, a0          /* a0 = {x0i, x0r} */
      mulpd  y0r, a0         /* a0 = {x0i*y0r, x0r*y0r} */
      addpd  (pA0,II,8), a0  /* a0 = {a0i+x0i*y0r, a0r+x0r*y0r} */

      movapd revx0, A0      /* A0 = {x0r, x0i} */
      mulpd  y0i, A0        /* A0 = {x0r*y0i, -x0i*y0i} */
      addpd  A0, a0;     /* a0 = {a0i+x0i*y0r+x0r*y0i,a0r+x0r*y0r-x0i*x0i} */

      movapd x0, a1          /* a1 = {x0i, x0r} */
             movapd (pW,II,8), x0    /* x0 = {w0i, w0r} */
      mulpd  y1r, a1        /* a1 = {x0i*y1r, x0r*y1r} */
      addpd  (pA1,II,8), a1 /* a1 = {A1i+x0i*y1r, A1r+x0r*y1r} */
      mulpd  y1i, revx0     /* rx0= {x0r*y1i, -x0i*y1i} */
      addpd  revx0, a1   /* a1={A1i+x0i*y1r+x0r*y1i, A1r+x0r*y1r-x0i*y1i} */
             pshufd $0x4E, x0, revx0        /* revx0 = {w0r, w0i} */
/*
 *    Reuse W to compute rank-1 update w*z for 2 columns of A to complete
 *    the rank-2 update of these two column elements
 */

      movapd x0, A0         /* A0 = {w0i, w0r} */
      mulpd  z0r, A0        /* A0 = {w0i*z0r, w0r*z0r} */
      addpd  A0, a0         /* a0 = {a0i+w0i*z0r, a0r+w0r*z0r} */

      movapd revx0, A0      /* A0 = {w0r, w0i} */
      mulpd  z0i, A0        /* A0 = {w0r*z0i, -w0i*z0i} */
      addpd  A0, a0         /* a0 = completed rank-2 update */
      movapd a0, (pA0,II,8) /* store completed rank-2 update */

      mulpd z1r, x0         /* x0 = {w0i*z1r, w0r*z1r} */
      addpd x0, a1          /* a1 = {a1i+w0i*z1r, a1r+w0r*z1r} */

      mulpd z1i, revx0      /* revx0={w0r*z1i, -w0i*z1i} */
      addpd revx0, a1       /* completed rank-2 update for a1 */
      movapd a1, (pA1,II,8)
      lea (pA0, lda, 2), pA0
      lea (pA1, lda, 2), pA1

   sub $2, N
   jnz LOOPN

/*
 * EPILOGUE: restore registers and return
 */
   movq -8(%rsp), %rbp
   movq -16(%rsp), %rbx
   movq -24(%rsp), %r12
#if 0
   movq %r13, -32(%rsp), %r13
   movq %r14, -40(%rsp), %r14
   movq %r15, -48(%rsp), %r15
#endif
   ret
@ROUT ATL_dgerk_1x1_x87 ATL_dgerk_1x1_sse ATL_dgerk_2x1_sse
#include "atlas_asm.h"
#ifndef ATL_GAS_x8664
   #error "This kernel requires x86-64 assembly!"
#endif
/*
 * Register usage
 */
#define M       %rdi  /* already in */
#define N       %rsi  /* already in */
#define II      %rax  /* loaded in loop */
#define pX      %rdx  /* already in */
#define pA0     %rcx  /* 8(%rsp) */
#define lda     %rbx  /* 16(%rsp) */
#define pY      %rbp  /* moved from r9 */
@ROUT ATL_dgerk_1x1_sse ATL_dgerk_2x1_sse

#define y0      %xmm0
#define x0      %xmm1
@ROUT ATL_dgerk_1x1_x87 ATL_dgerk_1x1_sse ATL_dgerk_2x1_sse

@skip #define movapd movaps
/*
void ATL_UGERK
          %rdi        %rsi              %xmm0           %rdx           
   (ATL_CINT M, ATL_CINT N, const TYPE alpha0, const TYPE *X,
             %rcx            %r8            %r9  8(%rsp)  16(%rsp)
    ATL_CINT incX, const TYPE *Y, ATL_CINT incY, TYPE *A, ATL_CINT lda)
    
*/
.text
.global ATL_asmdecor(ATL_UGERK)
ALIGN64
ATL_asmdecor(ATL_UGERK):

/*
 * Save callee-saved iregs
 */
   movq %rbp, -8(%rsp)
   movq %rbx, -16(%rsp)
#if 0
   movq %r12, -24(%rsp)
   movq %r13, -32(%rsp)
   movq %r14, -40(%rsp)
   movq %r15, -48(%rsp)
#endif
/*
 * Load & compute all integer variables
 */
@ROUT ATL_dgerk_1x1_x87 `   finit                        /* clear fp stack */`
   movq 8(%rsp), pA0
   movq %r8, pY
   movslq 16(%rsp), lda
   shl  $3, lda                /* lda *= sizeof */

   lea (pX, M, 8), pX           /* pX += M */
   lea (pA0, M, 8), pA0         /* pA0 += M */
   neg M                        /* M = -M */
   lea (pY, N, 8), pY           /* pY += N */
   neg N                        /* N = -N */
/*
 * Start N-loop
 */
   LOOPN:
@ROUT ATL_dgerk_2x1_sse `      movddup (pY,N,8), y0     /* y0 = {y0, y0} */`
@ROUT ATL_dgerk_1x1_sse `      movsd (pY,N,8), y0       /* y0 = {xx, y0} */`
@ROUT ATL_dgerk_1x1_x87 `      fldl (pY,N,8)            /* ST = y0 */`
      mov M, II

ALIGN32
      LOOPM:
@ROUT ATL_dgerk_1x1_x87
         fldl (pX,II,8)        /* ST = x0 y0 */
         fmul %st(1), %st      /* ST = x0*y0 y0 */
         faddl (pA0,II,8)      /* ST = a00+x0*y0 y0 */
         fstpl (pA0,II,8)      /* ST = y0 */
@ROUT ATL_dgerk_2x1_sse
         movapd (pX,II,8), x0  /* x0 = {x1, x0} */
         mulpd y0, x0          /* x0 = {x1*y0, x0*y0} */
         addpd (pA0,II,8), x0  /* x0 = {a10+x1*y0, a00+x0*y0} */
         movapd x0, (pA0,II,8)
@ROUT ATL_dgerk_1x1_sse
         movsd (pX,II,8), x0   /* x0 = {xx, x0} */
         mulsd y0, x0          /* x0 = {xx, x0*y0} */
         addsd (pA0,II,8), x0  /* x0 = {xx, a00+x0*y0} */
         movsd x0, (pA0,II,8)
@ROUT ATL_dgerk_1x1_x87 ATL_dgerk_1x1_sse
      add       $1, II
@ROUT ATL_dgerk_2x1_sse
      add       $2, II
@ROUT ATL_dgerk_1x1_x87 ATL_dgerk_1x1_sse ATL_dgerk_2x1_sse
      jnz LOOPM

@ROUT ATL_dgerk_1x1_x87 `      fstp %st                /* ST = nothing */`
      add lda, pA0
   add $1, N
   jnz LOOPN

/*
 * EPILOGUE: restore registers and return
 */
   movq -8(%rsp), %rbp
   movq -16(%rsp), %rbx
#if 0
   movq -24(%rsp), %r12
   movq %r13, -32(%rsp), %r13
   movq %r14, -40(%rsp), %r14
   movq %r15, -48(%rsp), %r15
#endif
   ret
@ROUT ATL_dgerk_1x2_x87 ATL_dgerk_1x2_sse ATL_dgerk_2x2_sse
@ROUT @push
#include "atlas_asm.h"
#ifndef ATL_GAS_x8664
   #error "This kernel requires x86-64 assembly!"
#endif
/*
 * Register usage
 */
#define M       %rdi  /* already in */
#define N       %rsi  /* already in */
#define II      %rax  /* loaded in loop */
#define pX      %rdx  /* already in */
#define pA0     %rcx  /* 8(%rsp) */
#define pA1     %rbp  /* computed */
#define lda     %rbx  /* 16(%rsp) */
#define pY      %r9   /* already in */

#define x0      %xmm0
#define y0      %xmm1
#define y1      %xmm2
#define x0b     %xmm3

#define movapd movaps
/*
void ATL_UGERK
          %rdi        %rsi              %xmm0           %rdx           
   (ATL_CINT M, ATL_CINT N, const TYPE alpha0, const TYPE *X,
             %rcx            %r8            %r9  8(%rsp)  16(%rsp)
    ATL_CINT incX, const TYPE *Y, ATL_CINT incY, TYPE *A, ATL_CINT lda)
    
*/
.text
.global ATL_asmdecor(ATL_UGERK)
ALIGN64
ATL_asmdecor(ATL_UGERK):

/*
 * Save callee-saved iregs
 */
   movq %rbp, -8(%rsp)
   movq %rbx, -16(%rsp)
#if 0
   movq %r12, -24(%rsp)
   movq %r13, -32(%rsp)
   movq %r14, -40(%rsp)
   movq %r15, -48(%rsp)
#endif
/*
 * Load & compute all integer variables
 */
@ROUT ATL_dgerk_1x2_x87 `   finit                        /* clear fp stack */`
   movq 8(%rsp), pA0
   movq %r8, pY
   movslq 16(%rsp), lda
   shl  $3, lda                /* lda *= 2*sizeof */

   lea (pX, M, 8), pX           /* pX += M */
   lea (pA0, M, 8), pA0         /* pA0 += M */
   neg M                        /* M = -M */
   lea (pY, N, 8), pY           /* pY += N */
   lea (pA0, lda), pA1          /* pA1 = pA0 + lda */
   neg N                        /* N = -N */
/*
 * Start N-loop
 */
   LOOPN:
@ROUT ATL_dgerk_2x2_sse 
      movddup (pY,N,8), y0     /* y0 = {y0, y0} */
      movddup 8(pY,N,8), y1    /* y1 = {y1, y1} */
@ROUT ATL_dgerk_1x2_sse 
      movsd (pY,N,8), y0       /* y0 = {xx, y0} */
      movsd 8(pY,N,8), y1      /* y0 = {xx, y1} */
@ROUT ATL_dgerk_1x2_x87 
      fldl (pY,N,8)            /* ST = y0 */
      fldl 8(pY,N,8)           /* ST = y1 y0 */
@ROUT @PEEK
      mov M, II

ALIGN32
      LOOPM:
@ROUT ATL_dgerk_1x2_x87
         fldl (pX,II,8)        /* ST = x0 y1 y0 */
         fldl %st              /* ST = x0 x0 y1 y0 */
         fmul %st(3), %st      /* ST = x0*y0 x0 y1 y0 */
         faddl (pA0,II,8)      /* ST = a00+x0*y0 x0 y1 y0 */
         fstpl (pA0,II,8)      /* ST = x0 y1 y0 */
         fmul %st(1), %st      /* ST = x0*y1, y1, y0 */
         faddl (pA1,II,8)      /* ST = a01+x0*y1 y1, y0 */
         fstpl (pA1,II,8)      /* ST = y1, y0 */
@ROUT ATL_dgerk_2x2_sse
         movapd (pX,II,8), x0  /* x0 = {x1, x0} */
         movapd x0, x0b        /* x0b= {x1, x0} */
         mulpd y0, x0          /* x0 = {x1*y0, x0*y0} */
         addpd (pA0,II,8), x0  /* x0 = {a10+x1*y0, a00+x0*y0} */
         movapd x0, (pA0,II,8)
         mulpd y1, x0b         /* x0b= {x1*y1, x0*y1} */
         addpd (pA1,II,8), x0b /* x0b= {a11+x1*y1, a01+x0*y1} */
         movapd x0b, (pA1,II,8)
@ROUT ATL_dgerk_1x2_sse
         movsd (pX,II,8), x0   /* x0 = {xx, x0} */
         movapd x0, x0b        /* x0b= {xx, x0} */
         mulsd y0, x0          /* x0 = {xx, x0*y0} */
         addsd (pA0,II,8), x0  /* x0 = {xx, a00+x0*y0} */
         movsd x0, (pA0,II,8)
         mulsd y1, x0b         /* x0b= {xx, x1*y0} */
         addsd (pA1,II,8), x0b /* x0b= {xx, a01+x0*y1} */
         movsd x0b, (pA1,II,8)
@ROUT ATL_dgerk_1x2_x87 ATL_dgerk_1x2_sse
      add       $1, II
@ROUT ATL_dgerk_2x2_sse
      add       $2, II
@ROUT @peek
      jnz LOOPM

@ROUT ATL_dgerk_1x2_x87 
      fstp %st                /* ST = y0 */
      fstp %st                /* ST = nothing */
@ROUT @peek
      lea (pA0, lda,2), pA0
      lea (pA1, lda,2), pA1
   add $2, N
   jnz LOOPN

/*
 * EPILOGUE: restore registers and return
 */
   movq -8(%rsp), %rbp
   movq -16(%rsp), %rbx
#if 0
   movq -24(%rsp), %r12
   movq %r13, -32(%rsp), %r13
   movq %r14, -40(%rsp), %r14
   movq %r15, -48(%rsp), %r15
#endif
   ret
@ROUT memgerB.idx memmvtB.idx
   @define dir @backwards@
@ROUT memgerF.idx memmvtF.idx
   @define dir @straight@
@ROUT memgerF.idx memgerB.idx
ID=2001 XU=8 YU=1 AUTH='R. Clint Whaley' ROUT='ATL_dgerk_CLx1_touch.c' \
  alignA=16 alignX=16 alignY=16 FYU=1 COMP='gcc' CFLAGS='-x assembler-with-cpp'
ID=2002 XU=8 YU=2 AUTH='R. Clint Whaley' ROUT='ATL_dgerk_CLx2_touch.c' \
  alignA=16 alignX=16 alignY=16 FYU=1 COMP='gcc' CFLAGS='-x assembler-with-cpp'
ID=2003 XU=8 YU=4 AUTH='R. Clint Whaley' ROUT='ATL_dgerk_CLx4_touch.c' \
  alignA=16 alignX=16 alignY=16 FYU=1 COMP='gcc' CFLAGS='-x assembler-with-cpp'
ID=2004 XU=8 YU=6 AUTH='R. Clint Whaley' ROUT='ATL_dgerk_CLx6_touch.c' \
  alignA=16 alignX=16 alignY=16 FYU=1 COMP='gcc' CFLAGS='-x assembler-with-cpp'
ID=2005 XU=8 YU=8 AUTH='R. Clint Whaley' ROUT='ATL_dgerk_CLx8_touch.c' \
  alignA=16 alignX=16 alignY=16 FYU=1 COMP='gcc' CFLAGS='-x assembler-with-cpp'
ID=2006 XU=8 YU=10 AUTH='R. Clint Whaley' ROUT='ATL_dgerk_CLx10_touch.c' \
  alignA=16 alignX=16 alignY=16 FYU=1 COMP='gcc' CFLAGS='-x assembler-with-cpp'
ID=2007 XU=8 YU=12 AUTH='R. Clint Whaley' ROUT='ATL_dgerk_CLx12_touch.c' \
  alignA=16 alignX=16 alignY=16 FYU=1 COMP='gcc' CFLAGS='-x assembler-with-cpp'
ID=2008 XU=8 YU=14 AUTH='R. Clint Whaley' ROUT='ATL_dgerk_CLx14_touch.c' \
  alignA=16 alignX=16 alignY=16 FYU=1 COMP='gcc' CFLAGS='-x assembler-with-cpp'
@ROUT memmvtF.idx
   @define ID @5000@
   @define rt @mvt@
@ROUT memmvtB.idx
   @define ID @5500@
   @define rt @mvt@
@ROUT memgerF.idx
   @define ID @3000@
   @define rt @ger@
@ROUT memgerB.idx
   @define ID @3500@
   @define rt @ger@
@ROUT memgerF.idx memmvtF.idx memgerB.idx memmvtB.idx
@define nu @1@
@iwhile nu < 16
   @ROUT memmvtF.idx memmvtB.idx
ID=@(ID) XU=8 YU=@(nu) AUTH='R. Clint Whaley' ROUT='ATL_@(rt)CL_1x@(nu).c' \
  alignA=16 alignX=16 alignY=16 FYU=1 COMP='gcc' CFLAGS='-x assembler-with-cpp'
      @iexp ID 1 @(ID) +
   @ROUT memgerF.idx memmvtF.idx memgerB.idx memmvtB.idx
   @define mu @2@
   @iwhile mu < 64
      @iexp mu8 @(mu) 8 *
ID=@(ID) XU=@(mu8) YU=@(nu) AUTH='R. Clint Whaley' ROUT='ATL_@(rt)CL_@(mu)@(dir)x@(nu).c' \
  alignA=16 alignX=16 alignY=16 FYU=1 COMP='gcc' CFLAGS='-x assembler-with-cpp'
      @iexp ID 1 @(ID) +
      @iexp mu 2 @(mu) *
   @endiwhile
   @iif nu = 1
       @undef nu
       @define nu @0@
   @endiif
   @iexp nu 2 @(nu) +
@endiwhile
@beginskip
ID=3001 XU=16 YU=1 AUTH='R. Clint Whaley' ROUT='ATL_@(rt)CL_2straightx1.c' \
  alignA=16 alignX=16 alignY=16 FYU=1 COMP='gcc' CFLAGS='-x assembler-with-cpp'
ID=3002 XU=16 YU=1 AUTH='R. Clint Whaley' ROUT='ATL_@(rt)CL_2backwardsx1.c' \
  alignA=16 alignX=16 alignY=16 FYU=1 COMP='gcc' CFLAGS='-x assembler-with-cpp'
ID=3003 XU=32 YU=1 AUTH='R. Clint Whaley' ROUT='ATL_@(rt)CL_4straightx1.c' \
  alignA=16 alignX=16 alignY=16 FYU=1 COMP='gcc' CFLAGS='-x assembler-with-cpp'
ID=3004 XU=32 YU=1 AUTH='R. Clint Whaley' ROUT='ATL_@(rt)CL_4backwardsx1.c' \
  alignA=16 alignX=16 alignY=16 FYU=1 COMP='gcc' CFLAGS='-x assembler-with-cpp'
@endskip
@ROUT dr1nucases.idx
ID=1000 XU=1 YU=1 AUTH='R. Clint Whaley' ROUT='ATL_dgerk_1x1_x87.c' \
   alignA=16 alignX=16 alignY=16 FYU=1 \
   COMP='gcc' CFLAGS='-x assembler-with-cpp'
ID=1001 XU=1 YU=1 AUTH='R. Clint Whaley' ROUT='ATL_dgerk_1x1_sse.c' \
   SSE=3 alignA=16 alignX=16 alignY=16 FYU=1 \
   COMP='gcc' CFLAGS='-x assembler-with-cpp'
ID=1002 XU=2 YU=1 AUTH='R. Clint Whaley' ROUT='ATL_dgerk_2x1_sse.c' \
   SSE=3 alignA=16 alignX=16 alignY=16 FYU=1 \
   COMP='gcc' CFLAGS='-x assembler-with-cpp'
#
ID=1003 XU=1 YU=2 AUTH='R. Clint Whaley' ROUT='ATL_dgerk_1x2_x87.c' \
   alignA=16 alignX=16 alignY=16 FYU=1 \
   COMP='gcc' CFLAGS='-x assembler-with-cpp'
ID=1004 XU=1 YU=2 AUTH='R. Clint Whaley' ROUT='ATL_dgerk_1x2_sse.c' \
   SSE=3 alignA=16 alignX=16 alignY=16 FYU=1 \
   COMP='gcc' CFLAGS='-x assembler-with-cpp'
ID=1005 XU=2 YU=2 AUTH='R. Clint Whaley' ROUT='ATL_dgerk_2x2_sse.c' \
   SSE=3 alignA=16 alignX=16 alignY=16 FYU=1 \
   COMP='gcc' CFLAGS='-x assembler-with-cpp'
#
ID=1006 XU=1 YU=4 AUTH='R. Clint Whaley' ROUT='ATL_dgerk_1x4_x87.c' \
   alignA=16 alignX=16 alignY=16 FYU=1 \
   COMP='gcc' CFLAGS='-x assembler-with-cpp'
ID=1007 XU=1 YU=4 AUTH='R. Clint Whaley' ROUT='ATL_dgerk_1x4_sse.c' \
   SSE=3 alignA=16 alignX=16 alignY=16 FYU=1 \
   COMP='gcc' CFLAGS='-x assembler-with-cpp'
ID=1008 XU=2 YU=4 AUTH='R. Clint Whaley' ROUT='ATL_dgerk_2x4_sse.c' \
   SSE=3 alignA=16 alignX=16 alignY=16 FYU=1 \
   COMP='gcc' CFLAGS='-x assembler-with-cpp'
#
ID=1009 XU=1 YU=6 AUTH='R. Clint Whaley' ROUT='ATL_dgerk_1x6_x87.c' \
   alignA=16 alignX=16 alignY=16 FYU=1 \
   COMP='gcc' CFLAGS='-x assembler-with-cpp'
ID=1010 XU=1 YU=6 AUTH='R. Clint Whaley' ROUT='ATL_dgerk_1x6_sse.c' \
   SSE=3 alignA=16 alignX=16 alignY=16 FYU=1 \
   COMP='gcc' CFLAGS='-x assembler-with-cpp'
ID=1011 XU=2 YU=6 AUTH='R. Clint Whaley' ROUT='ATL_dgerk_2x6_sse.c' \
   SSE=3 alignA=16 alignX=16 alignY=16 FYU=1 \
   COMP='gcc' CFLAGS='-x assembler-with-cpp'
#
ID=1012 XU=1 YU=8 AUTH='R. Clint Whaley' ROUT='ATL_dgerk_1x8_sse.c' \
   SSE=3 alignA=16 alignX=16 alignY=16 FYU=1 \
   COMP='gcc' CFLAGS='-x assembler-with-cpp'
ID=1013 XU=2 YU=8 AUTH='R. Clint Whaley' ROUT='ATL_dgerk_2x8_sse.c' \
   SSE=3 alignA=16 alignX=16 alignY=16 FYU=1 \
   COMP='gcc' CFLAGS='-x assembler-with-cpp'
#
ID=1014 XU=1 YU=10 AUTH='R. Clint Whaley' ROUT='ATL_dgerk_1x10_sse.c' \
   SSE=3 alignA=16 alignX=16 alignY=16 FYU=1 \
   COMP='gcc' CFLAGS='-x assembler-with-cpp'
ID=1015 XU=2 YU=10 AUTH='R. Clint Whaley' ROUT='ATL_dgerk_2x10_sse.c' \
   SSE=3 alignA=16 alignX=16 alignY=16 FYU=1 \
   COMP='gcc' CFLAGS='-x assembler-with-cpp'
#
ID=1015 XU=1 YU=12 AUTH='R. Clint Whaley' ROUT='ATL_dgerk_1x12_sse.c' \
   SSE=3 alignA=16 alignX=16 alignY=16 FYU=1 \
   COMP='gcc' CFLAGS='-x assembler-with-cpp'
ID=1016 XU=2 YU=12 AUTH='R. Clint Whaley' ROUT='ATL_dgerk_2x12_sse.c' \
   SSE=3 alignA=16 alignX=16 alignY=16 FYU=1 \
   COMP='gcc' CFLAGS='-x assembler-with-cpp'
#
ID=1017 XU=1 YU=14 AUTH='R. Clint Whaley' ROUT='ATL_dgerk_1x14_sse.c' \
   SSE=3 alignA=16 alignX=16 alignY=16 FYU=1 \
   COMP='gcc' CFLAGS='-x assembler-with-cpp'
ID=1018 XU=2 YU=14 AUTH='R. Clint Whaley' ROUT='ATL_dgerk_2x14_sse.c' \
   SSE=3 alignA=16 alignX=16 alignY=16 FYU=1 \
   COMP='gcc' CFLAGS='-x assembler-with-cpp'
#ID=1019 XU=8 YU=1 AUTH='R. Clint Whaley' ROUT='ATL_dgerk_8x1_Cw.c' \
#   SSE=3 alignA=16 alignX=16 alignY=16 FYU=1 \
#   COMP='gcc' CFLAGS='-x assembler-with-cpp'
@ROUT ATL_dgerk_1x4_x87 ATL_dgerk_1x4_sse ATL_dgerk_2x4_sse
@ROUT @push
#include "atlas_asm.h"
#ifndef ATL_GAS_x8664
   #error "This kernel requires x86-64 assembly!"
#endif
/*
 * Register usage
 */
#define M       %r9   /* moved from rdi */
#define N       %r10  /* moved from rsi */
#define II      %rax  /* loaded in loop */
#define pX      %rdx  /* already in */
#define pA0     %rcx  /* 8(%rsp) */
#define pA1     %rbp  /* computed */
#define pA2     %rdi  /* computed */
#define pA3     %rsi  /* computed */
#define lda     %rbx  /* 16(%rsp) */
#define pY      %r8   /* already in */

#define x0      %xmm0
#define x0b     %xmm1
#define y0      %xmm2
#define y1      %xmm3
#define y2      %xmm4
#define y3      %xmm5

#define movapd movaps
/*
void ATL_UGERK
          %rdi        %rsi              %xmm0           %rdx           
   (ATL_CINT M, ATL_CINT N, const TYPE alpha0, const TYPE *X,
             %rcx            %r8            %r9  8(%rsp)  16(%rsp)
    ATL_CINT incX, const TYPE *Y, ATL_CINT incY, TYPE *A, ATL_CINT lda)
    
*/
.text
.global ATL_asmdecor(ATL_UGERK)
ALIGN64
ATL_asmdecor(ATL_UGERK):

/*
 * Save callee-saved iregs
 */
   movq %rbp, -8(%rsp)
   movq %rbx, -16(%rsp)
#if 0
   movq %r12, -24(%rsp)
   movq %r13, -32(%rsp)
   movq %r14, -40(%rsp)
   movq %r15, -48(%rsp)
#endif
/*
 * Load & compute all integer variables
 */
@ROUT ATL_dgerk_1x4_x87 `   finit                        /* clear fp stack */`
   movq %rdi, M
   movq %rsi, N
   movq 8(%rsp), pA0
   movslq 16(%rsp), lda
   shl  $3, lda                /* lda *= 2*sizeof */

   lea (pX, M, 8), pX           /* pX += M */
   lea (pA0, M, 8), pA0         /* pA0 += M */
   neg M                        /* M = -M */
   lea (pY, N, 8), pY           /* pY += N */
   lea (pA0, lda), pA1          /* pA1 = pA0 + lda */
   lea (pA0, lda,2), pA2        /* pA2 = pA0 + 2*lda */
   lea (pA1, lda,2), pA3        /* pA3 = pA0 + 3*lda */
   neg N                        /* N = -N */
/*
 * Start N-loop
 */
   LOOPN:
@ROUT ATL_dgerk_2x4_sse 
      movddup (pY,N,8), y0     /* y0 = {y0, y0} */
      movddup 8(pY,N,8), y1    /* y1 = {y1, y1} */
      movddup 16(pY,N,8), y2   /* y2 = {y2, y2} */
      movddup 24(pY,N,8), y3   /* y3 = {y3, y3} */
@ROUT ATL_dgerk_1x4_sse 
      movsd (pY,N,8), y0       /* y0 = {xx, y0} */
      movsd 8(pY,N,8), y1      /* y1 = {xx, y1} */
      movsd 16(pY,N,8), y2     /* y2 = {xx, y2} */
      movsd 24(pY,N,8), y3     /* y3 = {xx, y3} */
@ROUT ATL_dgerk_1x4_x87 
      fldl (pY,N,8)            /* ST = y0 */
      fldl 8(pY,N,8)           /* ST = y1 y0 */
      fldl 16(pY,N,8)          /* ST = y2 y1 y0 */
      fldl 24(pY,N,8)          /* ST = y3 y2 y1 y0 */
@ROUT @PEEK
      mov M, II

ALIGN32
      LOOPM:
@ROUT ATL_dgerk_1x4_x87
         fldl (pX,II,8)        /* ST = x0 y3 y2 y1 y0 */

         fldl %st              /* ST = x0 x0 y3 y2 y1 y0 */
         fmul %st(5), %st      /* ST = x0*y0 x0 y3 y2 y1 y0 */
         faddl (pA0,II,8)      /* ST = a00+x0*y0 x0 y3 y2 y1 y0 */
         fstpl (pA0,II,8)      /* ST = x0 y3 y2 y1 y0 */

         fldl %st              /* ST = x0 x0 y3 y2 y1 y0 */
         fmul %st(4), %st      /* ST = x0*y1 x0 y3 y2 y1 y0 */
         faddl (pA1,II,8)      /* ST = a01+x0*y1 x0 y3 y2 y1 y0 */
         fstpl (pA1,II,8)      /* ST = x0 y3 y2 y1 y0 */

         fldl %st              /* ST = x0 x0 y3 y2 y1 y0 */
         fmul %st(3), %st      /* ST = x0*y2 x0 y3 y2 y1 y0 */
         faddl (pA2,II,8)      /* ST = a02+x0*y2 x0 y3 y2 y1 y0 */
         fstpl (pA2,II,8)      /* ST = x0 y3 y2 y1 y0 */

         fmul %st(1), %st      /* ST = x0*y3 y3 y2 y1 y0 */
         faddl (pA3,II,8)      /* ST = a03+x0*y2 y3 y2 y1 y0 */
         fstpl (pA3,II,8)      /* ST = y3 y2 y1 y0 */

@ROUT ATL_dgerk_2x4_sse
         movapd (pX,II,8), x0   /* x0 = {x1, x0} */

         movapd x0, x0b         /* x0b= {x1, x0} */
         mulpd y0, x0           /* x0 = {x1*y0, x0*y0} */
         addpd (pA0,II,8), x0   /* x0 = {a10+x1*y0, a00+x0*y0} */
         movapd x0, (pA0,II,8)

         movapd x0b, x0         /* x0 = {x1, x0} */
         mulpd y1, x0b          /* x0b= {x1*y1, x0*y1} */
         addpd (pA1,II,8), x0b  /* x0b= {a11+x1*y1, a01+x0*y1} */
         movapd x0b, (pA1,II,8)

         movapd x0, x0b         /* x0b= {x1, x0} */
         mulpd y2, x0           /* x0 = {x1*y2, x0*y2} */
         addpd (pA2,II,8), x0   /* x0 = {a12+x1*y2, a02+x0*y2} */
         movapd x0, (pA2,II,8)

         mulpd y3, x0b          /* x0b= {x1*y3, x0*y3} */
         addpd (pA3,II,8), x0b  /* x0b= {a13+x1*y3, a03+x0*y3} */
         movapd x0b, (pA3,II,8)

@ROUT ATL_dgerk_1x4_sse
         movsd (pX,II,8), x0   /* x0 = {xx, x0} */

         movapd x0, x0b        /* x0b= {xx, x0} */
         mulsd y0, x0          /* x0 = {xx, x0*y0} */
         addsd (pA0,II,8), x0  /* x0 = {xx, a00+x0*y0} */
         movsd x0, (pA0,II,8)

         movapd x0b, x0        /* x0= {xx, x0} */
         mulsd y1, x0b         /* x0b= {xx, x1*y0} */
         addsd (pA1,II,8), x0b /* x0b= {xx, a01+x0*y1} */
         movsd x0b, (pA1,II,8)

         movapd x0, x0b        /* x0b= {xx, x0} */
         mulsd y2, x0          /* x0 = {xx, x0*y2} */
         addsd (pA2,II,8), x0  /* x0 = {xx, a02+x0*y2} */
         movsd x0, (pA2,II,8)

         mulsd y3, x0b         /* x0b= {xx, x1*y3} */
         addsd (pA3,II,8), x0b /* x0b= {xx, a03+x0*y3} */
         movsd x0b, (pA3,II,8)

@ROUT ATL_dgerk_1x4_x87 ATL_dgerk_1x4_sse
      add       $1, II
@ROUT ATL_dgerk_2x4_sse
      add       $2, II
@ROUT @peek
      jnz LOOPM

@ROUT ATL_dgerk_1x4_x87 
      fstp %st                /* ST = y2 y1 y0 */
      fstp %st                /* ST = y1 y0 */
      fstp %st                /* ST = y0 */
      fstp %st                /* ST = nothing */
@ROUT @peek
      lea (pA0,lda,4), pA0
      lea (pA1,lda,4), pA1
      lea (pA2,lda,4), pA2
      lea (pA3,lda,4), pA3
   add $4, N
   jnz LOOPN

/*
 * EPILOGUE: restore registers and return
 */
   movq -8(%rsp), %rbp
   movq -16(%rsp), %rbx
#if 0
   movq -24(%rsp), %r12
   movq %r13, -32(%rsp), %r13
   movq %r14, -40(%rsp), %r14
   movq %r15, -48(%rsp), %r15
#endif
   ret
@ROUT ATL_dgerk_1x6_x87 ATL_dgerk_1x6_sse ATL_dgerk_2x6_sse
@ROUT @push
#include "atlas_asm.h"
#ifndef ATL_GAS_x8664
   #error "This kernel requires x86-64 assembly!"
#endif
/*
 * Register usage
 */
#define M       %r9   /* moved from rdi */
#define N       %r10  /* moved from rsi */
#define II      %rax  /* loaded in loop */
#define pX      %rdx  /* already in */
#define pA0     %rcx  /* 8(%rsp) */
#define pA1     %rbp  /* computed */
#define pA2     %rdi  /* computed */
#define pA3     %rsi  /* computed */
#define pA4     %rbx  /* computed */
#define pA5     %r11  /* computed */
#define lda     %r12  /* 16(%rsp) */
#define pY      %r8   /* already in */

#define x0      %xmm0
#define x0b     %xmm1
#define y0      %xmm2
#define y1      %xmm3
#define y2      %xmm4
#define y3      %xmm5
#define y4      %xmm6
#define y5      %xmm7

#define movapd movaps
/*
void ATL_UGERK
          %rdi        %rsi              %xmm0           %rdx           
   (ATL_CINT M, ATL_CINT N, const TYPE alpha0, const TYPE *X,
             %rcx            %r8            %r9  8(%rsp)  16(%rsp)
    ATL_CINT incX, const TYPE *Y, ATL_CINT incY, TYPE *A, ATL_CINT lda)
    
*/
.text
.global ATL_asmdecor(ATL_UGERK)
ALIGN64
ATL_asmdecor(ATL_UGERK):

/*
 * Save callee-saved iregs
 */
   movq %rbp, -8(%rsp)
   movq %rbx, -16(%rsp)
   movq %r12, -24(%rsp)
#if 0
   movq %r13, -32(%rsp)
   movq %r14, -40(%rsp)
   movq %r15, -48(%rsp)
#endif
/*
 * Load & compute all integer variables
 */
@ROUT ATL_dgerk_1x6_x87 `   finit                        /* clear fp stack */`
   movq %rdi, M
   movq %rsi, N
   movq 8(%rsp), pA0
   movslq 16(%rsp), lda
   shl  $3, lda                /* lda *= sizeof */

   lea (pX, M, 8), pX           /* pX += M */
   lea (pA0, M, 8), pA0         /* pA0 += M */
   neg M                        /* M = -M */
   lea (pY, N, 8), pY           /* pY += N */
   lea (pA0, lda), pA1          /* pA1 = pA0 + lda */
   lea (pA0, lda,2), pA2        /* pA2 = pA0 + 2*lda */
   lea (pA1, lda,2), pA3        /* pA3 = pA0 + 3*lda */
   lea (pA0, lda,4), pA4        /* pA4 = pA0 + 4*lda */
   lea (pA1, lda,4), pA5        /* pA5 = pA0 + 5*lda */
   neg N                        /* N = -N */
   lea (lda,lda,2), lda         /* lda *= 3 */
/*
 * Start N-loop
 */
   LOOPN:
@ROUT ATL_dgerk_2x6_sse 
      movddup (pY,N,8), y0     /* y0 = {y0, y0} */
      movddup 8(pY,N,8), y1    /* y1 = {y1, y1} */
      movddup 16(pY,N,8), y2   /* y2 = {y2, y2} */
      movddup 24(pY,N,8), y3   /* y3 = {y3, y3} */
      movddup 32(pY,N,8), y4   /* y4 = {y4, y4} */
      movddup 40(pY,N,8), y5   /* y5 = {y5, y5} */
@ROUT ATL_dgerk_1x6_sse 
      movsd (pY,N,8), y0       /* y0 = {xx, y0} */
      movsd 8(pY,N,8), y1      /* y1 = {xx, y1} */
      movsd 16(pY,N,8), y2     /* y2 = {xx, y2} */
      movsd 24(pY,N,8), y3     /* y3 = {xx, y3} */
      movsd 32(pY,N,8), y4     /* y4 = {xx, y4} */
      movsd 40(pY,N,8), y5     /* y5 = {xx, y5} */
@ROUT ATL_dgerk_1x6_x87 
      fldl (pY,N,8)            /* ST = y0 */
      fldl 8(pY,N,8)           /* ST = y1 y0 */
      fldl 16(pY,N,8)          /* ST = y2 y1 y0 */
      fldl 24(pY,N,8)          /* ST = y3 y2 y1 y0 */
      fldl 32(pY,N,8)          /* ST = y4 y3 y2 y1 y0 */
      fldl 40(pY,N,8)          /* ST = y5 y4 y3 y2 y1 y0 */
@ROUT @PEEK
      mov M, II

ALIGN32
      LOOPM:
@ROUT ATL_dgerk_1x6_x87
         fldl (pX,II,8)        /* ST = x0 y5 y4 y3 y2 y1 y0 */

         fldl %st              /* ST = x0 x0 y5 y4 y3 y2 y1 y0 */
         fmul %st(7), %st      /* ST = x0*y0 x0 y5 y4 y3 y2 y1 y0 */
         faddl (pA0,II,8)      /* ST = a00+x0*y0 x0 y5 y4 y3 y2 y1 y0 */
         fstpl (pA0,II,8)      /* ST = x0 y5 y4 y3 y2 y1 y0 */

         fldl %st              /* ST = x0 x0 y5 y4 y3 y2 y1 y0 */
         fmul %st(6), %st      /* ST = x0*y1 x0 y5 y4 y3 y2 y1 y0 */
         faddl (pA1,II,8)      /* ST = a01+x0*y1 x0 y5 y4 y3 y2 y1 y0 */
         fstpl (pA1,II,8)      /* ST = x0 y5 y4 y3 y2 y1 y0 */

         fldl %st              /* ST = x0 x0 y5 y4 y3 y2 y1 y0 */
         fmul %st(5), %st      /* ST = x0*y2 x0 y5 y4 y3 y2 y1 y0 */
         faddl (pA2,II,8)      /* ST = a02+x0*y2 x0 y5 y4 y3 y2 y1 y0 */
         fstpl (pA2,II,8)      /* ST = x0 y5 y4 y3 y2 y1 y0 */

         fldl %st              /* ST = x0 x0 y5 y4 y3 y2 y1 y0 */
         fmul %st(4), %st      /* ST = x0*y3 x0 y5 y4 y3 y2 y1 y0 */
         faddl (pA3,II,8)      /* ST = a03+x0*y3 x0 y5 y4 y3 y2 y1 y0 */
         fstpl (pA3,II,8)      /* ST = x0 y5 y4 y3 y2 y1 y0 */

         fldl %st              /* ST = x0 x0 y5 y4 y3 y2 y1 y0 */
         fmul %st(3), %st      /* ST = x0*y4 x0 y5 y4 y3 y2 y1 y0 */
         faddl (pA4,II,8)      /* ST = a04+x0*y4 x0 y5 y4 y3 y2 y1 y0 */
         fstpl (pA4,II,8)      /* ST = x0 y5 y4 y3 y2 y1 y0 */

         fmul %st(1), %st      /* ST = x0*y5 y5 y4 y3 y2 y1 y0 */
         faddl (pA5,II,8)      /* ST = a03+x0*y2 y5 y4 y3 y2 y1 y0 */
         fstpl (pA5,II,8)      /* ST = y5 y4 y3 y2 y1 y0 */

@ROUT ATL_dgerk_2x6_sse
         movapd (pX,II,8), x0   /* x0 = {x1, x0} */

         movapd x0, x0b         /* x0b= {x1, x0} */
         mulpd y0, x0           /* x0 = {x1*y0, x0*y0} */
         addpd (pA0,II,8), x0   /* x0 = {a10+x1*y0, a00+x0*y0} */
         movapd x0, (pA0,II,8)

         movapd x0b, x0         /* x0 = {x1, x0} */
         mulpd y1, x0b          /* x0b= {x1*y1, x0*y1} */
         addpd (pA1,II,8), x0b  /* x0b= {a11+x1*y1, a01+x0*y1} */
         movapd x0b, (pA1,II,8)

         movapd x0, x0b         /* x0b= {x1, x0} */
         mulpd y2, x0           /* x0 = {x1*y2, x0*y2} */
         addpd (pA2,II,8), x0   /* x0 = {a12+x1*y2, a02+x0*y2} */
         movapd x0, (pA2,II,8)

         movapd x0b, x0         /* x0 = {x1, x0} */
         mulpd y3, x0b          /* x0b= {x1*y3, x0*y1} */
         addpd (pA3,II,8), x0b  /* x0b= {a13+x1*y3, a03+x0*y3} */
         movapd x0b, (pA3,II,8)

         movapd x0, x0b         /* x0b= {x1, x0} */
         mulpd y4, x0           /* x0 = {x1*y4, x0*y4} */
         addpd (pA4,II,8), x0   /* x0 = {a12+x1*y4, a04+x0*y4} */
         movapd x0, (pA4,II,8)

         mulpd y5, x0b          /* x0b= {x1*y3, x0*y3} */
         addpd (pA5,II,8), x0b  /* x0b= {a15+x1*y5, a05+x0*y5} */
         movapd x0b, (pA5,II,8)

@ROUT ATL_dgerk_1x6_sse
         movsd (pX,II,8), x0   /* x0 = {xx, x0} */

         movapd x0, x0b        /* x0b= {xx, x0} */
         mulsd y0, x0          /* x0 = {xx, x0*y0} */
         addsd (pA0,II,8), x0  /* x0 = {xx, a00+x0*y0} */
         movsd x0, (pA0,II,8)

         movapd x0b, x0        /* x0= {xx, x0} */
         mulsd y1, x0b         /* x0b= {xx, x0*y1} */
         addsd (pA1,II,8), x0b /* x0b= {xx, a01+x0*y1} */
         movsd x0b, (pA1,II,8)

         movapd x0, x0b        /* x0b= {xx, x0} */
         mulsd y2, x0          /* x0 = {xx, x0*y2} */
         addsd (pA2,II,8), x0  /* x0 = {xx, a02+x0*y2} */
         movsd x0, (pA2,II,8)

         movapd x0b, x0        /* x0= {xx, x0} */
         mulsd y3, x0b         /* x0b= {xx, x1*y3} */
         addsd (pA3,II,8), x0b /* x0b= {xx, a01+x0*y3} */
         movsd x0b, (pA3,II,8)

         movapd x0, x0b        /* x0b= {xx, x0} */
         mulsd y4, x0          /* x0 = {xx, x0*y4} */
         addsd (pA4,II,8), x0  /* x0 = {xx, a04+x0*y4} */
         movsd x0, (pA4,II,8)

         mulsd y5, x0b         /* x0b= {xx, x1*y5} */
         addsd (pA5,II,8), x0b /* x0b= {xx, a05+x0*y5} */
         movsd x0b, (pA5,II,8)

@ROUT ATL_dgerk_1x6_x87 ATL_dgerk_1x6_sse
      add       $1, II
@ROUT ATL_dgerk_2x6_sse
      add       $2, II
@ROUT @peek
      jnz LOOPM

@ROUT ATL_dgerk_1x6_x87 
      fstp %st                /* ST = y4 y3 y2 y1 y0 */
      fstp %st                /* ST = y3 y2 y1 y0 */
      fstp %st                /* ST = y2 y1 y0 */
      fstp %st                /* ST = y1 y0 */
      fstp %st                /* ST = y0 */
      fstp %st                /* ST = nothing */
@ROUT @peek
      lea (pA0,lda,2), pA0
      lea (pA1,lda,2), pA1
      lea (pA2,lda,2), pA2
      lea (pA3,lda,2), pA3
      lea (pA4,lda,2), pA4
      lea (pA5,lda,2), pA5
   add $6, N
   jnz LOOPN

/*
 * EPILOGUE: restore registers and return
 */
   movq -8(%rsp), %rbp
   movq -16(%rsp), %rbx
   movq -24(%rsp), %r12
#if 0
   movq %r13, -32(%rsp), %r13
   movq %r14, -40(%rsp), %r14
   movq %r15, -48(%rsp), %r15
#endif
   ret
@ROUT ATL_dgerk_1x8_sse ATL_dgerk_2x8_sse
@ROUT @push
#include "atlas_asm.h"
#ifndef ATL_GAS_x8664
   #error "This kernel requires x86-64 assembly!"
#endif
/*
 * Register usage
 */
#define M       %r9   /* moved from rdi */
#define N       %r10  /* moved from rsi */
#define II      %rax  /* loaded in loop */
#define pX      %rdx  /* already in */
#define pA0     %rcx  /* 8(%rsp) */
#define pA1     %rbp  /* computed */
#define pA2     %rdi  /* computed */
#define pA3     %rsi  /* computed */
#define pA4     %rbx  /* computed */
#define pA5     %r11  /* computed */
#define pA6     %r13 
#define pA7     %r14
#define lda     %r12  /* 16(%rsp) */
#define pY      %r8   /* already in */

#define x0      %xmm0
#define x0b     %xmm1
#define y0      %xmm2
#define y1      %xmm3
#define y2      %xmm4
#define y3      %xmm5
#define y4      %xmm6
#define y5      %xmm7
#define y6      %xmm8
#define y7      %xmm9

#define movapd movaps
/*
void ATL_UGERK
          %rdi        %rsi              %xmm0           %rdx           
   (ATL_CINT M, ATL_CINT N, const TYPE alpha0, const TYPE *X,
             %rcx            %r8            %r9  8(%rsp)  16(%rsp)
    ATL_CINT incX, const TYPE *Y, ATL_CINT incY, TYPE *A, ATL_CINT lda)
    
*/
.text
.global ATL_asmdecor(ATL_UGERK)
ALIGN64
ATL_asmdecor(ATL_UGERK):

/*
 * Save callee-saved iregs
 */
   movq %rbp, -8(%rsp)
   movq %rbx, -16(%rsp)
   movq %r12, -24(%rsp)
   movq %r13, -32(%rsp)
   movq %r14, -40(%rsp)
#if 0
   movq %r15, -48(%rsp)
#endif
/*
 * Load & compute all integer variables
 */
   movq %rdi, M
   movq %rsi, N
   movq 8(%rsp), pA0
   movslq 16(%rsp), lda
   shl  $3, lda                /* lda *= sizeof */

   lea (pX, M, 8), pX           /* pX += M */
   lea (pA0, M, 8), pA0         /* pA0 += M */
   neg M                        /* M = -M */
   lea (pY, N, 8), pY           /* pY += N */
   lea (pA0, lda), pA1          /* pA1 = pA0 + lda */
   lea (pA0, lda,2), pA2        /* pA2 = pA0 + 2*lda */
   lea (pA1, lda,2), pA3        /* pA3 = pA0 + 3*lda */
   lea (pA0, lda,4), pA4        /* pA4 = pA0 + 4*lda */
   lea (pA1, lda,4), pA5        /* pA5 = pA0 + 5*lda */
   lea (pA2, lda,4), pA6        /* pA6 = pA0 + 6*lda */
   lea (pA3, lda,4), pA7        /* pA7 = pA0 + 7*lda */
   neg N                        /* N = -N */
/*
 * Start N-loop
 */
   LOOPN:
@ROUT ATL_dgerk_2x8_sse 
      movddup (pY,N,8), y0     /* y0 = {y0, y0} */
      movddup 8(pY,N,8), y1    /* y1 = {y1, y1} */
      movddup 16(pY,N,8), y2   /* y2 = {y2, y2} */
      movddup 24(pY,N,8), y3   /* y3 = {y3, y3} */
      movddup 32(pY,N,8), y4   /* y4 = {y4, y4} */
      movddup 40(pY,N,8), y5   /* y5 = {y5, y5} */
      movddup 48(pY,N,8), y6   /* y6 = {y6, y6} */
      movddup 56(pY,N,8), y7   /* y7 = {y7, y7} */
@ROUT ATL_dgerk_1x8_sse 
      movsd (pY,N,8), y0       /* y0 = {xx, y0} */
      movsd 8(pY,N,8), y1      /* y1 = {xx, y1} */
      movsd 16(pY,N,8), y2     /* y2 = {xx, y2} */
      movsd 24(pY,N,8), y3     /* y3 = {xx, y3} */
      movsd 32(pY,N,8), y4     /* y4 = {xx, y4} */
      movsd 40(pY,N,8), y5     /* y5 = {xx, y5} */
      movsd 48(pY,N,8), y6     /* y6 = {xx, y6} */
      movsd 56(pY,N,8), y7     /* y7 = {xx, y7} */
@ROUT @PEEK
      mov M, II

ALIGN32
      LOOPM:
@ROUT ATL_dgerk_2x8_sse
         movapd (pX,II,8), x0   /* x0 = {x1, x0} */

         movapd x0, x0b         /* x0b= {x1, x0} */
         mulpd y0, x0           /* x0 = {x1*y0, x0*y0} */
         addpd (pA0,II,8), x0   /* x0 = {a10+x1*y0, a00+x0*y0} */
         movapd x0, (pA0,II,8)

         movapd x0b, x0         /* x0 = {x1, x0} */
         mulpd y1, x0b          /* x0b= {x1*y1, x0*y1} */
         addpd (pA1,II,8), x0b  /* x0b= {a11+x1*y1, a01+x0*y1} */
         movapd x0b, (pA1,II,8)

         movapd x0, x0b         /* x0b= {x1, x0} */
         mulpd y2, x0           /* x0 = {x1*y2, x0*y2} */
         addpd (pA2,II,8), x0   /* x0 = {a12+x1*y2, a02+x0*y2} */
         movapd x0, (pA2,II,8)

         movapd x0b, x0         /* x0 = {x1, x0} */
         mulpd y3, x0b          /* x0b= {x1*y3, x0*y1} */
         addpd (pA3,II,8), x0b  /* x0b= {a13+x1*y3, a03+x0*y3} */
         movapd x0b, (pA3,II,8)

         movapd x0, x0b         /* x0b= {x1, x0} */
         mulpd y4, x0           /* x0 = {x1*y4, x0*y4} */
         addpd (pA4,II,8), x0   /* x0 = {a12+x1*y4, a04+x0*y4} */
         movapd x0, (pA4,II,8)

         movapd x0b, x0         /* x0 = {x1, x0} */
         mulpd y5, x0b          /* x0b= {x1*y3, x0*y3} */
         addpd (pA5,II,8), x0b  /* x0b= {a15+x1*y5, a05+x0*y5} */
         movapd x0b, (pA5,II,8)

         movapd x0, x0b         /* x0b= {x1, x0} */
         mulpd y6, x0           /* x0 = {x1*y4, x0*y4} */
         addpd (pA6,II,8), x0   /* x0 = {a12+x1*y4, a04+x0*y4} */
         movapd x0, (pA6,II,8)

         mulpd y7, x0b          /* x0b= {x1*y3, x0*y3} */
         addpd (pA7,II,8), x0b  /* x0b= {a15+x1*y5, a05+x0*y5} */
         movapd x0b, (pA7,II,8)

@ROUT ATL_dgerk_1x8_sse
         movsd (pX,II,8), x0   /* x0 = {xx, x0} */

         movapd x0, x0b        /* x0b= {xx, x0} */
         mulsd y0, x0          /* x0 = {xx, x0*y0} */
         addsd (pA0,II,8), x0  /* x0 = {xx, a00+x0*y0} */
         movsd x0, (pA0,II,8)

         movapd x0b, x0        /* x0= {xx, x0} */
         mulsd y1, x0b         /* x0b= {xx, x0*y1} */
         addsd (pA1,II,8), x0b /* x0b= {xx, a01+x0*y1} */
         movsd x0b, (pA1,II,8)

         movapd x0, x0b        /* x0b= {xx, x0} */
         mulsd y2, x0          /* x0 = {xx, x0*y2} */
         addsd (pA2,II,8), x0  /* x0 = {xx, a02+x0*y2} */
         movsd x0, (pA2,II,8)

         movapd x0b, x0        /* x0= {xx, x0} */
         mulsd y3, x0b         /* x0b= {xx, x1*y3} */
         addsd (pA3,II,8), x0b /* x0b= {xx, a01+x0*y3} */
         movsd x0b, (pA3,II,8)

         movapd x0, x0b        /* x0b= {xx, x0} */
         mulsd y4, x0          /* x0 = {xx, x0*y4} */
         addsd (pA4,II,8), x0  /* x0 = {xx, a04+x0*y4} */
         movsd x0, (pA4,II,8)

         movapd x0b, x0        /* x0= {xx, x0} */
         mulsd y5, x0b         /* x0b= {xx, x1*y5} */
         addsd (pA5,II,8), x0b /* x0b= {xx, a05+x0*y5} */
         movsd x0b, (pA5,II,8)

         movapd x0, x0b        /* x0b= {xx, x0} */
         mulsd y6, x0          /* x0 = {xx, x0*y4} */
         addsd (pA6,II,8), x0  /* x0 = {xx, a04+x0*y4} */
         movsd x0, (pA6,II,8)

         mulsd y7, x0b         /* x0b= {xx, x1*y5} */
         addsd (pA7,II,8), x0b /* x0b= {xx, a05+x0*y5} */
         movsd x0b, (pA7,II,8)

@ROUT ATL_dgerk_1x8_sse
      add       $1, II
@ROUT ATL_dgerk_2x8_sse
      add       $2, II
@ROUT @peek
      jnz LOOPM

      lea (pA0,lda,8), pA0
      lea (pA1,lda,8), pA1
      lea (pA2,lda,8), pA2
      lea (pA3,lda,8), pA3
      lea (pA4,lda,8), pA4
      lea (pA5,lda,8), pA5
      lea (pA6,lda,8), pA6
      lea (pA7,lda,8), pA7
   add $8, N
   jnz LOOPN

/*
 * EPILOGUE: restore registers and return
 */
   movq -8(%rsp), %rbp
   movq -16(%rsp), %rbx
   movq -24(%rsp), %r12
   movq -32(%rsp), %r13
   movq -40(%rsp), %r14
#if 0
   movq %r15, -48(%rsp)
#endif
   ret
@ROUT ATL_dgerk_1x10_sse ATL_dgerk_2x10_sse
@ROUT @push
#include "atlas_asm.h"
#ifndef ATL_GAS_x8664
   #error "This kernel requires x86-64 assembly!"
#endif
/*
 * Register usage
 */
#define M       %r9   /* moved from rdi */
#define N       %r10  /* moved from rsi */
#define II      %rax  /* loaded in loop */
#define pX      %rdx  /* already in */
#define pA2     %rcx  /* 8(%rsp) */
#define pA9     %rbp  /* computed */
#define lda     %rdi  /* 16(%rsp) */
#define mlda    %rsi  /* computed */
#define lda3    %rbx  /* computed */
#define pY      %r8   /* already in */
#define incAn   %r11  /* computed */

#define x0      %xmm0
#define x0b     %xmm1
#define y0      %xmm2
#define y1      %xmm3
#define y2      %xmm4
#define y3      %xmm5
#define y4      %xmm6
#define y5      %xmm7
#define y6      %xmm8
#define y7      %xmm9
#define y8      %xmm10
#define y9      %xmm11

#define movapd movaps
/*
void ATL_UGERK
          %rdi        %rsi              %xmm0           %rdx           
   (ATL_CINT M, ATL_CINT N, const TYPE alpha0, const TYPE *X,
             %rcx            %r8            %r9  8(%rsp)  16(%rsp)
    ATL_CINT incX, const TYPE *Y, ATL_CINT incY, TYPE *A, ATL_CINT lda)
    
*/
.text
.global ATL_asmdecor(ATL_UGERK)
ALIGN64
ATL_asmdecor(ATL_UGERK):

/*
 * Save callee-saved iregs
 */
   movq %rbp, -8(%rsp)
   movq %rbx, -16(%rsp)
#if 0
   movq %r12, -24(%rsp)
   movq %r13, -32(%rsp)
   movq %r14, -40(%rsp)
   movq %r15, -48(%rsp)
#endif
/*
 * Load & compute all integer variables
 */
   movq %rdi, M
   movq %rsi, N
   movq 8(%rsp), pA2
   movslq 16(%rsp), lda
   mov  lda, incAn             /* incAn = lda */
   shl  $3, lda                /* lda *= sizeof */

   lea (pX, M, 8), pX           /* pX += M */
   neg M                        /* M = -M */
   lea (pY, N, 8), pY           /* pY += N */
   lea (pA2, lda,8), pA9        /* pA9 = pA0 + 8*lda */
   add lda, pA9                 /* pA9 = pA0 + 9*lda */
   lea (pA2, lda,2), pA2        /* pA2 = pA0 + 2*lda */
   lea (lda, lda,2), lda3       /* lda3 = 3*lda */
   mov lda, mlda                /* mlda = lda */
   neg mlda                     /* mlda = -lda */
   neg N                        /* N = -N */
/*
 * Compute incAn = (lda-M)+9*lda
 */
   add M, incAn                 /* incAn = lda-M */
   shl $3, incAn                /* incAn = (lda-M)*sizeof */
   lea (incAn, lda,8), incAn   /* incAn = (lda-M)+8*lda */
   add lda, incAn               /* incAn = (lda-M)+9*lda */
/*
 * Start N-loop
 */
   LOOPN:
@ROUT ATL_dgerk_2x10_sse 
      movddup (pY,N,8), y0     /* y0 = {y0, y0} */
      movddup 8(pY,N,8), y1    /* y1 = {y1, y1} */
      movddup 16(pY,N,8), y2   /* y2 = {y2, y2} */
      movddup 24(pY,N,8), y3   /* y3 = {y3, y3} */
      movddup 32(pY,N,8), y4   /* y4 = {y4, y4} */
      movddup 40(pY,N,8), y5   /* y5 = {y5, y5} */
      movddup 48(pY,N,8), y6   /* y6 = {y6, y6} */
      movddup 56(pY,N,8), y7   /* y7 = {y7, y7} */
      movddup 64(pY,N,8), y8   /* y8 = {y8, y8} */
      movddup 72(pY,N,8), y9   /* y9 = {y9, y9} */
@ROUT ATL_dgerk_1x10_sse 
      movsd (pY,N,8), y0       /* y0 = {xx, y0} */
      movsd 8(pY,N,8), y1      /* y1 = {xx, y1} */
      movsd 16(pY,N,8), y2     /* y2 = {xx, y2} */
      movsd 24(pY,N,8), y3     /* y3 = {xx, y3} */
      movsd 32(pY,N,8), y4     /* y4 = {xx, y4} */
      movsd 40(pY,N,8), y5     /* y5 = {xx, y5} */
      movsd 48(pY,N,8), y6     /* y6 = {xx, y6} */
      movsd 56(pY,N,8), y7     /* y7 = {xx, y7} */
      movsd 64(pY,N,8), y8     /* y8 = {xx, y8} */
      movsd 72(pY,N,8), y9     /* y9 = {xx, y9} */
@ROUT @PEEK
      mov M, II

ALIGN32
      LOOPM:
@ROUT ATL_dgerk_2x10_sse
         movapd (pX,II,8), x0   /* x0 = {x1, x0} */

         movapd x0, x0b         /* x0b= {x1, x0} */
         mulpd y0, x0           /* x0 = {x1*y0, x0*y0} */
         addpd (pA2,mlda,2), x0 /* x0 = {a10+x1*y0, a00+x0*y0} */
         movapd x0, (pA2,mlda,2)

         movapd x0b, x0         /* x0 = {x1, x0} */
         mulpd y1, x0b          /* x0b= {x1*y1, x0*y1} */
         addpd (pA2,mlda), x0b  /* x0b= {a11+x1*y1, a01+x0*y1} */
         movapd x0b, (pA2,mlda)

         movapd x0, x0b         /* x0b= {x1, x0} */
         mulpd y2, x0           /* x0 = {x1*y2, x0*y2} */
         addpd (pA2), x0        /* x0 = {a12+x1*y2, a02+x0*y2} */
         movapd x0, (pA2)

         movapd x0b, x0         /* x0 = {x1, x0} */
         mulpd y3, x0b          /* x0b= {x1*y3, x0*y1} */
         addpd (pA2,lda), x0b   /* x0b= {a13+x1*y3, a03+x0*y3} */
         movapd x0b, (pA2,lda)

         movapd x0, x0b         /* x0b= {x1, x0} */
         mulpd y4, x0           /* x0 = {x1*y4, x0*y4} */
         addpd (pA2,lda,2), x0  /* x0 = {a12+x1*y4, a04+x0*y4} */
         movapd x0, (pA2,lda,2)

         movapd x0b, x0          /* x0= {x1, x0} */
         mulpd y5, x0b           /* x0b= {x1*y3, x0*y3} */
         addpd (pA9,mlda,4), x0b /* x0b= {a15+x1*y5, a05+x0*y5} */
         movapd x0b, (pA9,mlda,4)

         movapd x0, x0b         /* x0b= {x1, x0} */
         mulpd y6, x0           /* x0 = {x1*y4, x0*y4} */
         addpd (pA2,lda,4), x0  /* x0 = {a12+x1*y4, a04+x0*y4} */
         movapd x0, (pA2,lda,4)

         movapd x0b, x0          /* x0= {x1, x0} */
         mulpd y7, x0b           /* x0b= {x1*y3, x0*y3} */
         addpd (pA9,mlda,2), x0b /* x0b= {a15+x1*y5, a05+x0*y5} */
         movapd x0b, (pA9,mlda,2)

         movapd x0, x0b         /* x0b= {x1, x0} */
         mulpd y8, x0           /* x0 = {x1*y4, x0*y4} */
         addpd (pA9,mlda), x0   /* x0 = {a12+x1*y4, a04+x0*y4} */
         movapd x0, (pA9,mlda)

         mulpd y9, x0b           /* x0b= {x1*y3, x0*y3} */
         addpd (pA9), x0b        /* x0b= {a15+x1*y5, a05+x0*y5} */
         movapd x0b, (pA9)

@ROUT ATL_dgerk_1x10_sse
         movsd (pX,II,8), x0   /* x0 = {x1, x0} */

         movapd x0, x0b         /* x0b= {x1, x0} */
         mulsd y0, x0           /* x0 = {x1*y0, x0*y0} */
         addsd (pA2,mlda,2), x0 /* x0 = {a10+x1*y0, a00+x0*y0} */
         movsd x0, (pA2,mlda,2)

         movapd x0b, x0         /* x0 = {x1, x0} */
         mulsd y1, x0b          /* x0b= {x1*y1, x0*y1} */
         addsd (pA2,mlda), x0b  /* x0b= {a11+x1*y1, a01+x0*y1} */
         movsd x0b, (pA2,mlda)

         movapd x0, x0b         /* x0b= {x1, x0} */
         mulsd y2, x0           /* x0 = {x1*y2, x0*y2} */
         addsd (pA2), x0        /* x0 = {a12+x1*y2, a02+x0*y2} */
         movsd x0, (pA2)

         movapd x0b, x0         /* x0 = {x1, x0} */
         mulsd y3, x0b          /* x0b= {x1*y3, x0*y1} */
         addsd (pA2,lda), x0b   /* x0b= {a13+x1*y3, a03+x0*y3} */
         movsd x0b, (pA2,lda)

         movapd x0, x0b         /* x0b= {x1, x0} */
         mulsd y4, x0           /* x0 = {x1*y4, x0*y4} */
         addsd (pA2,lda,2), x0  /* x0 = {a12+x1*y4, a04+x0*y4} */
         movsd x0, (pA2,lda,2)

         movapd x0b, x0          /* x0= {x1, x0} */
         mulsd y5, x0b           /* x0b= {x1*y3, x0*y3} */
         addsd (pA9,mlda,4), x0b /* x0b= {a15+x1*y5, a05+x0*y5} */
         movsd x0b, (pA9,mlda,4)

         movapd x0, x0b         /* x0b= {x1, x0} */
         mulsd y6, x0           /* x0 = {x1*y4, x0*y4} */
         addsd (pA2,lda,4), x0  /* x0 = {a12+x1*y4, a04+x0*y4} */
         movsd x0, (pA2,lda,4)

         movapd x0b, x0          /* x0= {x1, x0} */
         mulsd y7, x0b           /* x0b= {x1*y3, x0*y3} */
         addsd (pA9,mlda,2), x0b /* x0b= {a15+x1*y5, a05+x0*y5} */
         movsd x0b, (pA9,mlda,2)

         movapd x0, x0b         /* x0b= {x1, x0} */
         mulsd y8, x0           /* x0 = {x1*y4, x0*y4} */
         addsd (pA9,mlda), x0   /* x0 = {a12+x1*y4, a04+x0*y4} */
         movsd x0, (pA9,mlda)

         mulsd y9, x0b           /* x0b= {x1*y3, x0*y3} */
         addsd (pA9), x0b        /* x0b= {a15+x1*y5, a05+x0*y5} */
         movsd x0b, (pA9)

@ROUT ATL_dgerk_1x10_sse
      add       $8, pA2
      add       $8, pA9
      add       $1, II
@ROUT ATL_dgerk_2x10_sse
      add       $16, pA2
      add       $16, pA9
      add       $2, II
@ROUT @peek
      jnz LOOPM

@ROUT @peek
      add incAn, pA2         /* pA2 += (lda-M)+9*lda */
      add incAn, pA9         /* pA9 += (lda-M)+9*lda */
   add $10, N
   jnz LOOPN

/*
 * EPILOGUE: restore registers and return
 */
   movq -8(%rsp), %rbp
   movq -16(%rsp), %rbx
#if 0
   movq -24(%rsp), %r12
   movq %r13, -32(%rsp), %r13
   movq %r14, -40(%rsp), %r14
   movq %r15, -48(%rsp), %r15
#endif
   ret
@ROUT ATL_dgerk_1x12_sse ATL_dgerk_2x12_sse
@ROUT @push
#include "atlas_asm.h"
#ifndef ATL_GAS_x8664
   #error "This kernel requires x86-64 assembly!"
#endif
/*
 * Register usage
 */
#define M       %r9   /* moved from rdi */
#define N       %r10  /* moved from rsi */
#define II      %rax  /* loaded in loop */
#define pX      %rdx  /* already in */
#define pA2     %rcx  /* 8(%rsp) */
#define pA9     %rbp  /* computed */
#define lda     %rdi  /* 16(%rsp) */
#define mlda    %rsi  /* computed */
#define lda3    %rbx  /* computed */
#define pY      %r8   /* already in */
#define incAn   %r11  /* computed */

#define x0      %xmm0
#define x0b     %xmm1
#define y0      %xmm2
#define y1      %xmm3
#define y2      %xmm4
#define y3      %xmm5
#define y4      %xmm6
#define y5      %xmm7
#define y6      %xmm8
#define y7      %xmm9
#define y8      %xmm10
#define y9      %xmm11
#define y10     %xmm12
#define y11     %xmm13

#define movapd movaps
/*
void ATL_UGERK
          %rdi        %rsi              %xmm0           %rdx           
   (ATL_CINT M, ATL_CINT N, const TYPE alpha0, const TYPE *X,
             %rcx            %r8            %r9  8(%rsp)  16(%rsp)
    ATL_CINT incX, const TYPE *Y, ATL_CINT incY, TYPE *A, ATL_CINT lda)
    
*/
.text
.global ATL_asmdecor(ATL_UGERK)
ALIGN64
ATL_asmdecor(ATL_UGERK):

/*
 * Save callee-saved iregs
 */
   movq %rbp, -8(%rsp)
   movq %rbx, -16(%rsp)
#if 0
   movq %r12, -24(%rsp)
   movq %r13, -32(%rsp)
   movq %r14, -40(%rsp)
   movq %r15, -48(%rsp)
#endif
/*
 * Load & compute all integer variables
 */
   movq %rdi, M
   movq %rsi, N
   movq 8(%rsp), pA2
   movslq 16(%rsp), lda
   mov  lda, incAn             /* incAn = lda */
   shl  $3, lda                /* lda *= sizeof */

   lea (pX, M, 8), pX           /* pX += M */
   neg M                        /* M = -M */
   lea (pY, N, 8), pY           /* pY += N */
   lea (pA2, lda,8), pA9        /* pA9 = pA0 + 8*lda */
   add lda, pA9                 /* pA9 = pA0 + 9*lda */
   lea (pA2, lda,2), pA2        /* pA2 = pA0 + 2*lda */
   lea (lda, lda,2), lda3       /* lda3 = 3*lda */
   mov lda, mlda                /* mlda = lda */
   neg mlda                     /* mlda = -lda */
   neg N                        /* N = -N */
/*
 * Compute incAn = (lda-M)+11*lda
 */
   add M, incAn                 /* incAn = lda-M */
   shl $3, incAn                /* incAn = (lda-M)*sizeof */
   lea (incAn, lda3,4), incAn   /* incAn = (lda-M)+12*lda */
   add mlda, incAn              /* incAn = (lda-M)+11*lda */
/*
 * Start N-loop
 */
   LOOPN:
@ROUT ATL_dgerk_2x12_sse 
      movddup (pY,N,8), y0     /* y0 = {y0, y0} */
      movddup 8(pY,N,8), y1    /* y1 = {y1, y1} */
      movddup 16(pY,N,8), y2   /* y2 = {y2, y2} */
      movddup 24(pY,N,8), y3   /* y3 = {y3, y3} */
      movddup 32(pY,N,8), y4   /* y4 = {y4, y4} */
      movddup 40(pY,N,8), y5   /* y5 = {y5, y5} */
      movddup 48(pY,N,8), y6   /* y6 = {y6, y6} */
      movddup 56(pY,N,8), y7   /* y7 = {y7, y7} */
      movddup 64(pY,N,8), y8   /* y8 = {y8, y8} */
      movddup 72(pY,N,8), y9   /* y9 = {y9, y9} */
      movddup 80(pY,N,8), y10  /* y10= {y10, y10} */
      movddup 88(pY,N,8), y11  /* y11= {y11, y11} */
@ROUT ATL_dgerk_1x12_sse 
      movsd (pY,N,8), y0       /* y0 = {xx, y0} */
      movsd 8(pY,N,8), y1      /* y1 = {xx, y1} */
      movsd 16(pY,N,8), y2     /* y2 = {xx, y2} */
      movsd 24(pY,N,8), y3     /* y3 = {xx, y3} */
      movsd 32(pY,N,8), y4     /* y4 = {xx, y4} */
      movsd 40(pY,N,8), y5     /* y5 = {xx, y5} */
      movsd 48(pY,N,8), y6     /* y6 = {xx, y6} */
      movsd 56(pY,N,8), y7     /* y7 = {xx, y7} */
      movsd 64(pY,N,8), y8     /* y8 = {xx, y8} */
      movsd 72(pY,N,8), y9     /* y9 = {xx, y9} */
      movsd 80(pY,N,8), y10    /* y10= {xx, y10} */
      movsd 88(pY,N,8), y11    /* y11= {xx, y11} */
@ROUT @PEEK
      mov M, II

ALIGN32
      LOOPM:
@ROUT ATL_dgerk_2x12_sse
         movapd (pX,II,8), x0   /* x0 = {x1, x0} */

         movapd x0, x0b         /* x0b= {x1, x0} */
         mulpd y0, x0           /* x0 = {x1*y0, x0*y0} */
         addpd (pA2,mlda,2), x0 /* x0 = {a10+x1*y0, a00+x0*y0} */
         movapd x0, (pA2,mlda,2)

         movapd x0b, x0         /* x0 = {x1, x0} */
         mulpd y1, x0b          /* x0b= {x1*y1, x0*y1} */
         addpd (pA2,mlda), x0b  /* x0b= {a11+x1*y1, a01+x0*y1} */
         movapd x0b, (pA2,mlda)

         movapd x0, x0b         /* x0b= {x1, x0} */
         mulpd y2, x0           /* x0 = {x1*y2, x0*y2} */
         addpd (pA2), x0        /* x0 = {a12+x1*y2, a02+x0*y2} */
         movapd x0, (pA2)

         movapd x0b, x0         /* x0 = {x1, x0} */
         mulpd y3, x0b          /* x0b= {x1*y3, x0*y1} */
         addpd (pA2,lda), x0b   /* x0b= {a13+x1*y3, a03+x0*y3} */
         movapd x0b, (pA2,lda)

         movapd x0, x0b         /* x0b= {x1, x0} */
         mulpd y4, x0           /* x0 = {x1*y4, x0*y4} */
         addpd (pA2,lda,2), x0  /* x0 = {a12+x1*y4, a04+x0*y4} */
         movapd x0, (pA2,lda,2)

         movapd x0b, x0          /* x0= {x1, x0} */
         mulpd y5, x0b           /* x0b= {x1*y3, x0*y3} */
         addpd (pA9,mlda,4), x0b /* x0b= {a15+x1*y5, a05+x0*y5} */
         movapd x0b, (pA9,mlda,4)

         movapd x0, x0b         /* x0b= {x1, x0} */
         mulpd y6, x0           /* x0 = {x1*y4, x0*y4} */
         addpd (pA2,lda,4), x0  /* x0 = {a12+x1*y4, a04+x0*y4} */
         movapd x0, (pA2,lda,4)

         movapd x0b, x0          /* x0= {x1, x0} */
         mulpd y7, x0b           /* x0b= {x1*y3, x0*y3} */
         addpd (pA9,mlda,2), x0b /* x0b= {a15+x1*y5, a05+x0*y5} */
         movapd x0b, (pA9,mlda,2)

         movapd x0, x0b         /* x0b= {x1, x0} */
         mulpd y8, x0           /* x0 = {x1*y4, x0*y4} */
         addpd (pA9,mlda), x0   /* x0 = {a12+x1*y4, a04+x0*y4} */
         movapd x0, (pA9,mlda)

         movapd x0b, x0          /* x0= {x1, x0} */
         mulpd y9, x0b           /* x0b= {x1*y3, x0*y3} */
         addpd (pA9), x0b        /* x0b= {a15+x1*y5, a05+x0*y5} */
         movapd x0b, (pA9)

         movapd x0, x0b         /* x0b= {x1, x0} */
         mulpd y10, x0          /* x0 = {x1*y4, x0*y4} */
         addpd (pA9,lda), x0    /* x0 = {a12+x1*y4, a04+x0*y4} */
         movapd x0, (pA9,lda)

         mulpd y11, x0b         /* x0b= {x1*y3, x0*y3} */
         addpd (pA9,lda,2), x0b /* x0b= {a15+x1*y5, a05+x0*y5} */
         movapd x0b, (pA9,lda,2)

@ROUT ATL_dgerk_1x12_sse
         movsd (pX,II,8), x0   /* x0 = {x1, x0} */

         movapd x0, x0b         /* x0b= {x1, x0} */
         mulsd y0, x0           /* x0 = {x1*y0, x0*y0} */
         addsd (pA2,mlda,2), x0 /* x0 = {a10+x1*y0, a00+x0*y0} */
         movsd x0, (pA2,mlda,2)

         movapd x0b, x0         /* x0 = {x1, x0} */
         mulsd y1, x0b          /* x0b= {x1*y1, x0*y1} */
         addsd (pA2,mlda), x0b  /* x0b= {a11+x1*y1, a01+x0*y1} */
         movsd x0b, (pA2,mlda)

         movapd x0, x0b         /* x0b= {x1, x0} */
         mulsd y2, x0           /* x0 = {x1*y2, x0*y2} */
         addsd (pA2), x0        /* x0 = {a12+x1*y2, a02+x0*y2} */
         movsd x0, (pA2)

         movapd x0b, x0         /* x0 = {x1, x0} */
         mulsd y3, x0b          /* x0b= {x1*y3, x0*y1} */
         addsd (pA2,lda), x0b   /* x0b= {a13+x1*y3, a03+x0*y3} */
         movsd x0b, (pA2,lda)

         movapd x0, x0b         /* x0b= {x1, x0} */
         mulsd y4, x0           /* x0 = {x1*y4, x0*y4} */
         addsd (pA2,lda,2), x0  /* x0 = {a12+x1*y4, a04+x0*y4} */
         movsd x0, (pA2,lda,2)

         movapd x0b, x0          /* x0= {x1, x0} */
         mulsd y5, x0b           /* x0b= {x1*y3, x0*y3} */
         addsd (pA9,mlda,4), x0b /* x0b= {a15+x1*y5, a05+x0*y5} */
         movsd x0b, (pA9,mlda,4)

         movapd x0, x0b         /* x0b= {x1, x0} */
         mulsd y6, x0           /* x0 = {x1*y4, x0*y4} */
         addsd (pA2,lda,4), x0  /* x0 = {a12+x1*y4, a04+x0*y4} */
         movsd x0, (pA2,lda,4)

         movapd x0b, x0          /* x0= {x1, x0} */
         mulsd y7, x0b           /* x0b= {x1*y3, x0*y3} */
         addsd (pA9,mlda,2), x0b /* x0b= {a15+x1*y5, a05+x0*y5} */
         movsd x0b, (pA9,mlda,2)

         movapd x0, x0b         /* x0b= {x1, x0} */
         mulsd y8, x0           /* x0 = {x1*y4, x0*y4} */
         addsd (pA9,mlda), x0   /* x0 = {a12+x1*y4, a04+x0*y4} */
         movsd x0, (pA9,mlda)

         movapd x0b, x0          /* x0= {x1, x0} */
         mulsd y9, x0b           /* x0b= {x1*y3, x0*y3} */
         addsd (pA9), x0b        /* x0b= {a15+x1*y5, a05+x0*y5} */
         movsd x0b, (pA9)

         movapd x0, x0b         /* x0b= {x1, x0} */
         mulsd y10, x0          /* x0 = {x1*y4, x0*y4} */
         addsd (pA9,lda), x0    /* x0 = {a12+x1*y4, a04+x0*y4} */
         movsd x0, (pA9,lda)

         mulsd y11, x0b         /* x0b= {x1*y3, x0*y3} */
         addsd (pA9,lda,2), x0b /* x0b= {a15+x1*y5, a05+x0*y5} */
         movsd x0b, (pA9,lda,2)

@ROUT ATL_dgerk_1x12_sse
      add       $8, pA2
      add       $8, pA9
      add       $1, II
@ROUT ATL_dgerk_2x12_sse
      add       $16, pA2
      add       $16, pA9
      add       $2, II
@ROUT @peek
      jnz LOOPM

@ROUT @peek
      add incAn, pA2         /* pA2 += (lda-M)+11*lda */
      add incAn, pA9         /* pA9 += (lda-M)+11*lda */
   add $12, N
   jnz LOOPN

/*
 * EPILOGUE: restore registers and return
 */
   movq -8(%rsp), %rbp
   movq -16(%rsp), %rbx
#if 0
   movq -24(%rsp), %r12
   movq %r13, -32(%rsp), %r13
   movq %r14, -40(%rsp), %r14
   movq %r15, -48(%rsp), %r15
#endif
   ret
@ROUT ATL_dgerk_1x14_sse ATL_dgerk_2x14_sse
@ROUT @push
#include "atlas_asm.h"
#ifndef ATL_GAS_x8664
   #error "This kernel requires x86-64 assembly!"
#endif
/*
 * Register usage
 */
#define M       %r9   /* moved from rdi */
#define N       %r10  /* moved from rsi */
#define II      %rax  /* loaded in loop */
#define pX      %rdx  /* already in */
#define pA2     %rcx  /* 8(%rsp) */
#define pA9     %rbp  /* computed */
#define lda     %rdi  /* 16(%rsp) */
#define mlda    %rsi  /* computed */
#define lda3    %rbx  /* computed */
#define pY      %r8   /* already in */
#define incAn   %r11  /* computed */

#define x0      %xmm0
#define x0b     %xmm1
#define y0      %xmm2
#define y1      %xmm3
#define y2      %xmm4
#define y3      %xmm5
#define y4      %xmm6
#define y5      %xmm7
#define y6      %xmm8
#define y7      %xmm9
#define y8      %xmm10
#define y9      %xmm11
#define y10     %xmm12
#define y11     %xmm13
#define y12     %xmm14
#define y13     %xmm15

#define movapd movaps
/*
void ATL_UGERK
          %rdi        %rsi              %xmm0           %rdx           
   (ATL_CINT M, ATL_CINT N, const TYPE alpha0, const TYPE *X,
             %rcx            %r8            %r9  8(%rsp)  16(%rsp)
    ATL_CINT incX, const TYPE *Y, ATL_CINT incY, TYPE *A, ATL_CINT lda)
    
*/
.text
.global ATL_asmdecor(ATL_UGERK)
ALIGN64
ATL_asmdecor(ATL_UGERK):

/*
 * Save callee-saved iregs
 */
   movq %rbp, -8(%rsp)
   movq %rbx, -16(%rsp)
#if 0
   movq %r12, -24(%rsp)
   movq %r13, -32(%rsp)
   movq %r14, -40(%rsp)
   movq %r15, -48(%rsp)
#endif
/*
 * Load & compute all integer variables
 */
   movq %rdi, M
   movq %rsi, N
   movq 8(%rsp), pA2
   movslq 16(%rsp), lda
   mov  lda, incAn             /* incAn = lda */
   shl  $3, lda                /* lda *= sizeof */

   lea (pX, M, 8), pX           /* pX += M */
   neg M                        /* M = -M */
   lea (pY, N, 8), pY           /* pY += N */
   lea (pA2, lda,8), pA9        /* pA9 = pA0 + 8*lda */
   add lda, pA9                 /* pA9 = pA0 + 9*lda */
   lea (pA2, lda,2), pA2        /* pA2 = pA0 + 2*lda */
   lea (lda, lda,2), lda3       /* lda3 = 3*lda */
   mov lda, mlda                /* mlda = lda */
   neg mlda                     /* mlda = -lda */
   neg N                        /* N = -N */
/*
 * Compute incAn = (lda-M)+13*lda
 */
   add M, incAn                 /* incAn = lda-M */
   shl $3, incAn                /* incAn = (lda-M)*sizeof */
   lea (incAn, lda3,4), incAn   /* incAn = (lda-M)+12*lda */
   add lda, incAn               /* incAn = (lda-M)+13*lda */
/*
 * Start N-loop
 */
   LOOPN:
@ROUT ATL_dgerk_2x14_sse 
      movddup (pY,N,8), y0     /* y0 = {y0, y0} */
      movddup 8(pY,N,8), y1    /* y1 = {y1, y1} */
      movddup 16(pY,N,8), y2   /* y2 = {y2, y2} */
      movddup 24(pY,N,8), y3   /* y3 = {y3, y3} */
      movddup 32(pY,N,8), y4   /* y4 = {y4, y4} */
      movddup 40(pY,N,8), y5   /* y5 = {y5, y5} */
      movddup 48(pY,N,8), y6   /* y6 = {y6, y6} */
      movddup 56(pY,N,8), y7   /* y7 = {y7, y7} */
      movddup 64(pY,N,8), y8   /* y8 = {y8, y8} */
      movddup 72(pY,N,8), y9   /* y9 = {y9, y9} */
      movddup 80(pY,N,8), y10  /* y10= {y10, y10} */
      movddup 88(pY,N,8), y11  /* y11= {y11, y11} */
      movddup 96(pY,N,8), y12  /* y12= {y12, y12} */
      movddup 104(pY,N,8), y13 /* y13= {y13, y13} */
@ROUT ATL_dgerk_1x14_sse 
      movsd (pY,N,8), y0       /* y0 = {xx, y0} */
      movsd 8(pY,N,8), y1      /* y1 = {xx, y1} */
      movsd 16(pY,N,8), y2     /* y2 = {xx, y2} */
      movsd 24(pY,N,8), y3     /* y3 = {xx, y3} */
      movsd 32(pY,N,8), y4     /* y4 = {xx, y4} */
      movsd 40(pY,N,8), y5     /* y5 = {xx, y5} */
      movsd 48(pY,N,8), y6     /* y6 = {xx, y6} */
      movsd 56(pY,N,8), y7     /* y7 = {xx, y7} */
      movsd 64(pY,N,8), y8     /* y8 = {xx, y8} */
      movsd 72(pY,N,8), y9     /* y9 = {xx, y9} */
      movsd 80(pY,N,8), y10    /* y10= {xx, y10} */
      movsd 88(pY,N,8), y11    /* y11= {xx, y11} */
      movsd 96(pY,N,8), y12    /* y12= {xx, y12} */
      movsd 104(pY,N,8), y13   /* y13= {xx, y13} */
@ROUT @PEEK
      mov M, II

ALIGN32
      LOOPM:
@ROUT ATL_dgerk_2x14_sse
         movapd (pX,II,8), x0   /* x0 = {x1, x0} */

         movapd x0, x0b         /* x0b= {x1, x0} */
         mulpd y0, x0           /* x0 = {x1*y0, x0*y0} */
         addpd (pA2,mlda,2), x0 /* x0 = {a10+x1*y0, a00+x0*y0} */
         movapd x0, (pA2,mlda,2)

         movapd x0b, x0         /* x0 = {x1, x0} */
         mulpd y1, x0b          /* x0b= {x1*y1, x0*y1} */
         addpd (pA2,mlda), x0b  /* x0b= {a11+x1*y1, a01+x0*y1} */
         movapd x0b, (pA2,mlda)

         movapd x0, x0b         /* x0b= {x1, x0} */
         mulpd y2, x0           /* x0 = {x1*y2, x0*y2} */
         addpd (pA2), x0        /* x0 = {a12+x1*y2, a02+x0*y2} */
         movapd x0, (pA2)

         movapd x0b, x0         /* x0 = {x1, x0} */
         mulpd y3, x0b          /* x0b= {x1*y3, x0*y1} */
         addpd (pA2,lda), x0b   /* x0b= {a13+x1*y3, a03+x0*y3} */
         movapd x0b, (pA2,lda)

         movapd x0, x0b         /* x0b= {x1, x0} */
         mulpd y4, x0           /* x0 = {x1*y4, x0*y4} */
         addpd (pA2,lda,2), x0  /* x0 = {a12+x1*y4, a04+x0*y4} */
         movapd x0, (pA2,lda,2)

         movapd x0b, x0          /* x0= {x1, x0} */
         mulpd y5, x0b           /* x0b= {x1*y3, x0*y3} */
         addpd (pA9,mlda,4), x0b /* x0b= {a15+x1*y5, a05+x0*y5} */
         movapd x0b, (pA9,mlda,4)

         movapd x0, x0b         /* x0b= {x1, x0} */
         mulpd y6, x0           /* x0 = {x1*y4, x0*y4} */
         addpd (pA2,lda,4), x0  /* x0 = {a12+x1*y4, a04+x0*y4} */
         movapd x0, (pA2,lda,4)

         movapd x0b, x0          /* x0= {x1, x0} */
         mulpd y7, x0b           /* x0b= {x1*y3, x0*y3} */
         addpd (pA9,mlda,2), x0b /* x0b= {a15+x1*y5, a05+x0*y5} */
         movapd x0b, (pA9,mlda,2)

         movapd x0, x0b         /* x0b= {x1, x0} */
         mulpd y8, x0           /* x0 = {x1*y4, x0*y4} */
         addpd (pA9,mlda), x0   /* x0 = {a12+x1*y4, a04+x0*y4} */
         movapd x0, (pA9,mlda)

         movapd x0b, x0          /* x0= {x1, x0} */
         mulpd y9, x0b           /* x0b= {x1*y3, x0*y3} */
         addpd (pA9), x0b        /* x0b= {a15+x1*y5, a05+x0*y5} */
         movapd x0b, (pA9)

         movapd x0, x0b         /* x0b= {x1, x0} */
         mulpd y10, x0          /* x0 = {x1*y4, x0*y4} */
         addpd (pA9,lda), x0    /* x0 = {a12+x1*y4, a04+x0*y4} */
         movapd x0, (pA9,lda)

         movapd x0b, x0         /* x0= {x1, x0} */
         mulpd y11, x0b         /* x0b= {x1*y3, x0*y3} */
         addpd (pA9,lda,2), x0b /* x0b= {a15+x1*y5, a05+x0*y5} */
         movapd x0b, (pA9,lda,2)

         movapd x0, x0b         /* x0b= {x1, x0} */
         mulpd y12, x0          /* x0 = {x1*y4, x0*y4} */
         addpd (pA9,lda3), x0  /* x0 = {a12+x1*y4, a04+x0*y4} */
         movapd x0, (pA9,lda3)

         mulpd y13, x0b           /* x0b= {x1*y3, x0*y3} */
         addpd (pA9,lda,4), x0b   /* x0b= {a15+x1*y5, a05+x0*y5} */
         movapd x0b, (pA9,lda,4)
@ROUT ATL_dgerk_1x14_sse
         movsd (pX,II,8), x0   /* x0 = {x1, x0} */

         movapd x0, x0b         /* x0b= {x1, x0} */
         mulsd y0, x0           /* x0 = {x1*y0, x0*y0} */
         addsd (pA2,mlda,2), x0 /* x0 = {a10+x1*y0, a00+x0*y0} */
         movsd x0, (pA2,mlda,2)

         movapd x0b, x0         /* x0 = {x1, x0} */
         mulsd y1, x0b          /* x0b= {x1*y1, x0*y1} */
         addsd (pA2,mlda), x0b  /* x0b= {a11+x1*y1, a01+x0*y1} */
         movsd x0b, (pA2,mlda)

         movapd x0, x0b         /* x0b= {x1, x0} */
         mulsd y2, x0           /* x0 = {x1*y2, x0*y2} */
         addsd (pA2), x0        /* x0 = {a12+x1*y2, a02+x0*y2} */
         movsd x0, (pA2)

         movapd x0b, x0         /* x0 = {x1, x0} */
         mulsd y3, x0b          /* x0b= {x1*y3, x0*y1} */
         addsd (pA2,lda), x0b   /* x0b= {a13+x1*y3, a03+x0*y3} */
         movsd x0b, (pA2,lda)

         movapd x0, x0b         /* x0b= {x1, x0} */
         mulsd y4, x0           /* x0 = {x1*y4, x0*y4} */
         addsd (pA2,lda,2), x0  /* x0 = {a12+x1*y4, a04+x0*y4} */
         movsd x0, (pA2,lda,2)

         movapd x0b, x0          /* x0= {x1, x0} */
         mulsd y5, x0b           /* x0b= {x1*y3, x0*y3} */
         addsd (pA9,mlda,4), x0b /* x0b= {a15+x1*y5, a05+x0*y5} */
         movsd x0b, (pA9,mlda,4)

         movapd x0, x0b         /* x0b= {x1, x0} */
         mulsd y6, x0           /* x0 = {x1*y4, x0*y4} */
         addsd (pA2,lda,4), x0  /* x0 = {a12+x1*y4, a04+x0*y4} */
         movsd x0, (pA2,lda,4)

         movapd x0b, x0          /* x0= {x1, x0} */
         mulsd y7, x0b           /* x0b= {x1*y3, x0*y3} */
         addsd (pA9,mlda,2), x0b /* x0b= {a15+x1*y5, a05+x0*y5} */
         movsd x0b, (pA9,mlda,2)

         movapd x0, x0b         /* x0b= {x1, x0} */
         mulsd y8, x0           /* x0 = {x1*y4, x0*y4} */
         addsd (pA9,mlda), x0   /* x0 = {a12+x1*y4, a04+x0*y4} */
         movsd x0, (pA9,mlda)

         movapd x0b, x0          /* x0= {x1, x0} */
         mulsd y9, x0b           /* x0b= {x1*y3, x0*y3} */
         addsd (pA9), x0b        /* x0b= {a15+x1*y5, a05+x0*y5} */
         movsd x0b, (pA9)

         movapd x0, x0b         /* x0b= {x1, x0} */
         mulsd y10, x0          /* x0 = {x1*y4, x0*y4} */
         addsd (pA9,lda), x0    /* x0 = {a12+x1*y4, a04+x0*y4} */
         movsd x0, (pA9,lda)

         movapd x0b, x0         /* x0= {x1, x0} */
         mulsd y11, x0b         /* x0b= {x1*y3, x0*y3} */
         addsd (pA9,lda,2), x0b /* x0b= {a15+x1*y5, a05+x0*y5} */
         movsd x0b, (pA9,lda,2)

         movapd x0, x0b         /* x0b= {x1, x0} */
         mulsd y12, x0          /* x0 = {x1*y4, x0*y4} */
         addsd (pA9,lda3), x0  /* x0 = {a12+x1*y4, a04+x0*y4} */
         movsd x0, (pA9,lda3)

         mulsd y13, x0b           /* x0b= {x1*y3, x0*y3} */
         addsd (pA9,lda,4), x0b   /* x0b= {a15+x1*y5, a05+x0*y5} */
         movsd x0b, (pA9,lda,4)

@ROUT ATL_dgerk_1x14_sse
      add       $8, pA2
      add       $8, pA9
      add       $1, II
@ROUT ATL_dgerk_2x14_sse
      add       $16, pA2
      add       $16, pA9
      add       $2, II
@ROUT @peek
      jnz LOOPM

      add incAn, pA2         /* pA2 += (lda-M)+13*lda */
      add incAn, pA9         /* pA9 += (lda-M)+13*lda */
   add $14, N
   jnz LOOPN

/*
 * EPILOGUE: restore registers and return
 */
   movq -8(%rsp), %rbp
   movq -16(%rsp), %rbx
#if 0
   movq -24(%rsp), %r12
   movq %r13, -32(%rsp), %r13
   movq %r14, -40(%rsp), %r14
   movq %r15, -48(%rsp), %r15
#endif
   ret
@ROUT ATL_dgerk_CLx1_touch ATL_dgerk_CLx2_touch ATL_dgerk_CLx8_touch @\
      ATL_dgerk_CLx4_touch ATL_dgerk_CLx6_touch 
@ROUT @push
#include "atlas_asm.h"
#ifndef ATL_GAS_x8664
   #error "This kernel requires x86-64 assembly!"
#endif
/*
 * Register usage
 */
@ROUT ATL_dgerk_CLx1_touch
#define M       %rdi  /* already in */
#define N       %rsi  /* already in */
#define II      %rax  /* loaded in loop */
#define pX      %rdx  /* already in */
#define pA0     %rcx  /* 8(%rsp) */
#define lda     %rbx  /* 16(%rsp) */
#define pY      %rbp  /* moved from r9 */
@ROUT ATL_dgerk_CLx2_touch
#define M       %rdi  /* already in */
#define N       %rsi  /* already in */
#define II      %rax  /* loaded in loop */
#define pX      %rdx  /* already in */
#define pA0     %rcx  /* 8(%rsp) */
#define pA1     %rbx  /* 16(%rsp) */
#define lda     %r8   /* 16(%rsp) */
#define pY      %rbp  /* moved from r9 */
@ROUT ATL_dgerk_CLx8_touch ATL_dgerk_CLx4_touch ATL_dgerk_CLx6_touch
#define M       %rdi  /* already in */
#define N       %rsi  /* already in */
#define II      %rax  /* loaded in loop */
#define pX      %rdx  /* already in */
#define pA0     %rcx  /* 8(%rsp) */
#define pA1     %rbx  /* 16(%rsp) */
#define lda     %r12   /* 16(%rsp) */
#define pY      %rbp  /* moved from r9 */
#define pA2     %r9
#define pA3     %r10
#define pA4     %r11
#define pA5     %r14
#define pA6     %r15
#define pA7     %r8
@ROUT @peek

/*
void ATL_UGERK
          %rdi        %rsi              %xmm0           %rdx           
   (ATL_CINT M, ATL_CINT N, const TYPE alpha0, const TYPE *X,
             %rcx            %r8            %r9  8(%rsp)  16(%rsp)
    ATL_CINT incX, const TYPE *Y, ATL_CINT incY, TYPE *A, ATL_CINT lda)
    
*/
.text
.global ATL_asmdecor(ATL_UGERK)
ALIGN64
ATL_asmdecor(ATL_UGERK):

/*
 * Save callee-saved iregs
 */
   movq %rbp, -8(%rsp)
   movq %rbx, -16(%rsp)
@ROUT ATL_dgerk_CLx4_touch ATL_dgerk_CLx6_touch ATL_dgerk_CLx8_touch
   movq %r12, -24(%rsp)
//   movq %r13, -32(%rsp)
   movq %r14, -40(%rsp)
   movq %r15, -48(%rsp)
@ROUT @peek
/*
 * Load & compute all integer variables
 */
   movq 8(%rsp), pA0
   movq %r8, pY
   movslq 16(%rsp), lda
   shl  $3, lda                /* lda *= sizeof */

   lea (pX, M, 8), pX           /* pX += M */
   lea (pA0, M, 8), pA0         /* pA0 += M */
@ROUT ATL_dgerk_CLx2_touch
   lea (pA0,lda), pA1
   add lda, lda
@ROUT ATL_dgerk_CLx4_touch ATL_dgerk_CLx6_touch ATL_dgerk_CLx8_touch
   lea (pA0, lda), pA1
   lea (pA0, lda,2), pA2
   lea (pA1, lda,2), pA3
@ROUT ATL_dgerk_CLx6_touch ATL_dgerk_CLx8_touch
   lea (pA0, lda,4), pA4
   lea (pA1, lda,4), pA5
@ROUT ATL_dgerk_CLx8_touch
   lea (pA2, lda,4), pA6
   lea (pA3, lda,4), pA7
@ROUT @peek
   neg M                        /* M = -M */
   lea (pY, N, 8), pY           /* pY += N */
   neg N                        /* N = -N */
/*
 * Start N-loop
 */
   LOOPN:
      mov M, II

ALIGN32
      LOOPM:
         movq (pA0,II,8), pY
         movq pY, 8(pA0,II,8)
@ROUT ATL_dgerk_CLx2_touch ATL_dgerk_CLx8_touch ATL_dgerk_CLx6_touch @\
      ATL_dgerk_CLx4_touch
         movq (pA1,II,8), pY
         movq pY, 8(pA1,II,8)
@ROUT ATL_dgerk_CLx8_touch ATL_dgerk_CLx6_touch ATL_dgerk_CLx4_touch
         movq (pA2,II,8), pY
         movq pY, 8(pA2,II,8)
         movq (pA3,II,8), pY
         movq pY, 8(pA3,II,8)
@ROUT ATL_dgerk_CLx8_touch ATL_dgerk_CLx6_touch
         movq (pA4,II,8), pY
         movq pY, 8(pA4,II,8)
         movq (pA5,II,8), pY
         movq pY, 8(pA5,II,8)
@ROUT ATL_dgerk_CLx8_touch
         movq (pA6,II,8), pY
         movq pY, 8(pA6,II,8)
         movq (pA7,II,8), pY
         movq pY, 8(pA7,II,8)
@ROUT @peek
      add       $8, II
      jnz LOOPM
@ROUT ATL_dgerk_CLx1_touch
      add lda, pA0
   add $1, N
@ROUT ATL_dgerk_CLx2_touch
      add lda, pA0
      add lda, pA1
   add $2, N
@ROUT ATL_dgerk_CLx8_touch
      lea (pA0, lda,8), pA0
      lea (pA1, lda,8), pA1
      lea (pA2, lda,8), pA2
      lea (pA3, lda,8), pA3
      lea (pA4, lda,8), pA4
      lea (pA5, lda,8), pA5
      lea (pA6, lda,8), pA6
      lea (pA7, lda,8), pA7
   add $8, N
@ROUT ATL_dgerk_CLx6_touch
      lea (pA2, lda,4), pA0
      lea (pA3, lda,4), pA1
      lea (pA4, lda,4), pA2
      lea (pA5, lda,4), pA3
      lea (pA0, lda,4), pA4
      lea (pA1, lda,4), pA5
   add $6, N
@ROUT ATL_dgerk_CLx4_touch
      lea (pA0, lda,4), pA0
      lea (pA1, lda,4), pA1
      lea (pA2, lda,4), pA2
      lea (pA3, lda,4), pA3
   add $4, N
@ROUT @peek
   jnz LOOPN

/*
 * EPILOGUE: restore registers and return
 */
   movq -8(%rsp), %rbp
   movq -16(%rsp), %rbx
@ROUT ATL_dgerk_CLx8_touch ATL_dgerk_CLx6_touch ATL_dgerk_CLx4_touch
   movq -24(%rsp), %r12
//   movq -32(%rsp), %r13
   movq -40(%rsp), %r14
   movq -48(%rsp), %r15
@ROUT @peek
   ret
@ROUT ATL_gerCL_urx1 ATL_gerCL_urx2 ATL_gerCL_urx8 ATL_gerCL_urx4 ATL_gerCL_urx6
   @addkeys GER=T
@ROUT ATL_mvtCL_urx1 ATL_mvtCL_urx2 ATL_mvtCL_urx8 ATL_mvtCL_urx4 ATL_mvtCL_urx6
   @addkeys GER=F
@ROUT ATL_gerCL_urx1 ATL_gerCL_urx2 ATL_gerCL_urx8 ATL_gerCL_urx4 @\
      ATL_gerCL_urx6 @\
      ATL_mvtCL_urx1 ATL_mvtCL_urx2 ATL_mvtCL_urx8 ATL_mvtCL_urx4 ATL_mvtCL_urx6
@ROUT @push
#include "atlas_asm.h"
#ifndef ATL_GAS_x8664
   #error "This kernel requires x86-64 assembly!"
#endif
/*
 * Register usage
 */
@ROUT ATL_gerCL_urx1 ATL_mvtCL_urx1
#define M       %rdi  /* already in */
#define N       %rsi  /* already in */
#define II      %rax  /* loaded in loop */
#define pX      %rdx  /* already in */
#define pA0     %rcx  /* 8(%rsp) */
#define lda     %rbx  /* 16(%rsp) */
#define pY      %rbp  /* moved from r9 */
@ROUT ATL_gerCL_urx2 ATL_mvtCL_urx2
#define M       %rdi  /* already in */
#define N       %rsi  /* already in */
#define II      %rax  /* loaded in loop */
#define pX      %rdx  /* already in */
#define pA0     %rcx  /* 8(%rsp) */
#define pA1     %rbx  /* 16(%rsp) */
#define lda     %r8   /* 16(%rsp) */
#define pY      %rbp  /* moved from r9 */
@ROUT ATL_gerCL_urx8 ATL_gerCL_urx4 ATL_gerCL_urx6 @\
      ATL_mvtCL_urx8 ATL_mvtCL_urx4 ATL_mvtCL_urx6 
#define M       %rdi  /* already in */
#define N       %rsi  /* already in */
#define II      %rax  /* loaded in loop */
#define pX      %rdx  /* already in */
#define pA0     %rcx  /* 8(%rsp) */
#define pA1     %rbx  /* 16(%rsp) */
#define lda     %r12   /* 16(%rsp) */
#define pY      %rbp  /* moved from r9 */
#define pA2     %r9
#define pA3     %r10
#define pA4     %r11
#define pA5     %r14
#define pA6     %r15
#define pA7     %r8
@ROUT @peek

/*
void ATL_UGERK
          %rdi        %rsi              %xmm0           %rdx           
   (ATL_CINT M, ATL_CINT N, const TYPE alpha0, const TYPE *X,
             %rcx            %r8            %r9  8(%rsp)  16(%rsp)
    ATL_CINT incX, const TYPE *Y, ATL_CINT incY, TYPE *A, ATL_CINT lda)
    
*/
.text
.global ATL_asmdecor(ATL_UGERK)
ALIGN64
ATL_asmdecor(ATL_UGERK):

/*
 * Save callee-saved iregs
 */
   movq %rbp, -8(%rsp)
   movq %rbx, -16(%rsp)
@ROUT ATL_gerCL_urx4 ATL_gerCL_urx6 ATL_gerCL_urx8 @\
      ATL_mvtCL_urx4 ATL_mvtCL_urx6 ATL_mvtCL_urx8 
   movq %r12, -24(%rsp)
//   movq %r13, -32(%rsp)
   movq %r14, -40(%rsp)
   movq %r15, -48(%rsp)
@ROUT @peek
/*
 * Load & compute all integer variables
 */
   movq 8(%rsp), pA0
   movq %r8, pY
   movslq 16(%rsp), lda
   shl  $3, lda                /* lda *= sizeof */

   lea (pX, M, 8), pX           /* pX += M */
   lea (pA0, M, 8), pA0         /* pA0 += M */
@ROUT ATL_gerCL_urx2 ATL_mvtCL_urx2
   lea (pA0,lda), pA1
@ROUT ATL_gerCL_urx4 ATL_gerCL_urx6 ATL_gerCL_urx8 @\
      ATL_mvtCL_urx4 ATL_mvtCL_urx6 ATL_mvtCL_urx8 
   lea (pA0, lda), pA1
   lea (pA0, lda,2), pA2
   lea (pA1, lda,2), pA3
@ROUT ATL_gerCL_urx6 ATL_gerCL_urx8 ATL_mvtCL_urx6 ATL_mvtCL_urx8
   lea (pA0, lda,4), pA4
   lea (pA1, lda,4), pA5
@ROUT ATL_gerCL_urx8 ATL_mvtCL_urx8
   lea (pA2, lda,4), pA6
   lea (pA3, lda,4), pA7
@ROUT @peek
   neg M                        /* M = -M */
   lea (pY, N, 8), pY           /* pY += N */
   neg N                        /* N = -N */
/*
 * Start N-loop
 */
   LOOPN:
      mov M, II

@UR 1
   @multidef ur 0
   @define incM @8@
@UR 2backwards
   @multidef ur 0 64
   @define incM @16@
@UR 2straight
   @multidef ur 64 0
   @define incM @16@
@UR 4backwards
   @multidef ur 0 64 128 192
   @define incM @32@
@UR 4straight
   @multidef ur 192 128 64 0
   @define incM @32@
@UR 8backwards
   @multidef ur 0 64 128 192 256 320 384 448
   @define incM @64@
@UR 8straight
   @multidef ur 448 384 320 256 192 128 64 0
   @define incM @64@
@UR 16backwards
   @multidef ur 0 64 128 192 256 320 384 448 512 576 640 704 768 832 896 960 
   @define incM @128@
@UR 16straight
   @multidef ur 960 896 832 768 704 640 576 512 448 384 320 256 192 128 64 0
   @define incM @128@
@UR 32backwards
   @define ii @0@
   @iwhile ii < 2048
      @define ur @@(ii)@
      @iexp ii @(ii) 64 +
   @endiwhile
@UR 32straight
   @define ii @2048@
   @iwhile ii > 0
      @iexp ii @(ii) -64 +
      @define ur @@(ii)@
   @endiwhile
@UR 32straight 32backwards
   @define incM @256@
@UR !
ALIGN32
      LOOPM:
@whiledef ur
         movq @(ur)(pA0,II,8), pY
@GER T `         movq pY, 8+@(ur)(pA0,II,8)`
   @ROUT ATL_gerCL_urx2 ATL_gerCL_urx8 ATL_gerCL_urx6 ATL_gerCL_urx4 @\
         ATL_mvtCL_urx2 ATL_mvtCL_urx8 ATL_mvtCL_urx6 ATL_mvtCL_urx4 
         movq @(ur)(pA1,II,8), pY
@GER T `         movq pY, 8+@(ur)(pA1,II,8)`
   @ROUT ATL_gerCL_urx8 ATL_gerCL_urx6 ATL_gerCL_urx4 @\
         ATL_mvtCL_urx8 ATL_mvtCL_urx6 ATL_mvtCL_urx4
         movq @(ur)(pA2,II,8), pY
@GER T `         movq pY, 8+@(ur)(pA2,II,8)`
         movq @(ur)(pA3,II,8), pY
@GER T `         movq pY, 8+@(ur)(pA3,II,8)`
   @ROUT ATL_gerCL_urx8 ATL_gerCL_urx6 ATL_mvtCL_urx8 ATL_mvtCL_urx6
         movq @(ur)(pA4,II,8), pY
@GER T `         movq pY, 8+@(ur)(pA4,II,8)`
         movq @(ur)(pA5,II,8), pY
@GER T `         movq pY, 8+@(ur)(pA5,II,8)`
   @ROUT ATL_gerCL_urx8 ATL_mvtCL_urx8
         movq @(ur)(pA6,II,8), pY
@GER T `         movq pY, 8+@(ur)(pA6,II,8)`
         movq @(ur)(pA7,II,8), pY
@GER T `         movq pY, 8+@(ur)(pA7,II,8)`
   @ROUT @peek
@endwhile
      add       $@(incM), II
      jnz LOOPM
@ROUT ATL_gerCL_urx1 ATL_mvtCL_urx1
      add lda, pA0
   add $1, N
@ROUT ATL_gerCL_urx2 ATL_mvtCL_urx2
      lea (pA0, lda,2), pA0
      lea (pA1, lda,2), pA1
   add $2, N
@ROUT ATL_gerCL_urx8 ATL_mvtCL_urx8
      lea (pA0, lda,8), pA0
      lea (pA1, lda,8), pA1
      lea (pA2, lda,8), pA2
      lea (pA3, lda,8), pA3
      lea (pA4, lda,8), pA4
      lea (pA5, lda,8), pA5
      lea (pA6, lda,8), pA6
      lea (pA7, lda,8), pA7
   add $8, N
@ROUT ATL_gerCL_urx6 ATL_mvtCL_urx6
      lea (pA2, lda,4), pA0
      lea (pA3, lda,4), pA1
      lea (pA4, lda,4), pA2
      lea (pA5, lda,4), pA3
      lea (pA0, lda,4), pA4
      lea (pA1, lda,4), pA5
   add $6, N
@ROUT ATL_gerCL_urx4 ATL_mvtCL_urx4
      lea (pA0, lda,4), pA0
      lea (pA1, lda,4), pA1
      lea (pA2, lda,4), pA2
      lea (pA3, lda,4), pA3
   add $4, N
@ROUT @peek
   jnz LOOPN

/*
 * EPILOGUE: restore registers and return
 */
   movq -8(%rsp), %rbp
   movq -16(%rsp), %rbx
@ROUT ATL_gerCL_urx4 ATL_gerCL_urx6 ATL_gerCL_urx8 @\
      ATL_mvtCL_urx4 ATL_mvtCL_urx6 ATL_mvtCL_urx8
   movq -24(%rsp), %r12
//   movq -32(%rsp), %r13
   movq -40(%rsp), %r14
   movq -48(%rsp), %r15
@ROUT @peek
   ret
@ROUT ATL_dgerk_CLx14_touch ATL_dgerk_CLx12_touch ATL_dgerk_CLx10_touch
@ROUT @push
#include "atlas_asm.h"
#ifndef ATL_GAS_x8664
   #error "This kernel requires x86-64 assembly!"
#endif

#ifndef CL
   #define CL 8
#endif
/*
 * Register usage
 */
#define M       %rdi  /* already in */
#define N       %rsi  /* already in */
#define II      %rax  /* loaded in loop */
#define pX      %rdx  /* already in */
#define pA2     %rcx  /* 8(%rsp) */
#define pA9     %rbx  /* computed */
#define lda     %r11  /* 16(%rsp) */
#define pY      %rbp  /* moved from r9 */
#define mlda    %r8   /* computed */
#define lda3    %r9   /* computed */
#define incAn   %r10  /* computed */

/*
void ATL_UGERK
          %rdi        %rsi              %xmm0           %rdx
   (ATL_CINT M, ATL_CINT N, const TYPE alpha0, const TYPE *X,
             %rcx            %r8            %r9  8(%rsp)  16(%rsp)
    ATL_CINT incX, const TYPE *Y, ATL_CINT incY, TYPE *A, ATL_CINT lda)

*/
.text
.global ATL_asmdecor(ATL_UGERK)
ALIGN64
ATL_asmdecor(ATL_UGERK):

/*
 * Save callee-saved iregs
 */
   movq %rbp, -8(%rsp)
   movq %rbx, -16(%rsp)
#if 0
   movq %r12, -24(%rsp)
//   movq %r13, -32(%rsp)
   movq %r14, -40(%rsp)
   movq %r15, -48(%rsp)
#endif
/*
 * Load & compute all integer variables
 */
   movq 8(%rsp), pA2
   movq %r8, pY
   movslq 16(%rsp), lda
   mov  lda, incAn             /* incAn = lda */
   shl  $3, lda                /* lda *= sizeof */

   lea (lda,lda,2), lda3        /* lda3 = 3*lda */
   mov lda, mlda
   neg mlda                     /* mlda = -lda */
   lea (pA2, lda3,2), pA9       /* pA9 = pA0 + 6*lda */
   lea (pA9, lda3), pA9         /* pA9 = pA0 + (6+3)*lda = pA0 + 9*lda */
   lea (pA2, lda,2), pA2        /* pA2 = pA0 + 2*lda */
   lea (pX, M, 8), pX           /* pX += M */
   neg M                        /* M = -M */
   lea (pY, N, 8), pY           /* pY += N */
   neg N                        /* N = -N */
   add M, incAn                 /* incAn = lda - M */
   shl $3, incAn                /* incAn = (lda-M)*sizeof */
@ROUT ATL_dgerk_CLx10_touch
   lea (incAn, lda,8), incAn    /* incAn = (lda-M) + 8*lda */
   add lda, incAn               /* incAn = (lda-M) + 9*lda */
@ROUT ATL_dgerk_CLx12_touch
   lea (incAn, lda,8), incAn    /* incAn = (lda-M) + 8*lda */
   add lda3, incAn              /* incAn = (lda-M) + 11*lda */
@ROUT ATL_dgerk_CLx14_touch 
   lea (incAn, lda,8), incAn    /* incAn = (lda-M) + 8*lda */
   lea (incAn, lda3,2), incAn   /* incAn = (lda-M) + 14*lda */
   add mlda, incAn              /* incAn = (lda-M) + 13*lda */
@ROUT @peek
/*
 * Start N-loop
 */
   LOOPN:
      mov M, II

ALIGN32
      LOOPM:
         movq (pA2,mlda,2), pY
         movq pY, (pA2,mlda,2)        /* pA0 */
         movq (pA2,mlda), pY
         movq pY, (pA2,mlda)          /* pA1 */
         movq (pA2), pY
         movq pY, (pA2)               /* pA2 */
         movq (pA2,lda), pY
         movq pY, (pA2,lda)           /* pA3 */
         movq (pA2,lda,2), pY
         movq pY, (pA2,lda,2)         /* pA4 */
         movq (pA9,mlda,4), pY
         movq pY, (pA9,mlda,4)        /* pA5 */
         movq (pA2,lda,4), pY
         movq pY, (pA2,lda,4)         /* pA6 */
         movq (pA9,mlda,2), pY
         movq pY, (pA9,mlda,2)        /* pA7 */
         movq (pA9,mlda), pY
         movq pY, (pA9,mlda)          /* pA8 */
         movq (pA9), pY
         movq pY, (pA9)               /* pA9 */
@ROUT ATL_dgerk_CLx14_touch ATL_dgerk_CLx12_touch
         movq (pA9,lda), pY
         movq pY, (pA9,lda)           /* pA10 */
         movq (pA9,lda,2), pY
         movq pY, (pA9,lda,2)         /* pA11 */
@ROUT ATL_dgerk_CLx14_touch
         movq (pA9,lda3), pY
         movq pY, (pA9,lda3)          /* pA12 */
         movq (pA9,lda,4), pY
         movq pY, (pA9,lda,4)          /* pA13 */
@ROUT @peek
         add $8*CL, pA2
         add $8*CL, pA9
      add       $CL, II
      jnz LOOPM
      add incAn, pA2
      add incAn, pA9
@ROUT ATL_dgerk_CLx14_touch `   add $14, N`
@ROUT ATL_dgerk_CLx12_touch `   add $12, N`
@ROUT ATL_dgerk_CLx10_touch `   add $10, N`
   jnz LOOPN

/*
 * EPILOGUE: restore registers and return
 */
   movq -8(%rsp), %rbp
   movq -16(%rsp), %rbx
#if 0
   movq -24(%rsp), %r12
//   movq -32(%rsp), %r13
   movq -40(%rsp), %r14
   movq -48(%rsp), %r15
#endif
   ret
@ROUT ATL_gerCL_urx14 ATL_gerCL_urx12 ATL_gerCL_urx10 
   @addkeys GER=T
@ROUT ATL_mvtCL_urx14 ATL_mvtCL_urx12 ATL_mvtCL_urx10 
   @addkeys GER=F
@ROUT ATL_gerCL_urx14 ATL_gerCL_urx12 ATL_gerCL_urx10 @\
      ATL_mvtCL_urx14 ATL_mvtCL_urx12 ATL_mvtCL_urx10 
@ROUT @push
#include "atlas_asm.h"
#ifndef ATL_GAS_x8664
   #error "This kernel requires x86-64 assembly!"
#endif

#ifndef CL
   #define CL 8
#endif
/*
 * Register usage
 */
#define M       %rdi  /* already in */
#define N       %rsi  /* already in */
#define II      %rax  /* loaded in loop */
#define pX      %rdx  /* already in */
#define pA2     %rcx  /* 8(%rsp) */
#define pA9     %rbx  /* computed */
#define lda     %r12  /* 16(%rsp) */
#define pY      %rbp  /* moved from r9 */
#define mlda    %r8   /* computed */
#define lda3    %r9   /* computed */
#define incAn   %r10  /* computed */

/*
void ATL_UGERK
          %rdi        %rsi              %xmm0           %rdx
   (ATL_CINT M, ATL_CINT N, const TYPE alpha0, const TYPE *X,
             %rcx            %r8            %r9  8(%rsp)  16(%rsp)
    ATL_CINT incX, const TYPE *Y, ATL_CINT incY, TYPE *A, ATL_CINT lda)

*/
.text
.global ATL_asmdecor(ATL_UGERK)
ALIGN64
ATL_asmdecor(ATL_UGERK):

/*
 * Save callee-saved iregs
 */
   movq %rbp, -8(%rsp)
   movq %rbx, -16(%rsp)
#if 0
   movq %r12, -24(%rsp)
//   movq %r13, -32(%rsp)
   movq %r14, -40(%rsp)
   movq %r15, -48(%rsp)
#endif
/*
 * Load & compute all integer variables
 */
   movq 8(%rsp), pA2
   movq %r8, pY
   movslq 16(%rsp), lda
   mov  lda, incAn             /* incAn = lda */
   shl  $3, lda                /* lda *= sizeof */

   lea (lda,lda,2), lda3        /* lda3 = 3*lda */
   mov lda, mlda
   neg mlda                     /* mlda = -lda */
   lea (pA2, lda3,2), pA9       /* pA9 = pA0 + 6*lda */
   lea (pA9, lda3), pA9         /* pA9 = pA0 + (6+3)*lda = pA0 + 9*lda */
   lea (pA2, lda,2), pA2        /* pA2 = pA0 + 2*lda */
   lea (pX, M, 8), pX           /* pX += M */
   neg M                        /* M = -M */
   lea (pY, N, 8), pY           /* pY += N */
   neg N                        /* N = -N */
   add M, incAn                 /* incAn = lda - M */
   shl $3, incAn                /* incAn = (lda-M)*sizeof */
@ROUT ATL_gerCL_urx10
   lea (incAn, lda,8), incAn    /* incAn = (lda-M) + 8*lda */
   add lda, incAn               /* incAn = (lda-M) + 9*lda */
@ROUT ATL_gerCL_urx12
   lea (incAn, lda,8), incAn    /* incAn = (lda-M) + 8*lda */
   add lda3, incAn              /* incAn = (lda-M) + 11*lda */
@ROUT ATL_gerCL_urx14
   lea (incAn, lda,8), incAn    /* incAn = (lda-M) + 8*lda */
   lea (incAn, lda3,2), incAn   /* incAn = (lda-M) + 14*lda */
   add mlda, incAn              /* incAn = (lda-M) + 13*lda */
@ROUT @peek
/*
 * Start N-loop
 */
   LOOPN:
      mov M, II
@UR 1
   @multidef ur 0
   @define incM @8@
@UR 2backwards
   @multidef ur 0 64
   @define incM @16@
@UR 2straight
   @multidef ur 64 0
   @define incM @16@
@UR 4backwards
   @multidef ur 0 64 128 192
   @define incM @32@
@UR 4straight
   @multidef ur 192 128 64 0
   @define incM @32@
@UR 8backwards
   @multidef ur 0 64 128 192 256 320 384 448
   @define incM @64@
@UR 8straight
   @multidef ur 448 384 320 256 192 128 64 0
   @define incM @64@
@UR 16backwards
   @multidef ur 0 64 128 192 256 320 384 448 512 576 640 704 768 832 896 960 
   @define incM @128@
@UR 16straight
   @multidef ur 960 896 832 768 704 640 576 512 448 384 320 256 192 128 64 0
   @define incM @128@
@UR 32backwards
   @define ii @0@
   @iwhile ii < 2048
      @define ur @@(ii)@
      @iexp ii @(ii) 64 +
   @endiwhile
   @define incM @256@
@UR 32straight
   @define ii @2048@
   @iwhile ii > 0
      @iexp ii @(ii) -64 +
      @define ur @@(ii)@
   @endiwhile
   @define incM @256@
@UR !
ALIGN32
      LOOPM:
@whiledef ur
         movq @(ur)(pA2,mlda,2), pY        /* pA0 */
@GER T `         movq pY, @(ur)(pA2,mlda,2)`
         movq @(ur)(pA2,mlda), pY          /* pA1 */
@GER T `         movq pY, @(ur)(pA2,mlda)`
         movq @(ur)(pA2), pY               /* pA2 */
@GER T `         movq pY, @(ur)(pA2)`
         movq @(ur)(pA2,lda), pY           /* pA3 */
@GER T `         movq pY, @(ur)(pA2,lda)`
         movq @(ur)(pA2,lda,2), pY         /* pA4 */
@GER T `         movq pY, @(ur)(pA2,lda,2)`
         movq @(ur)(pA9,mlda,4), pY        /* pA5 */
@GER T `         movq pY, @(ur)(pA9,mlda,4)`
         movq @(ur)(pA2,lda,4), pY         /* pA6 */
@GER T `         movq pY, @(ur)(pA2,lda,4)`
         movq @(ur)(pA9,mlda,2), pY        /* pA7 */
@GER T `         movq pY, @(ur)(pA9,mlda,2)`
         movq @(ur)(pA9,mlda), pY          /* pA8 */
@GER T `         movq pY, @(ur)(pA9,mlda)`
         movq @(ur)(pA9), pY               /* pA9 */
@GER T `         movq pY, @(ur)(pA9)`
   @ROUT ATL_gerCL_urx14 ATL_gerCL_urx12 ATL_mvtCL_urx14 ATL_mvtCL_urx12
         movq @(ur)(pA9,lda), pY           /* pA10 */
@GER T `         movq pY, @(ur)(pA9,lda)`
         movq @(ur)(pA9,lda,2), pY         /* pA11 */
@GER T `         movq pY, @(ur)(pA9,lda,2)`
   @ROUT ATL_gerCL_urx14 ATL_mvtCL_urx14
         movq @(ur)(pA9,lda3), pY          /* pA12 */
@GER T `         movq pY, @(ur)(pA9,lda3)`
         movq @(ur)(pA9,lda,4), pY          /* pA13 */
@GER T `         movq pY, @(ur)(pA9,lda,4)`
   @ROUT @peek

@endwhile
@killkeys GER
         add $8*@(incM), pA2
         add $8*@(incM), pA9
      add       $@(incM), II
      jnz LOOPM
      add incAn, pA2
      add incAn, pA9
@ROUT ATL_gerCL_urx14 ATL_mvtCL_urx14 `   add $14, N`
@ROUT ATL_gerCL_urx12 ATL_mvtCL_urx12 `   add $12, N`
@ROUT ATL_gerCL_urx10 ATL_mvtCL_urx10 `   add $10, N`
   jnz LOOPN

/*
 * EPILOGUE: restore registers and return
 */
   movq -8(%rsp), %rbp
   movq -16(%rsp), %rbx
#if 0
   movq -24(%rsp), %r12
//   movq -32(%rsp), %r13
   movq -40(%rsp), %r14
   movq -48(%rsp), %r15
#endif
   ret
@ROUT ATL_gerCL_Fburst ATL_gerCL_Bburst
#include "atlas_asm.h"
#ifndef ATL_GAS_x8664
   #error "This kernel requires x86-64 assembly!"
#endif
/*
 * Register usage
 */
#define M       %rdi  /* already in */
#define N       %rsi  /* already in */
#define II      %rax  /* loaded in loop */
#define pX      %rdx  /* already in */
#define pA0     %rcx
#define IB      %rbx  /* 16(%rsp) */
#define pY      %rbp  /* moved from r9 */
#define incA    %r8   /* 8(%rsp) */
#define blksz   %r9
#define clsz    %r10

#define y0      %xmm0
#define x0      %xmm1

#ifndef BLKSZ 
   #define BLKSZ 1024
#endif
#ifndef CLSZ
   #define CLSZ 64
#endif
/*
void ATL_UGERK
          %rdi        %rsi              %xmm0           %rdx
   (ATL_CINT M, ATL_CINT N, const TYPE alpha0, const TYPE *X,
             %rcx            %r8            %r9  8(%rsp)  16(%rsp)
    ATL_CINT incX, const TYPE *Y, ATL_CINT incY, TYPE *A, ATL_CINT lda)

*/
.text
.global ATL_asmdecor(ATL_UGERK)
ALIGN64
ATL_asmdecor(ATL_UGERK):

/*
 * Save callee-saved iregs
 */
   movq %rbp, -8(%rsp)
   movq %rbx, -16(%rsp)
#if 0
   movq %r12, -24(%rsp)
   movq %r13, -32(%rsp)
   movq %r14, -40(%rsp)
   movq %r15, -48(%rsp)
#endif
/*
 * Load & compute all integer variables
 */
   movq 8(%rsp), pA0
   movq %r8, pY
   movslq 16(%rsp), incA
   sub  M, incA                /* incA = (lda-M) */
   shl  $3, incA               /* lda = (lda-M)*sizeof */

   neg M                        /* M = -M */
   neg N                        /* N = -N */
   movq $-BLKSZ, blksz
   sub blksz, pA0               /* pA0 += BLKSZ */
   movq $CLSZ, clsz
/*
 * Start N-loop
 */
   LOOPN:
      mov M, II
ALIGN4
      LOOPM:
         mov blksz, IB             /* IB = -BLKSZ */
ALIGN4
         LDLOOP:
            movq (pA0,IB), pY
            add clsz, IB
         jnz LDLOOP

         mov blksz, IB             /* IB = -BLKSZ */
ALIGN4
         STLOOP:
            movq pY, (pA0,IB)
            add clsz, IB
         jnz STLOOP

         sub blksz, pA0         /* pA0 += BLKSZ */
      add $BLKSZ/8, II
      jnz LOOPM

      add incA, pA0
   add $1, N
   jnz LOOPN

/*
 * EPILOGUE: restore registers and return
 */
   movq -8(%rsp), %rbp
   movq -16(%rsp), %rbx
#if 0
   movq -24(%rsp), %r12
   movq %r13, -32(%rsp), %r13
   movq %r14, -40(%rsp), %r14
   movq %r15, -48(%rsp), %r15
#endif
   ret
@ROUT ATL_dgerk_8x1_Cw

#include "atlas_asm.h"
#ifndef ATL_GAS_x8664
   #error "This kernel requires x86-64 assembly!"
#endif
/*
 * Register usage
 */
#define M       %rdi  /* already in */
#define N       %rsi  /* already in */
#define itmp    %rax  /* tmp used for byte load/use pipeline */
#define itb     %al
#define pX      %rdx  /* already in */
#define pA0     %rcx  /* 8(%rsp) */
#define Ab      %cl
#define II      %rbx  /* loaded in loop */
#define pY      %rbp  /* moved from r9 */
#define incAn   %r8   /* 16(%rsp) */
#define mask    %r9
#define Mr      %r10  
#define mask7   %r11
#define incX    %r12

#define y0      %xmm0
#define x0      %xmm1

/*
void ATL_UGERK
          %rdi        %rsi              %xmm0           %rdx
   (ATL_CINT M, ATL_CINT N, const TYPE alpha0, const TYPE *X,
             %rcx            %r8            %r9  8(%rsp)  16(%rsp)
    ATL_CINT incX, const TYPE *Y, ATL_CINT incY, TYPE *A, ATL_CINT lda)

*/
.text
.global ATL_asmdecor(ATL_UGERK)
ALIGN64
ATL_asmdecor(ATL_UGERK):

/*
 * Save callee-saved iregs
 */
   movq %rbp, -8(%rsp)
   movq %rbx, -16(%rsp)
   movq %r12, -24(%rsp)
#if 0
   movq %r13, -32(%rsp)
   movq %r14, -40(%rsp)
   movq %r15, -48(%rsp)
#endif
/*
 * Load & compute all integer variables
 */
   movq 8(%rsp), pA0
   movq %r8, pY
   movslq 16(%rsp), incAn       /* incAn = lda */
   sub  M, incAn                /* incAn = lda - M */
   shl  $3, incAn               /* incAn = (lda-M)*sizeof */
   movq $0x3F, mask
   movq $0x7, mask7
   not  mask7
   mov M, incX                  /* incX = M */
   shl $3, incX                 /* incX = M*sizeof */

   lea (pY, N, 8), pY           /* pY += N */
   neg N                        /* N = -N */
/*
 * Start N-loop
 */
   NLOOP:
      movb (pA0), itb           /* forced fetch of pA0 */
      movb itb, (pA0)           /* force cache write coherence message */

      movddup (pY,N,8), y0      /* y0 = {y0, y0} */
      mov M, II

/*
 *    Align data on CL boundary
 */
      test mask, pA0            /* if (pA0 & 0x3F) --> is 64-byte aligned */
      jz CLALIGNED              /* start aligned loop */
      LOOPALIGNCL:              /* loop until aligned or out of ops */
         movsd (pX), x0         /* x0 = {xx, x0} */
         mulsd y0, x0           /* x0 = {xx, x0*y0} */
         addsd (pA0), x0        /* x0 = {xx, a00+x0*y0} */
         movsd x0, (pA0)
         add $8, pA0
         add $8, pX
         sub       $1, II
         jz MLOOPDONE           /* finish MLOOP if out of M */
      test mask, pA0            /* if (pA0 & 0x3F) --> is 64-byte aligned */
      jnz LOOPALIGNCL           /* continue until aligned */

CLALIGNED:
      mov II, Mr                /* Mr = remaining iterations */
      and mask7, II             /* II = ((remaining iter)/8)*8 */
      jz  ROLLED_DO_MR          /* if nothing left, goto cleanup loop */
      sub II, Mr                /* Mr = # of iter left after II (mul8) done */

      sub $8, II                /* stop 1 iter early; if not enough its left */
      jbe ROLLED_ADD8           /* handle everything in rolled loop */

/*
 *    This loop starts at a 64-byte cache line boundary 
 */
      movapd (pX), x0           /* x0 = {x1, x0}, pipelined out of loop */
      MLOOP:
         movb 64(pA0), itb      /* forced fetch of pA0 */
         movb itb, 64(pA0)      /* force cache write coherence message */
         mulpd y0, x0           /* x0 = {x1*y0, x0*y0} */
         addpd (pA0), x0        /* x0 = {a10+x1*y0, a00+x0*y0} */
         movapd x0, (pA0)

         movapd 16(pX),x0       /* x0 = {x1, x0} */
         mulpd y0, x0           /* x0 = {x1*y0, x0*y0} */
         addpd 16(pA0), x0      /* x0 = {a10+x1*y0, a00+x0*y0} */
         movapd x0, 16(pA0)

         movapd 32(pX),x0       /* x0 = {x1, x0} */
         mulpd y0, x0           /* x0 = {x1*y0, x0*y0} */
         addpd 32(pA0), x0        /* x0 = {a10+x1*y0, a00+x0*y0} */
         movapd x0, 32(pA0)

         movapd 48(pX),x0       /* x0 = {x1, x0} */
         add    $64, pX
         mulpd y0, x0           /* x0 = {x1*y0, x0*y0} */
         addpd 48(pA0), x0      /* x0 = {a10+x1*y0, a00+x0*y0} */
         movapd x0, 48(pA0)

         movapd (pX),x0 /* x0 = {x1, x0}, SW pipelined */
         add    $64, pA0
      sub       $8, II
      jnz MLOOP
/*
 *    Drain preloading pipe
 */
      mulpd y0, x0           /* x0 = {x1*y0, x0*y0} */
      addpd (pA0), x0        /* x0 = {a10+x1*y0, a00+x0*y0} */
      movapd x0, (pA0)

      movapd 16(pX),x0          /* x0 = {x1, x0} */
      mulpd y0, x0              /* x0 = {x1*y0, x0*y0} */
      addpd 16(pA0), x0         /* x0 = {a10+x1*y0, a00+x0*y0} */
      movapd x0, 16(pA0)

      movapd 32(pX),x0         /* x0 = {x1, x0} */
      mulpd y0, x0              /* x0 = {x1*y0, x0*y0} */
      addpd 32(pA0), x0         /* x0 = {a10+x1*y0, a00+x0*y0} */
      movapd x0, 32(pA0)

      movapd 48(pX),x0          /* x0 = {x1, x0} */
      mulpd y0, x0              /* x0 = {x1*y0, x0*y0} */
      addpd 48(pA0), x0         /* x0 = {a10+x1*y0, a00+x0*y0} */
      movapd x0, 48(pA0)
         add $64, pX
         add $64, pA0
      test Mr, Mr
      jnz  ROLLED_DO_MR

MLOOPDONE:
      sub incX, pX
      add incAn, pA0
   add $1, N
   jnz NLOOP

/*
 * EPILOGUE: restore registers and return
 */
   movq -8(%rsp), %rbp
   movq -16(%rsp), %rbx
   movq -24(%rsp), %r12
#if 0
   movq %r13, -32(%rsp), %r13
   movq %r14, -40(%rsp), %r14
   movq %r15, -48(%rsp), %r15
#endif
   ret

ROLLED_ADD8:
   lea 8(II, Mr), II
   jmp MLOOP_ROLLED
ROLLED_DO_MR:
   mov Mr, II
MLOOP_ROLLED:
   movsd (pX), x0       /* x0 = {xx, x0} */
   mulsd y0, x0         /* x0 = {xx, x0*y0} */
   addsd (pA0), x0      /* x0 = {xx, a00+x0*y0} */
   movsd x0, (pA0)
   add $8, pX
   add $8, pA0          /* pA0++ */
sub  $1, II
jnz MLOOP_ROLLED
jmp MLOOPDONE
@ROUT mvtch.idx mvtch_L2.idx
   @define rn @mvtch@
@ROUT r1tch1_L2.idx r1tch1.idx
   @define rn @r1tch1@
@ROUT r1tch2_L2.idx r1tch2.idx
   @define rn @r1tch2@
@ROUT mvtch_L2.idx
   @define id @11000@
@ROUT r1tch1_L2.idx
   @define id @12000@
@ROUT r1tch2_L2.idx
   @define id @13000@
@ROUT mvtch_L2.idx r1tch1_L2.idx r1tch2_L2.idx
@multidef nu 12 10 8 6 5 4 3 2 1
@ROUT mvtch.idx
   @define id @15000@
@ROUT r1tch1.idx
   @define id @16000@
@ROUT r1tch2.idx
   @define id @17000@
@ROUT mvtch.idx r1tch1.idx r1tch2.idx
@multidef nu 14 13 12 11 10 9 8 7 6 5 4 3 2 1
@ROUT mvtch.idx mvtch_L2.idx r1tch1.idx r1tch2.idx r1tch1_L2.idx r1tch2_L2.idx
@whiledef nu
   @whiledef mu 128 120 112 104 96 88 80 72 64 56 48 40 32 24 16 8
      @iexp id 1 @(id) +
ID=@(id) XU=@(mu) YU=@(nu) AUTH='RCW' ROUT='ATL_@(rn)_@(mu)x@(nu).c' FYU=1 \
   comp='gcc' CFLAGS='-x assembler-with-cpp' alignA=16 alignX=16 alignY=16 
   @endwhile
@endwhile
@ROUT mvtch r1tch1 r1tch2
#include "atlas_asm.h"
#ifndef ATL_GAS_x8664
   #error "This kernel requires x86-64 assembly!"
#endif
/*
 * Register usage
 */
#define N       %rsi
#define M       %r9
#define incN    %rbp
#define pA0     %r8   /* 8(%rsp) */
#define lda     %rax  /* 16(%rsp) */
#define lda3    %rcx
#define lda5    %rdi
#define lda7    %rdx
@NU 10 11 12 13 14
#define lda9    %r11
@NU 12 13 14
#define lda11   %r12
@NU 14
#define lda13   %r13
@NU !
#define II      %r10

@NU 14
#define incA $-@(@mu)*8
@NU ! 14
#define incA    %r13
@NU 14 13 12
#define incII $@(@mu)
@NU ! 14 13 12
#define incII   %r12
@NU !
@NU ! 14 13 12 11 10
#define pY      %r11
@NU 14 13 12 11 10
#define pY      %xmm0
@NU !

/*
void ATL_UGERK
          %rdi        %rsi              %xmm0           %rdx
   (ATL_CINT M, ATL_CINT N, const TYPE alpha0, const TYPE *X,
             %rcx            %r8            %r9  8(%rsp)  16(%rsp)
    ATL_CINT incX, const TYPE *Y, ATL_CINT incY, TYPE *A, ATL_CINT lda)

*/
.text
.global ATL_asmdecor(ATL_UGERK)
ALIGN64
ATL_asmdecor(ATL_UGERK):

/*
 * Save callee-saved iregs
 */
   movq %rbp, -8(%rsp)
   movq %rbx, -16(%rsp)
   movq %r12, -24(%rsp)
   movq %r13, -32(%rsp)
   movq %r14, -40(%rsp)
   movq %r15, -48(%rsp)
/*
 * Save M & N, load A
 */
   movq %rdi, M

   movq 8(%rsp), pA0            /* pA0 = pA0 */
   movslq 16(%rsp), lda         /* lda = lda */
   shl  $3, lda                 /* rtmp = lda * sizeof */

@MU ! 8 16
   sub $-128, pA0               /* pA0 = ((char*)pA0)+128 */
@MU !
   lea (lda, lda,2), lda3       /* lda3  = 3*lda*sizeof */
   lea (lda3, lda,2), lda5      /* lda5  = 5*lda*sizeof */
   lea (lda3, lda,4), lda7      /* lda7  = 7*lda*sizeof */
@NU ! 1 2 3 4 5 6 7 8 9
   lea (lda, lda,8), lda9       /* lda9  = 9*lda*sizeof */
@NU 14 13 12
   lea (lda3, lda,8), lda11     /* lda11 = 11*lda*sizeof */
@NU ! 14 13 12
   mov $@(@mu), incII           /* incII = MU */
@NU 14
   lea (lda3, lda5,2), lda13    /* lda13 = 13*lda*sizeof */
@NU ! 14
   mov $-@(@mu)*8, incA         /* incA = MU*sizeof */
@NU !
   mov M, incN                  /* incN = M */
   shl $3, incN                 /* incN = M*sizeof */
   neg incN                     /* incN = -M*sizeof */
@NU 1
   lea (incN, lda), incN        /* incN = (lda - M)*sizeof */
@NU 2
   lea (incN, lda,2), incN      /* incN = (2*lda - M)*sizeof */
@NU 3
   lea (incN, lda3), incN       /* incN = (3*lda - M)*sizeof */
@NU 4
   lea (incN, lda,4), incN      /* incN = (4*lda - M)*sizeof */
@NU 5
   lea (incN, lda5), incN       /* incN = (5*lda - M)*sizeof */
@NU 6
   lea (incN, lda3,2), incN     /* incN = (6*lda - M)*sizeof */
@NU 7
   lea (incN, lda7), incN       /* incN = (7*lda - M)*sizeof */
@NU 8
   lea (incN, lda,8), incN      /* incN = (8*lda - M)*sizeof */
@NU 9
   lea (incN, lda,8), incN      /* incN = (8*lda - M)*sizeof */
   add lda, incN                /* incN = (9*lda - M)*sizeof */
@NU 10
   lea (incN, lda5,2), incN     /* incN = (10*lda - M)*sizeof */
@NU 11
   lea (incN, lda5,2), incN     /* incN = (10*lda - M)*sizeof */
   add lda, incN                /* incN = (11*lda - M)*sizeof */
@NU 12
   lea (incN, lda3,4), incN     /* incN = (12*lda - M)*sizeof */
@NU 13
   lea (incN, lda3,4), incN     /* incN = (12*lda - M)*sizeof */
   add lda, incN                /* incN = (13*lda - M)*sizeof */
@NU 14
   lea (incN, lda7,2), incN     /* incN = (14*lda - M)*sizeof */
@NU !
   mov M, II

/*
 * Start N-loop
 */
   ALIGN32
   LOOPN:
/*      LOOPM: */
@MU 8 16
         movq (pA0), pY
@ROUT r1tch1 `         movq pY, (pA0)`
@ROUT r1tch2 `         movq II, (pA0)`
  @NU ! 1
         movq (pA0,lda), pY
@ROUT r1tch1 `         movq pY, (pA0,lda)`
@ROUT r1tch2 `         movq II, (pA0,lda)`
  @NU ! 1 2
         movq (pA0,lda,2), pY
@ROUT r1tch1 `         movq pY, (pA0,lda,2)`
@ROUT r1tch2 `         movq II, (pA0,lda,2)`
  @NU ! 1 2 3
         movq (pA0,lda3), pY
@ROUT r1tch1 `         movq pY, (pA0,lda3)`
@ROUT r1tch2 `         movq II, (pA0,lda3)`
  @NU ! 1 2 3 4
         movq (pA0,lda,4), pY
@ROUT r1tch1 `         movq pY, (pA0,lda,4)`
@ROUT r1tch2 `         movq II, (pA0,lda,4)`
  @NU ! 1 2 3 4 5
         movq (pA0,lda5), pY
@ROUT r1tch1 `         movq pY, (pA0,lda5)`
@ROUT r1tch2 `         movq II, (pA0,lda5)`
  @NU ! 1 2 3 4 5 6
         movq (pA0,lda3,2), pY
@ROUT r1tch1 `         movq pY, (pA0,lda3,2)`
@ROUT r1tch2 `         movq II, (pA0,lda3,2)`
  @NU ! 1 2 3 4 5 6 7
         movq (pA0,lda7), pY
@ROUT r1tch1 `         movq pY, (pA0,lda7)`
@ROUT r1tch2 `         movq II, (pA0,lda7)`
  @NU ! 1 2 3 4 5 6 7 8
         movq (pA0,lda,8), pY
@ROUT r1tch1 `         movq pY, (pA0,lda,8)`
@ROUT r1tch2 `         movq II, (pA0,lda,8)`
  @NU ! 1 2 3 4 5 6 7 8 9
         movq (pA0,lda9), pY
@ROUT r1tch1 `         movq pY, (pA0,lda9)`
@ROUT r1tch2 `         movq II, (pA0,lda9)`
  @NU ! 1 2 3 4 5 6 7 8 9 10
         movq (pA0,lda5,2), pY
@ROUT r1tch1 `         movq pY, (pA0,lda5,2)`
@ROUT r1tch2 `         movq II, (pA0,lda5,2)`
  @NU ! 1 2 3 4 5 6 7 8 9 10 11
         movq (pA0,lda11), pY
@ROUT r1tch1 `         movq pY, (pA0,lda11)`
@ROUT r1tch2 `         movq II, (pA0,lda11)`
  @NU ! 1 2 3 4 5 6 7 8 9 10 11 12
         movq (pA0,lda3,4), pY
@ROUT r1tch1 `         movq pY, (pA0,lda3,4)`
@ROUT r1tch2 `         movq II, (pA0,lda3,4)`
  @NU ! 1 2 3 4 5 6 7 8 9 10 11 12 13
         movq (pA0,lda13), pY
@ROUT r1tch1 `         movq pY, (pA0,lda13)`
@ROUT r1tch2 `         movq II, (pA0,lda13)`
  @NU !

@MU 16
         movq 64(pA0), pY
@ROUT r1tch1 `         movq pY, 64(pA0)`
@ROUT r1tch2 `         movq II, 64(pA0)`
  @NU ! 1
         movq 64(pA0,lda), pY
@ROUT r1tch1 `         movq pY, 64(pA0,lda)`
@ROUT r1tch2 `         movq II, 64(pA0,lda)`
  @NU ! 1 2
         movq 64(pA0,lda,2), pY
@ROUT r1tch1 `         movq pY, 64(pA0,lda,2)`
@ROUT r1tch2 `         movq II, 64(pA0,lda,2)`
  @NU ! 1 2 3
         movq 64(pA0,lda3), pY
@ROUT r1tch1 `         movq pY, 64(pA0,lda3)`
@ROUT r1tch2 `         movq II, 64(pA0,lda3)`
  @NU ! 1 2 3 4
         movq 64(pA0,lda,4), pY
@ROUT r1tch1 `         movq pY, 64(pA0,lda,4)`
@ROUT r1tch2 `         movq II, 64(pA0,lda,4)`
  @NU ! 1 2 3 4 5
         movq 64(pA0,lda5), pY
@ROUT r1tch1 `         movq pY, 64(pA0,lda5)`
@ROUT r1tch2 `         movq II, 64(pA0,lda5)`
  @NU ! 1 2 3 4 5 6
         movq 64(pA0,lda3,2), pY
@ROUT r1tch1 `         movq pY, 64(pA0,lda3,2)`
@ROUT r1tch2 `         movq II, 64(pA0,lda3,2)`
  @NU ! 1 2 3 4 5 6 7
         movq 64(pA0,lda7), pY
@ROUT r1tch1 `         movq pY, 64(pA0,lda7)`
@ROUT r1tch2 `         movq II, 64(pA0,lda7)`
  @NU ! 1 2 3 4 5 6 7 8
         movq 64(pA0,lda,8), pY
@ROUT r1tch1 `         movq pY, 64(pA0,lda,8)`
@ROUT r1tch2 `         movq II, 64(pA0,lda,8)`
  @NU ! 1 2 3 4 5 6 7 8 9
         movq 64(pA0,lda9), pY
@ROUT r1tch1 `         movq pY, 64(pA0,lda9)`
@ROUT r1tch2 `         movq II, 64(pA0,lda9)`
  @NU ! 1 2 3 4 5 6 7 8 9 10
         movq 64(pA0,lda5,2), pY
@ROUT r1tch1 `         movq pY, 64(pA0,lda5,2)`
@ROUT r1tch2 `         movq II, 64(pA0,lda5,2)`
  @NU ! 1 2 3 4 5 6 7 8 9 10 11
         movq 64(pA0,lda11), pY
@ROUT r1tch1 `         movq pY, 64(pA0,lda11)`
@ROUT r1tch2 `         movq II, 64(pA0,lda11)`
  @NU ! 1 2 3 4 5 6 7 8 9 10 11 12
         movq 64(pA0,lda3,4), pY
@ROUT r1tch1 `         movq pY, 64(pA0,lda3,4)`
@ROUT r1tch2 `         movq II, 64(pA0,lda3,4)`
  @NU ! 1 2 3 4 5 6 7 8 9 10 11 12 13
         movq 64(pA0,lda13), pY
@ROUT r1tch1 `         movq pY, 64(pA0,lda13)`
@ROUT r1tch2 `         movq II, 64(pA0,lda13)`
  @NU !

@MU ! 8 16
   @define n @@(@mu)@
   @iexp n 8 @(n) *
   @define i @0@
   @iwhile i < @(n)
      @iif @(i) = 128
         movq (pA0), pY
@ROUT r1tch1 `         movq pY, (pA0)`
@ROUT r1tch2 `         movq II, (pA0)`
         @NU ! 1
         movq (pA0,lda), pY
@ROUT r1tch1 `         movq pY, (pA0,lda)`
@ROUT r1tch2 `         movq II, (pA0,lda)`
         @NU ! 1 2
         movq (pA0,lda,2), pY
@ROUT r1tch1 `         movq pY, (pA0,lda,2)`
@ROUT r1tch2 `         movq II, (pA0,lda,2)`
         @NU ! 1 2 3
         movq (pA0,lda3), pY
@ROUT r1tch1 `         movq pY, (pA0,lda3)`
@ROUT r1tch2 `         movq II, (pA0,lda3)`
         @NU ! 1 2 3 4
         movq (pA0,lda,4), pY
@ROUT r1tch1 `         movq pY, (pA0,lda,4)`
@ROUT r1tch2 `         movq II, (pA0,lda,4)`
         @NU ! 1 2 3 4 5
         movq (pA0,lda5), pY
@ROUT r1tch1 `         movq pY, (pA0,lda5)`
@ROUT r1tch2 `         movq II, (pA0,lda5)`
         @NU ! 1 2 3 4 5 6
         movq (pA0,lda3,2), pY
@ROUT r1tch1 `         movq pY, (pA0,lda3,2)`
@ROUT r1tch2 `         movq II, (pA0,lda3,2)`
         @NU ! 1 2 3 4 5 6 7
         movq (pA0,lda7), pY
@ROUT r1tch1 `         movq pY, (pA0,lda7)`
@ROUT r1tch2 `         movq II, (pA0,lda7)`
         @NU ! 1 2 3 4 5 6 7 8
         movq (pA0,lda,8), pY
@ROUT r1tch1 `         movq pY, (pA0,lda,8)`
@ROUT r1tch2 `         movq II, (pA0,lda,8)`
         @NU ! 1 2 3 4 5 6 7 8 9
         movq (pA0,lda9), pY
@ROUT r1tch1 `         movq pY, (pA0,lda9)`
@ROUT r1tch2 `         movq II, (pA0,lda9)`
         @NU ! 1 2 3 4 5 6 7 8 9 10
         movq (pA0,lda5,2), pY
@ROUT r1tch1 `         movq pY, (pA0,lda5,2)`
@ROUT r1tch2 `         movq II, (pA0,lda5,2)`
         @NU ! 1 2 3 4 5 6 7 8 9 10 11
         movq (pA0,lda11), pY
@ROUT r1tch1 `         movq pY, (pA0,lda11)`
@ROUT r1tch2 `         movq II, (pA0,lda11)`
         @NU ! 1 2 3 4 5 6 7 8 9 10 11 12
         movq (pA0,lda3,4), pY
@ROUT r1tch1 `         movq pY, (pA0,lda3,4)`
@ROUT r1tch2 `         movq II, (pA0,lda3,4)`
         @NU ! 1 2 3 4 5 6 7 8 9 10 11 12 13
         movq (pA0,lda13), pY
@ROUT r1tch1 `         movq pY, (pA0,lda13)`
@ROUT r1tch2 `         movq II, (pA0,lda13)`
         @NU !

      @endiif
      @iif @(i) ! 128
         movq @(i)-128(pA0), pY
@ROUT r1tch1 `         movq pY, @(i)-128(pA0)`
@ROUT r1tch2 `         movq II, @(i)-128(pA0)`
         @NU ! 1
         movq @(i)-128(pA0,lda), pY
@ROUT r1tch1 `         movq pY, @(i)-128(pA0,lda)`
@ROUT r1tch2 `         movq II, @(i)-128(pA0,lda)`
         @NU ! 1 2
         movq @(i)-128(pA0,lda,2), pY
@ROUT r1tch1 `         movq pY, @(i)-128(pA0,lda,2)`
@ROUT r1tch2 `         movq II, @(i)-128(pA0,lda,2)`
         @NU ! 1 2 3
         movq @(i)-128(pA0,lda3), pY
@ROUT r1tch1 `         movq pY, @(i)-128(pA0,lda3)`
@ROUT r1tch2 `         movq II, @(i)-128(pA0,lda3)`
         @NU ! 1 2 3 4
         movq @(i)-128(pA0,lda,4), pY
@ROUT r1tch1 `         movq pY, @(i)-128(pA0,lda,4)`
@ROUT r1tch2 `         movq II, @(i)-128(pA0,lda,4)`
         @NU ! 1 2 3 4 5
         movq @(i)-128(pA0,lda5), pY
@ROUT r1tch1 `         movq pY, @(i)-128(pA0,lda5)`
@ROUT r1tch2 `         movq II, @(i)-128(pA0,lda5)`
         @NU ! 1 2 3 4 5 6
         movq @(i)-128(pA0,lda3,2), pY
@ROUT r1tch1 `         movq pY, @(i)-128(pA0,lda3,2)`
@ROUT r1tch2 `         movq II, @(i)-128(pA0,lda3,2)`
         @NU ! 1 2 3 4 5 6 7
         movq @(i)-128(pA0,lda7), pY
@ROUT r1tch1 `         movq pY, @(i)-128(pA0,lda7)`
@ROUT r1tch2 `         movq II, @(i)-128(pA0,lda7)`
         @NU ! 1 2 3 4 5 6 7 8
         movq @(i)-128(pA0,lda,8), pY
@ROUT r1tch1 `         movq pY, @(i)-128(pA0,lda,8)`
@ROUT r1tch2 `         movq II, @(i)-128(pA0,lda,8)`
         @NU ! 1 2 3 4 5 6 7 8 9
         movq @(i)-128(pA0,lda9), pY
@ROUT r1tch1 `         movq pY, @(i)-128(pA0,lda9)`
@ROUT r1tch2 `         movq II, @(i)-128(pA0,lda9)`
         @NU ! 1 2 3 4 5 6 7 8 9 10
         movq @(i)-128(pA0,lda5,2), pY
@ROUT r1tch1 `         movq pY, @(i)-128(pA0,lda5,2)`
@ROUT r1tch2 `         movq II, @(i)-128(pA0,lda5,2)`
         @NU ! 1 2 3 4 5 6 7 8 9 10 11
         movq @(i)-128(pA0,lda11), pY
@ROUT r1tch1 `         movq pY, @(i)-128(pA0,lda11)`
@ROUT r1tch2 `         movq II, @(i)-128(pA0,lda11)`
         @NU ! 1 2 3 4 5 6 7 8 9 10 11 12
         movq @(i)-128(pA0,lda3,4), pY
@ROUT r1tch1 `         movq pY, @(i)-128(pA0,lda3,4)`
@ROUT r1tch2 `         movq II, @(i)-128(pA0,lda3,4)`
         @NU ! 1 2 3 4 5 6 7 8 9 10 11 12 13
         movq @(i)-128(pA0,lda13), pY
@ROUT r1tch1 `         movq pY, @(i)-128(pA0,lda13)`
@ROUT r1tch2 `         movq II, @(i)-128(pA0,lda13)`
         @NU !

      @endiif
      @iexp i 64 @(i) +
   @endiwhile
@MU !
      sub incA, pA0
      sub incII, II
      jnz LOOPN
      mov M, II
      add  incN, pA0
   sub $@(@nu), N
   jnz LOOPN

/*
 * EPILOGUE: restore registers and return
 */
   movq -8(%rsp), %rbp
   movq -16(%rsp), %rbx
   movq -24(%rsp), %r12
   movq -32(%rsp), %r13
   movq -40(%rsp), %r14
   movq -48(%rsp), %r15
   ret
@ROUT touch_1x14_nz
#include "atlas_asm.h"
#ifndef ATL_GAS_x8664
   #error "This kernel requires x86-64 assembly!"
#endif
/*
 * Register usage
 */
#define pA0     %r8   /* 8(%rsp) */
#define pA1     %r9  
#define pA2     %r10
#define pA3     %r11
#define pA4     %r14
#define pA5     %r15
#define pA6     %rbx
#define pA7     %rbp
#define pA8     %rdi
#define pA9     %rsi
#define pA10    %rdx
#define pA11    %rcx
#define pA12    %r12
#define pA13    %r13
#define pA14    %r14
#define rtmp    %rax  /* 16(%rsp) */
#define pY      %xmm0

/*
void ATL_UGERK
          %rdi        %rsi              %xmm0           %rdx
   (ATL_CINT M, ATL_CINT N, const TYPE alpha0, const TYPE *X,
             %rcx            %r8            %r9  8(%rsp)  16(%rsp)
    ATL_CINT incX, const TYPE *Y, ATL_CINT incY, TYPE *A, ATL_CINT lda)

*/
.text
.global ATL_asmdecor(ATL_UGERK)
ALIGN64
ATL_asmdecor(ATL_UGERK):

/*
 * Save callee-saved iregs
 */
   movq %rbp, -8(%rsp)
   movq %rbx, -16(%rsp)
   movq %r12, -24(%rsp)
   movq %r13, -32(%rsp)
   movq %r14, -40(%rsp)
   movq %r15, -48(%rsp)
#define NOFF -56
#define MOFF -64
#define LDAOFF  -72
#define MMOFF -80
#define INCNOFF -88
/*
 * Save M & N, load A
 */
   movq %rdi, MOFF(%rsp)
   movq %rsi, NOFF(%rsp)

   movq 8(%rsp), pA0
   lea  (pA0, %rdi, 8), pA0     /* pA0 += M */
   movslq 16(%rsp), rtmp
   shl  $3, rtmp                /* rtmp = lda * sizeof */
   movq rtmp, LDAOFF(%rsp)

   lea (pA0, rtmp), pA1
   lea (pA0, rtmp,2), pA2
   lea (pA0, rtmp,4), pA4
   lea (pA0, rtmp,8), pA8
   lea (pA1, rtmp,2), pA3
   lea (pA1, rtmp,4), pA5
   lea (pA1, rtmp,8), pA9
   lea (pA2, rtmp,4), pA6
   lea (pA2, rtmp,8), pA10
   lea (pA3, rtmp,4), pA7
   lea (pA3, rtmp,8), pA11
   lea (pA4, rtmp,8), pA12
   lea (rtmp, rtmp,4), pA13      /* pA13 = 5*lda*sizeof */
   lea (pA13, rtmp,2), pA13      /* pA13 = 7*lda*sizeof */
   shl $1, pA13                 /* pA13 = 14*lda*sizeof */
@skip    shl $3, rtmp
@skip   sub rtmp, pA13               /* pA13 = (14*lda-M)*sizeof */
   movq pA13, INCNOFF(%rsp)
   lea (pA5, rtmp,8), pA13
   movq MOFF(%rsp), rtmp        /* rtmp = M */
   neg rtmp                     /* rtmp = -M */
   movq rtmp, MMOFF(%rsp)
   
/*
 * Start N-loop
 */
   ALIGN32
   LOOPN:
/*      LOOPM: */
         #ifndef NO_OP
            movq (pA0,rtmp,8), pY
            movq (pA1,rtmp,8), pY
            movq (pA2,rtmp,8), pY
            movq (pA3,rtmp,8), pY
            movq (pA4,rtmp,8), pY
            movq (pA5,rtmp,8), pY
            movq (pA6,rtmp,8), pY
            movq (pA7,rtmp,8), pY
            movq (pA8,rtmp,8), pY
            movq (pA9,rtmp,8), pY
            movq (pA10,rtmp,8), pY
            movq (pA11,rtmp,8), pY
            movq (pA12,rtmp,8), pY
            movq (pA13,rtmp,8), pY
         #endif
      add       $8, rtmp
      jnz LOOPN
      movq INCNOFF(%rsp), rtmp
      add  rtmp, pA0
      add  rtmp, pA1
      add  rtmp, pA2
      add  rtmp, pA3
      add  rtmp, pA4
      add  rtmp, pA5
      add  rtmp, pA6
      add  rtmp, pA7
      add  rtmp, pA8
      add  rtmp, pA9
      add  rtmp, pA10
      add  rtmp, pA11
      add  rtmp, pA12
      add  rtmp, pA13
      movq MMOFF(%rsp), rtmp
   sub $14, NOFF(%rsp)
   jnz LOOPN

/*
 * EPILOGUE: restore registers and return
 */
   movq -8(%rsp), %rbp
   movq -16(%rsp), %rbx
   movq -24(%rsp), %r12
   movq -32(%rsp), %r13
   movq -40(%rsp), %r14
   movq -48(%rsp), %r15
   ret
@ROUT touch_1x1_nz touch_1x1_inc_nz touch_1x1_lp touch_1x1_dec_nz
#include "atlas_asm.h"
#ifndef ATL_GAS_x8664
   #error "This kernel requires x86-64 assembly!"
#endif
/*
 * Register usage
 */
#define M       %rdi  /* already in */
#define N       %rsi  /* already in */
#define II      %rcx  /* loaded in loop */
#define pX      %rdx  /* already in */
#define pA0     %rax  /* 8(%rsp) */
#define lda     %rbx  /* 16(%rsp) */
#define pY      %rbp  /* moved from r9 */

/*
void ATL_UGERK
          %rdi        %rsi              %xmm0           %rdx
   (ATL_CINT M, ATL_CINT N, const TYPE alpha0, const TYPE *X,
             %rcx            %r8            %r9  8(%rsp)  16(%rsp)
    ATL_CINT incX, const TYPE *Y, ATL_CINT incY, TYPE *A, ATL_CINT lda)

*/
.text
.global ATL_asmdecor(ATL_UGERK)
ALIGN64
ATL_asmdecor(ATL_UGERK):

/*
 * Save callee-saved iregs
 */
   movq %rbp, -8(%rsp)
   movq %rbx, -16(%rsp)
/*
 * Load & compute all integer variables
 */
   movq 8(%rsp), pA0
   movq %r8, pY
   movslq 16(%rsp), lda
   shl  $3, lda                /* lda *= sizeof */

@ROUT touch_1x1_nz touch_1x1_inc_nz
   lea (pA0, M, 8), pA0         /* pA0 += M */
   neg M                        /* M = -M */
@ROUT touch_1x1_lp touch_1x1_dec_nz 
   shl $3, M                    /* M *= sizeof */
   sub M,  lda                  /* lda = (lda - M)*sizeof */
   shr $6, M                    /* M = M/8 */
@ROUT touch_1x1_nz touch_1x1_inc_nz touch_1x1_lp touch_1x1_dec_nz
   neg N                        /* N = -N */
   mov M, II
/*
 * Start N-loop
 */
   ALIGN32
   LOOPN:
/*      LOOPM: */
@ROUT touch_1x1_nz touch_1x1_inc_nz
         #ifndef NO_OP
            movq (pA0,II,8), pY
         #endif
      add       $8, II
      jnz LOOPN
@ROUT touch_1x1_lp touch_1x1_dec_nz
         #ifndef NO_OP
            movq (pA0), pY
         #endif
         add  $64, pA0
@ROUT touch_1x1_dec_nz
      dec II
      jnz LOOPN
@ROUT touch_1x1_lp
      LOOP LOOPN
@ROUT touch_1x1_nz touch_1x1_lp touch_1x1_inc_nz touch_1x1_dec_nz
      add lda, pA0
      mov M, II
@ROUT touch_1x1_nz touch_1x1_lp
   add $1, N
@ROUT touch_1x1_inc_nz touch_1x1_dec_nz
   inc N
@ROUT touch_1x1_nz touch_1x1_lp touch_1x1_inc_nz touch_1x1_dec_nz
   jnz LOOPN

/*
 * EPILOGUE: restore registers and return
 */
   movq -8(%rsp), %rbp
   movq -16(%rsp), %rbx
   ret
@ROUT ATL_mvtch8x1pf ATL_r1tch8x1pf
#include "atlas_asm.h"
#ifndef ATL_GAS_x8664
   #error "This kernel requires x86-64 assembly!"
#endif
/*
 * Register usage
 */
#define N       %rsi
#define M       %r9
#define incN    %rbp
#define pA0     %r8   /* 8(%rsp) */
#define lda     %rax  /* 16(%rsp) */
#define lda3    %rcx
#define lda5    %rdi
#define lda7    %rdx
#define II      %r10

#define incA    %r13
#define incII   %r12
#define pY      %r11

#ifndef PFDIST
   #define PFDIST
#endif
#ifndef PREFETCH
   #define PREFETCH prefetchnta
#endif
/*
void ATL_UGERK
          %rdi        %rsi              %xmm0           %rdx
   (ATL_CINT M, ATL_CINT N, const TYPE alpha0, const TYPE *X,
             %rcx            %r8            %r9  8(%rsp)  16(%rsp)
    ATL_CINT incX, const TYPE *Y, ATL_CINT incY, TYPE *A, ATL_CINT lda)

*/
.text
.global ATL_asmdecor(ATL_UGERK)
ALIGN64
ATL_asmdecor(ATL_UGERK):

/*
 * Save callee-saved iregs
 */
   movq %rbp, -8(%rsp)
   movq %rbx, -16(%rsp)
   movq %r12, -24(%rsp)
   movq %r13, -32(%rsp)
   movq %r14, -40(%rsp)
   movq %r15, -48(%rsp)
/*
 * Save M & N, load A
 */
   movq %rdi, M

   movq 8(%rsp), pA0            /* pA0 = pA0 */
   movslq 16(%rsp), lda         /* lda = lda */
   shl  $3, lda                 /* rtmp = lda * sizeof */

   lea (lda, lda,2), lda3       /* lda3  = 3*lda*sizeof */
   lea (lda3, lda,2), lda5      /* lda5  = 5*lda*sizeof */
   lea (lda3, lda,4), lda7      /* lda7  = 7*lda*sizeof */
   mov $8, incII           /* incII = MU */
   mov $-8*8, incA         /* incA = MU*sizeof */
   mov M, incN                  /* incN = M */
   shl $3, incN                 /* incN = M*sizeof */
   neg incN                     /* incN = -M*sizeof */
   lea (incN, lda), incN        /* incN = (lda - M)*sizeof */
   mov M, II

/*
 * Start N-loop
 */
   ALIGN32
   LOOPN:
/*      LOOPM: */
         movq (pA0), pY
@ROUT ATL_r1tch8x1pf `         movq pY, (pA0)`

      sub incA, pA0
      sub incII, II
   
#ifdef PFCOL
       PREFETCH -64(pA0,lda)
#else
       PREFETCH PFDIST(pA0)
#endif
      jnz LOOPN
      mov M, II
      add  incN, pA0
   sub $1, N
   jnz LOOPN

/*
 * EPILOGUE: restore registers and return
 */
   movq -8(%rsp), %rbp
   movq -16(%rsp), %rbx
   movq -24(%rsp), %r12
   movq -32(%rsp), %r13
   movq -40(%rsp), %r14
   movq -48(%rsp), %r15
   ret
@ROUT mvpf8x1_NTA.idx r1pf8x1_NTA.idx
   @define pf @prefetchnta@
@ROUT mvpf8x1_T0.idx r1pf8x1_T0.idx
   @define pf @prefetcht0@
@ROUT r1pf8x1_W.idx
   @define pf @prefetchw@
@ROUT mvpf8x1_T1.idx r1pf8x1_T1.idx
   @define pf @prefetcht1@
@ROUT mvpf8x1_R.idx r1pf8x1_R.idx
   @define pf @prefetch@
@ROUT mvpf8x1_NTA.idx mvpf8x1_T0.idx mvpf8x1_T1.idx mvpf8x1_R.idx
   @define rt @mv@
@ROUT r1pf8x1_NTA.idx r1pf8x1_T0.idx r1pf8x1_T1.idx r1pf8x1_R.idx r1pf8x1_W.idx
   @define rt @r1@
@ROUT mvpf8x1_NTA.idx 
   @define ID @20000@
@ROUT mvpf8x1_T0.idx 
   @define ID @20100@
@ROUT mvpf8x1_T1.idx 
   @define ID @20200@
@ROUT mvpf8x1_R.idx 
   @define ID @20300@
@ROUT r1pf8x1_NTA.idx 
   @define ID @20400@
@ROUT r1pf8x1_T0.idx 
   @define ID @20500@
@ROUT r1pf8x1_T1.idx 
   @define ID @20600@
@ROUT r1pf8x1_R.idx 
   @define ID @20700@
@ROUT r1pf8x1_W.idx
   @define ID @20800@
@ROUT mvpf8x1_NTA.idx mvpf8x1_T0.idx mvpf8x1_T1.idx mvpf8x1_R.idx @\
      r1pf8x1_NTA.idx r1pf8x1_T0.idx r1pf8x1_T1.idx r1pf8x1_R.idx r1pf8x1_W.idx
ID=@(ID) XU=8 YU=1 AUTH='R. Clint Whaley' ROUT='ATL_@(rt)tch8x1pf.c' \
   alignA=16 alignX=16 alignY=16 FYU=1 COMP='gcc' \
   CFLAGS='-x assembler-with-cpp'
   @iexp ID @(ID) 1 +
ID=@(ID) XU=8 YU=1 AUTH='R. Clint Whaley' ROUT='ATL_@(rt)tch8x1pf.c' \
   alignA=16 alignX=16 alignY=16 FYU=1 COMP='gcc' \
   CFLAGS='-x assembler-with-cpp -DPREFETCH=@(pf) -DPFCOL=1'
   @iexp ID @(ID) 1 +
@whiledef dist 4096 2048 1024 512 256 128 64
ID=@(ID) XU=8 YU=1 AUTH='R. Clint Whaley' ROUT='ATL_@(rt)tch8x1pf.c' \
   alignA=16 alignX=16 alignY=16 FYU=1 COMP='gcc' \
   CFLAGS='-x assembler-with-cpp -DPREFETCH=@(pf) -DPFDIST=@(dist)'
   @iexp ID @(ID) 1 +
@endwhile
@ROUT loop1x1.idx loop1x1_noop.idx
@multidef ID          10001            10002        10003            10004
@whiledef rout touch_1x1_lp touch_1x1_dec_nz  touch_1x1_nz touch_1x1_inc_nz
ID=@(ID) XU=8 YU=1 AUTH='R. Clint Whaley' ROUT='@(rout).c' \
   alignA=16 alignX=16 alignY=16 FYU=1 COMP='gcc' \
@ROUT loop1x1_noop.idx `   CFLAGS='-DNO_OP=1 -x assembler-with-cpp'`
@ROUT loop1x1.idx `   CFLAGS='-x assembler-with-cpp'`
   @undef ID
@endwhile
@ROUT loop_L2M_noop.sh
   @define of @loop_L2M_noop.out@
   @define M @15600@
   @define N @28@
   @define df @ -DNO_OP=1@
@ROUT loop_L2N_noop.sh
   @define of @loop_L2N_noop.out@
   @define M @8@
   @define N @280000@
   @define df @ -DNO_OP=1@
@ROUT loop_L2M.sh
   @define of @loop_L2M.out@
   @define M @1560@
   @define N @28@
   @define df @@
@ROUT loop_L2N.sh
   @define of @loop_L2N.out@
   @define M @8@
   @define N @5460@
   @define df @@
@ROUT pftch_OOC.sh
@beginproc massage inf lab
   rm -f tmptmp.out tmptmp.csv
   n=`fgrep -n 'CUT HERE' @(inf) | sed 's/:.*//'`
   head -n $n @(inf) > tmptmp.out
   ./xr1sum2csv -i tmptmp.out -m $imf -D 1 ID -p 1 -o tmptmp.csv
   mfb=`fgrep "BASE MFLOP=" tmptmp.csv | sed 's/BASE MFLOP=//'`
   fgrep mflop tmptmp.csv | sed "s/mflop/@(lab)(${mfb})/" >> pf.csv
@skip   fgrep -A 1 'IVAL (' tmptmp.csv | sed 's/IVAL.*//' | \
@skip      sed "s/mflop/@(lab)(${mfb})/" >> pf.csv
@endproc
#!/bin/sh
#
# arg is size of L2 to pass to total flush
#
# Try all SSE prefetch variants 
#
imf=2
rm -f pf.csv
echo "PFDIST:,,NONE,COL,64,128,256,512,1024,2048,4096" > pf.csv
@whiledef op R NTA T1 T0
./xr1ksearch -T 2 -i R1CASES/mvpf8x1_@(op).idx -2 $1 -o mv8x1pf_@(op).out
@callproc massage mv8x1pf_@(op).out MV_@(op)
@endwhile
echo "" >> pf.csv
@whiledef op W R NTA T1 T0
./xr1ksearch -T 2 -i R1CASES/r1pf8x1_@(op).idx -2 $1 -o r18x1pf_@(op).out
@callproc massage r18x1pf_@(op).out R1_@(op)
@endwhile
@ROUT loop_L2M.sh loop_L2N.sh loop_L2M_noop.sh loop_L2N_noop.sh
#! /bin/sh
rm -f @(of)
echo "START TIMINGS" > @(of)
@whiledef rout touch_1x1_lp touch_1x1_dec_nz touch_1x1_nz touch_1x1_inc_nz
echo "******************** @(rout) ********************" >> @(of)
make dr1ktime dR1CFLAGS="-x assembler-with-cpp@(df)" flushKB=0 \
     tflags=" -F 8000000 -# 8" xu=8 yu=1 M=@(M) N=@(N) r1rout=@(rout).c \
     >> @(of) 2>&1
@endwhile
