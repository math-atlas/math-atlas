@ROUT dmvtcases.idx smvtcases.idx zmvtcases.idx cmvtcases.idx
#
# In this file, any line beginning with a '#' is ignored, but the # must be in
# column 0.  All multiple whitespace is reduced to one space (i.e. used only
# to distinguish where words begin/end).  Lines may be extended by putting '\'
# as the *last* character of line.
#
# The file has the following format:
# ROUT='routine name' AUTH='author names' COMP='compiler name' FLAGS='flags'
# ID=<id> NU=<nu> MU=<mu> minY=<#> minX=<#> alignX=<#> alignY=<#> alignA=<#>,
# SSE=[0,1,2,3] X87=[0,1] PREF[a,x,y]=[DIST,INSTDIST, INST] LDAMUL=<#>
# ALLALIGNXY=[0,1] GEMMBASED=[0,1] CONJDEF=[0,1] FNU=[0,1]
# ASM=[asmlist], eg., asmlist is "GAS_x8664,GAS_x8632" or "GAS_SPARC"
# ASM defaults to no assembly dialect required.
# If NU/MU is negative, then the routine can only handle multiples of NU/MU.
#
# Some less-obvious keywords:
# LDAMUL    : Kernel will only work if lda is a multiple of # (in bytes)
# PFTUNEx   : Kernel uses pref_x(mem) macro for each op=x (A,y,x).  prefetch
#             inst can be varied wt this macro, as can fetch distance.
#             If set to INSTDIST, tune both distance and instruction type;
#             If set to INST, tune instruction type only
#             If set to DIST, tune distance only
# FNU       : if set, kernel can only handle N where N%NU == 0
@ROUT dmvtcases.idx smvtcases.idx
ID=1 TA='T' MU=16 NU=1 AUTH='R. Clint Whaley' ROUT='ATL_gemvT_dot.c'
@ROUT dmvtcases.idx
ID=2 TA='T' MU=2 NU=8 ALIGNX2A=1 alignY=16 minN=8 LDAMUL=16 \
     AUTH='R. Clint Whaley' ROUT='ATL_dgemvT_2x8_sse3.c' \
     COMP='gcc' FLAGS='-fomit-frame-pointer -mfpmath=sse -msse3 -Os'
@ROUT smvtcases.idx
ID=2 TA='T' MU=8 NU=4 ALIGNX2A=1 alignY=16 LDAMUL=16 \
     AUTH='R. Clint Whaley' ROUT='ATL_sgemvT_8x4_sse.c'
@ROUT dmvtcases.idx
ID=3 TA='T' NU=8 MU=6 AUTH='IBM', ROUT='ATL_gemvT_8xv3_vsx.c' \
     COMP='gcc' FLAGS='-O3 -mvsx'
@ROUT smvtcases.idx
ID=3 TA='T' NU=8 MU=12 AUTH='IBM', ROUT='ATL_gemvT_8xv3_vsx.c' \
     COMP='gcc' FLAGS='-O3 -mvsx'
@ROUT cmvtcases.idx zmvtcases.idx
ID=1 TA='T' MU=16 NU=1 AUTH='R. Clint Whaley', ROUT='ATL_cgemvT_dot.c'
@ROUT cmvtcases.idx
ID=2 TA='T' MU=8 NU=4 alignA=8 ALIGNX2A=1 alignY=16 LDAMUL=16 SSE=3 FNU=1 \
     minM=9 minN=4 PFTUNABLE=1 ASM="GAS_x8664" \
     AUTH='R. Clint Whaley' ROUT='ATL_cgemvT_8x4_sse3.c' \
     COMP='gcc' CFLAGS='-x assembler-with-cpp'
ID=3 TA='T' MU=8 NU=4 alignA=32 ALIGNX2A=1 alignY=32 LDAMUL=32 \
     PFTUNABLE=0 ASM="GAS_x8664" \
     AUTH='R. Clint Whaley' ROUT='ATL_cgemvT_8x4_avx.c' \
     COMP='gcc' CFLAGS='-x assembler-with-cpp'
@rout ATL_gemvT_dot ATL_cgemvT_dot
@extract -b @(topd)/cw.inc lang=c -define cwdate 2010
#include "atlas_misc.h"
#include "atlas_level1.h"

void ATL_UGEMV(ATL_CINT M, ATL_CINT N, const TYPE *A, ATL_CINT lda,
               const TYPE *X, TYPE *Y)
/*
 *  y = [0,1]*y + A*x, A is MxN, storing the transpose of the matrix
 */
@rout ATL_gemvT_dot
{
   TYPE y0;
   ATL_INT j;

   for (j=0; j < N; j++, A += lda)
   {
      y0 = Mjoin(PATL,dot)(M, A, 1, X, 1);
      #ifdef BETA0
         *Y++ = y0;
      #else
         *Y++ += y0;
      #endif
   }
}
@rout ATL_cgemvT_dot
{
   TYPE ry, iy;
   ATL_CINT lda2 = lda+lda;
   ATL_INT j;

   for (j=0; j < N; j++, A += lda2, Y += 2)
   {
      #ifdef BETA0
         Mjoin(PATL,dotu_sub)(M, A, 1, X, 1, Y);
      #else
         #ifdef __clang__  /* workaround for clang error */
            TYPE dot[2];
            Mjoin(PATL,dotu_sub)(M, A, 1, X, 1, dot);
            *Y += *dot;
            Y[1] += dot[1];
         #else
            ry = *Y; iy = Y[1];
            Mjoin(PATL,dotu_sub)(M, A, 1, X, 1, Y);
            *Y += ry;
            Y[1] += iy;
         #endif
      #endif
   }
}
@ROUT ATL_dgemvT_2x8_sse3
@beginskip
/*
 * NOTE: kludged this problem by adding -Os to flag.
 * This routine seg faults when run under windows.  Probably a compiler
 * error, but it is too difficult to work on the platform for me to track
 * down further.  Right now, this code won't help there anyway, since it
 * will need 16 xmm regs for decent performance, and Cygwin limits us to use
 * the 32-bit legacy code with support for only 8 registers.
 */
#if defined(ATL_OS_WinNT) || defined(ATL_OS_Win9x) || defined(OSWinSFU)
   #error "Does not compile correctly under windows for some reason!"
#endif
@endskip

#include <xmmintrin.h>
#include "atlas_misc.h"
#define _my_hadd_pd(dst, src) \
   __asm__ __volatile__ ("haddpd %2, %0" : "=x"(dst) : "0" (dst), "x"(src))

void ATL_UGEMV(ATL_CINT M, ATL_CINT N, const TYPE *A, ATL_CINT lda1,
               const TYPE *X, TYPE *Y)
{/* BEGIN GEMV: nMU=1, MU=2, NU=8 */
   ATL_INT i, j;
   ATL_CINT MAp = ((((size_t)A)&0xF) || M==1) ? 1 : 2;
   ATL_CINT MA = M - MAp;
   #define A0 A
   const TYPE *A1=A0+lda1, *A2=A1+lda1, *A3=A2+lda1, *A4=A3+lda1, *A5=A4+lda1, *A6=A5+lda1, *A7=A6+lda1;
   ATL_CINT M2=((((((MA) >> 1)) << 1)))+MAp, N8=(((((N) >> 3)) << 3)), lda8=(((lda1) << 3));
   __m128d x0, x1, y0, y1, y2, y3, y4, y5, y6, y7, a0_0, a0_1, a0_2, a0_3, a0_4, a0_5, a0_6, a0_7;
   if (!M || !N) return;

   for (j=0; j < N8; j += 8, A0 += lda8, A1 += lda8, A2 += lda8, A3 += lda8, A4 += lda8, A5 += lda8, A6 += lda8, A7 += lda8)
   {/* BEGIN N-LOOP UR=8 */
      if (MAp != 1)
      {/* peel to zero Y */
         i=0;
         x0 = _mm_load_pd(X+i+0);
         y0 = _mm_load_pd(A0+i);
         y0 = _mm_mul_pd(y0, x0);
         y1 = _mm_load_pd(A1+i);
         y1 = _mm_mul_pd(y1, x0);
         y2 = _mm_load_pd(A2+i);
         y2 = _mm_mul_pd(y2, x0);
         y3 = _mm_load_pd(A3+i);
         y3 = _mm_mul_pd(y3, x0);
         y4 = _mm_load_pd(A4+i);
         y4 = _mm_mul_pd(y4, x0);
         y5 = _mm_load_pd(A5+i);
         y5 = _mm_mul_pd(y5, x0);
         y6 = _mm_load_pd(A6+i);
         y6 = _mm_mul_pd(y6, x0);
         y7 = _mm_load_pd(A7+i);
         y7 = _mm_mul_pd(y7, x0);
      } /* end zero Y peel */
      else /* if (MAp == 1)*/
      {/* peel to force X/A alignment, zero Y */
         i=0;
         x0 = _mm_load_sd(X+i+0);
         y0 = _mm_load_sd(A0+i);
         y0 = _mm_mul_sd(y0, x0);
         y1 = _mm_load_sd(A1+i);
         y1 = _mm_mul_sd(y1, x0);
         y2 = _mm_load_sd(A2+i);
         y2 = _mm_mul_sd(y2, x0);
         y3 = _mm_load_sd(A3+i);
         y3 = _mm_mul_sd(y3, x0);
         y4 = _mm_load_sd(A4+i);
         y4 = _mm_mul_sd(y4, x0);
         y5 = _mm_load_sd(A5+i);
         y5 = _mm_mul_sd(y5, x0);
         y6 = _mm_load_sd(A6+i);
         y6 = _mm_mul_sd(y6, x0);
         y7 = _mm_load_sd(A7+i);
         y7 = _mm_mul_sd(y7, x0);
      } /* end force-align/zeroY peel */

      for (i=MAp; i < M2; i += 2)
      {/* ----- BEGIN M-LOOP BODY ----- */
         /* --- BEGIN MUxNU UNROLL 0 --- */
         x0 = _mm_load_pd(X+i+0);
         a0_0 = _mm_load_pd(A0+i);
         a0_0 = _mm_mul_pd(a0_0, x0);
         y0 = _mm_add_pd(y0, a0_0);

         a0_1 = _mm_load_pd(A1+i);
         a0_1 = _mm_mul_pd(a0_1, x0);
         y1 = _mm_add_pd(y1, a0_1);

         a0_2 = _mm_load_pd(A2+i);
         a0_2 = _mm_mul_pd(a0_2, x0);
         y2 = _mm_add_pd(y2, a0_2);

         a0_3 = _mm_load_pd(A3+i);
         a0_3 = _mm_mul_pd(a0_3, x0);
         y3 = _mm_add_pd(y3, a0_3);

         a0_4 = _mm_load_pd(A4+i);
         a0_4 = _mm_mul_pd(a0_4, x0);
         y4 = _mm_add_pd(y4, a0_4);

         a0_5 = _mm_load_pd(A5+i);
         a0_5 = _mm_mul_pd(a0_5, x0);
         y5 = _mm_add_pd(y5, a0_5);

         a0_6 = _mm_load_pd(A6+i);
         a0_6 = _mm_mul_pd(a0_6, x0);
         y6 = _mm_add_pd(y6, a0_6);

         a0_7 = _mm_load_pd(A7+i);
         a0_7 = _mm_mul_pd(a0_7, x0);
         y7 = _mm_add_pd(y7, a0_7);

         /* --- END MUxNU UNROLL 0 --- */
      }/* ----- END M-LOOP BODY ----- */
      if (M != M2)
      {/* ----- BEGIN SCALAR M CLEANUP ----- */
         x0 = _mm_load_sd(X+i+0);
         a0_0 = _mm_load_sd(A0+i);
         a0_0 = _mm_mul_sd(a0_0, x0);
         y0 = _mm_add_sd(y0, a0_0);

         a0_1 = _mm_load_sd(A1+i);
         a0_1 = _mm_mul_sd(a0_1, x0);
         y1 = _mm_add_sd(y1, a0_1);

         a0_2 = _mm_load_sd(A2+i);
         a0_2 = _mm_mul_sd(a0_2, x0);
         y2 = _mm_add_sd(y2, a0_2);

         a0_3 = _mm_load_sd(A3+i);
         a0_3 = _mm_mul_sd(a0_3, x0);
         y3 = _mm_add_sd(y3, a0_3);

         a0_4 = _mm_load_sd(A4+i);
         a0_4 = _mm_mul_sd(a0_4, x0);
         y4 = _mm_add_sd(y4, a0_4);

         a0_5 = _mm_load_sd(A5+i);
         a0_5 = _mm_mul_sd(a0_5, x0);
         y5 = _mm_add_sd(y5, a0_5);

         a0_6 = _mm_load_sd(A6+i);
         a0_6 = _mm_mul_sd(a0_6, x0);
         y6 = _mm_add_sd(y6, a0_6);

         a0_7 = _mm_load_sd(A7+i);
         a0_7 = _mm_mul_sd(a0_7, x0);
         y7 = _mm_add_sd(y7, a0_7);

      }/* ----- END SCALAR M CLEANUP ----- */
      _my_hadd_pd(y0, y1);
      #ifndef BETA0
         a0_0 = _mm_load_pd(Y+j+0);
         y0 = _mm_add_pd(y0, a0_0);
      #endif
      _mm_store_pd(Y+j+0, y0);
      _my_hadd_pd(y2, y3);
      #ifndef BETA0
         a0_1 = _mm_load_pd(Y+j+2);
         y2 = _mm_add_pd(y2, a0_1);
      #endif
      _mm_store_pd(Y+j+2, y2);
      _my_hadd_pd(y4, y5);
      #ifndef BETA0
         a0_2 = _mm_load_pd(Y+j+4);
         y4 = _mm_add_pd(y4, a0_2);
      #endif
      _mm_store_pd(Y+j+4, y4);
      _my_hadd_pd(y6, y7);
      #ifndef BETA0
         a0_3 = _mm_load_pd(Y+j+6);
         y6 = _mm_add_pd(y6, a0_3);
      #endif
      _mm_store_pd(Y+j+6, y6);
   }/* END N-LOOP UR=8 */

   for (j=N8; j < N; j++, A0 += lda1)
   {/* BEGIN N-LOOP UR=1 */
      if (MAp != 1)
      {/* peel to zero Y */
         i=0;
         x0 = _mm_load_pd(X+i+0);
         y0 = _mm_load_pd(A0+i);
         y0 = _mm_mul_pd(y0, x0);
      } /* end zero Y peel */
      else /* if (MAp == 1)*/
      {/* peel to force X/A alignment, zero Y */
         i=0;
         x0 = _mm_load_sd(X+i+0);
         y0 = _mm_load_sd(A0+i);
         y0 = _mm_mul_sd(y0, x0);
      } /* end force-align/zeroY peel */

      for (i=MAp; i < M2; i += 2)
      {/* ----- BEGIN M-LOOP BODY ----- */
         /* --- BEGIN MUxNU UNROLL 0 --- */
         x0 = _mm_load_pd(X+i+0);
         a0_0 = _mm_load_pd(A0+i);
         a0_0 = _mm_mul_pd(a0_0, x0);
         y0 = _mm_add_pd(y0, a0_0);

         /* --- END MUxNU UNROLL 0 --- */
      }/* ----- END M-LOOP BODY ----- */
      if (M != M2)
      {/* ----- BEGIN SCALAR M CLEANUP ----- */
         x0 = _mm_load_sd(X+i+0);
         a0_0 = _mm_load_sd(A0+i);
         a0_0 = _mm_mul_sd(a0_0, x0);
         y0 = _mm_add_sd(y0, a0_0);

      }/* ----- END SCALAR M CLEANUP ----- */
      _my_hadd_pd(y0, y0);
      #ifndef BETA0
         a0_0 = _mm_load_sd(Y+j+0);
         y0 = _mm_add_sd(y0, a0_0);
      #endif
      _mm_store_sd(Y+j+0, y0);
   }/* END N-LOOP UR=1 */
}/* END GEMV: nMU=1, MU=2, NU=8 */
#ifdef MA
   #undef MA
#endif
#ifdef MAp
   #undef MAp
#endif
#ifdef A0
   #undef A0
#endif
@ROUT ATL_sgemvT_8x4_sse
@extract -b @(topd)/cw.inc lang=c -define cwdate 2010

#include <xmmintrin.h>
#include <stdio.h>
#include "atlas_misc.h"
#define _my_hadd_ps(dst, src) \
   __asm__ __volatile__ ("haddps %2, %0" : "=x"(dst) : "0" (dst), "x"(src))


void ATL_UGEMV(ATL_CINT M, ATL_CINT N, const TYPE *A, ATL_CINT lda1,
               const TYPE *X, TYPE *Y)
{/* BEGIN GEMVN: nMU=1, MU=8, NU=4 */
   ATL_INT i, j;
   ATL_CINT MAp = (M > 11) ?
       ( (((((size_t)A)+15)>>4)<<4) - ((size_t)A) )/sizeof(TYPE) : M;
   ATL_CINT MA=M-MAp;
   ATL_CINT M8=((MA/8)*8)+MAp, N4=((N/4)*4), lda2=lda1+lda1, lda3=lda2+lda1, lda4=lda3+lda1;
   __m128 y0, y1, y2, y3, x0, x4; 
   __m128 a0_0, a4_0, a0_1, a4_1, a0_2, a4_2, a0_3, a4_3;

   for (j=0; j < N4; j += 4, A += lda4, Y += 4)
   {/* BEGIN N-LOOP UR=4 */
      if (MAp)
      {
         i=0;
         x0 = _mm_load_ss(X+i);
         y0 =_mm_load_ss(A+i);
         y0 = _mm_mul_ss(y0, x0);
         y1 =_mm_load_ss(A+i+lda1);
         y1 = _mm_mul_ss(y1, x0);
         y2 =_mm_load_ss(A+i+lda2);
         y2 = _mm_mul_ss(y2, x0);
         y3 =_mm_load_ss(A+i+lda3);
         y3 = _mm_mul_ss(y3, x0);
         for (i=1; i < MAp; i++)
         {/* peel to force X/A alignment */
            x0 = _mm_load_ss(X+i);
            a0_0 =_mm_load_ss(A+i);
            a0_0 = _mm_mul_ss(a0_0, x0);
            y0 = _mm_add_ss(y0, a0_0);
            a0_1 =_mm_load_ss(A+i+lda1);
            a0_1 = _mm_mul_ss(a0_1, x0);
            y1 = _mm_add_ss(y1, a0_1);
            a0_2 =_mm_load_ss(A+i+lda2);
            a0_2 = _mm_mul_ss(a0_2, x0);
            y2 = _mm_add_ss(y2, a0_2);
            a0_3 =_mm_load_ss(A+i+lda3);
            a0_3 = _mm_mul_ss(a0_3, x0);
            y3 = _mm_add_ss(y3, a0_3);
         } /* end force-align peel */
      }
      else
      {
         y0 = _mm_xor_ps(y0, y0);
         y1 = _mm_xor_ps(y1, y1);
         y2 = _mm_xor_ps(y2, y2);
         y3 = _mm_xor_ps(y3, y3);
      }
      for (i=MAp; i < M8; i += 8)
      {/* ----- BEGIN M-LOOP BODY ----- */
         x0 = _mm_load_ps(X+i);
         a0_0 =_mm_load_ps(A+i);
         a0_0 = _mm_mul_ps(a0_0, x0);
         y0 = _mm_add_ps(y0, a0_0);
         a0_1 =_mm_load_ps(A+lda1+i);
         a0_1 = _mm_mul_ps(a0_1, x0);
         y1 = _mm_add_ps(y1, a0_1);
         a0_2 =_mm_load_ps(A+lda2+i);
         a0_2 = _mm_mul_ps(a0_2, x0);
         y2 = _mm_add_ps(y2, a0_2);
         a0_3 =_mm_load_ps(A+lda3+i);
         a0_3 = _mm_mul_ps(a0_3, x0);
         y3 = _mm_add_ps(y3, a0_3);

         x4 = _mm_load_ps(X+i+4);
         a4_0 =_mm_load_ps(A+i+4);
         a4_0 = _mm_mul_ps(a4_0, x4);
         y0 = _mm_add_ps(y0, a4_0);
         a4_1 =_mm_load_ps(A+lda1+i+4);
         a4_1 = _mm_mul_ps(a4_1, x4);
         y1 = _mm_add_ps(y1, a4_1);
         a4_2 =_mm_load_ps(A+lda2+i+4);
         a4_2 = _mm_mul_ps(a4_2, x4);
         y2 = _mm_add_ps(y2, a4_2);
         a4_3 =_mm_load_ps(A+lda3+i+4);
         a4_3 = _mm_mul_ps(a4_3, x4);
         y3 = _mm_add_ps(y3, a4_3);
      }/* ----- END M-LOOP BODY ----- */
      for (i=M8; i < M; i++)
      {/* ----- BEGIN SCALAR M CLEANUP ----- */
         x0 = _mm_load_ss(X+i);
         a0_0 =_mm_load_ss(A+i);
         a0_0 = _mm_mul_ss(a0_0, x0);
         y0 = _mm_add_ss(y0, a0_0);
         a0_1 =_mm_load_ss(A+i+lda1);
         a0_1 = _mm_mul_ss(a0_1, x0);
         y1 = _mm_add_ss(y1, a0_1);
         a0_2 =_mm_load_ss(A+i+lda2);
         a0_2 = _mm_mul_ss(a0_2, x0);
         y2 = _mm_add_ss(y2, a0_2);
         a0_3 =_mm_load_ss(A+i+lda3);
         a0_3 = _mm_mul_ss(a0_3, x0);
         y3 = _mm_add_ss(y3, a0_3);
      }/* ----- END SCALAR M CLEANUP ----- */
                            /* y3 = {y3d, y3c, y3b, y3a} */
                            /* y2 = {y2d, y2c, y2b, y2a} */
                            /* y1 = {y1d, y1c, y1b, y1a} */
                            /* y0 = {y0d, y0c, y0b, y0a} */
      _my_hadd_ps(y0, y1);  /* y0 = {y1d+y1c, y1b+y1a, y0d+y0c, y0b+y0a} */
      _my_hadd_ps(y2, y3);  /* y2 = {y3d+y3c, y3b+y3a, y2d+y2c, y2b+y2a} */
      _my_hadd_ps(y0, y2);  /* y0 = {y3abcd, y2abcd, y1abcd, y0abcd} */
      #ifndef BETA0
         a0_0 = _mm_load_ps(Y);
         y0 = _mm_add_ps(y0, a0_0);
      #endif
      _mm_store_ps(Y, y0);
   }/* END N-LOOP UR=4 */

   for (j=N4; j < N; j++, A += lda1, Y++)
   {/* BEGIN N-LOOP UR=1 */
      y0 = _mm_xor_ps(y0, y0);
      y1 = _mm_xor_ps(y1, y1);
      for (i=0; i < MAp; i++)
      {/* peel to force X/A alignment */
         x0 = _mm_load_ss(X+i);
         a0_0 =_mm_load_ss(A+i);
         a0_0 = _mm_mul_ss(a0_0, x0);
         y0 = _mm_add_ss(y0, a0_0);
      } /* end force-align peel */

      for (i=MAp; i < M8; i += 8)
      {/* ----- BEGIN M-LOOP BODY ----- */
         x0 = _mm_load_ps(X+i);
         a0_0 =_mm_load_ps(A+i);
         a0_0 = _mm_mul_ps(a0_0, x0);
         y0 = _mm_add_ps(y0, a0_0);
         x4 = _mm_load_ps(X+i+4);
         a4_0 =_mm_load_ps(A+i+4);
         a4_0 = _mm_mul_ps(a4_0, x4);
         y1 = _mm_add_ps(y1, a4_0);
      }/* ----- END M-LOOP BODY ----- */
      for (i=M8; i < M; i++)
      {/* ----- BEGIN SCALAR M CLEANUP ----- */
         x0 = _mm_load_ss(X+i);
         a0_0 =_mm_load_ss(A+i);
         a0_0 = _mm_mul_ss(a0_0, x0);
         y0 = _mm_add_ss(y0, a0_0);
      }/* ----- END SCALAR M CLEANUP ----- */
                            /* y1 = {y0h, y0g, y0f, y0e} */
                            /* y0 = {y0d, y0c, y0b, y0a} */
      y0 = _mm_add_ps(y0, y1);
      _my_hadd_ps(y0, y0);
      _my_hadd_ps(y0, y0);
      #ifndef BETA0
         a0_0 = _mm_load_ss(Y);
         y0 = _mm_add_ss(y0, a0_0);
      #endif
      _mm_store_ss(Y, y0);
   }/* END N-LOOP UR=1 */
}/* END GER: nMU=1, MU=8, NU=4 */
#ifdef MA
   #undef MA
#endif
#ifdef MAp
   #undef MAp
#endif
@ROUT ATL_cgemvT_8x4_avx
@extract -b @(topd)/cw.inc lang=c -define cwdate 2011
#include "atlas_asm.h"
#ifndef ATL_GAS_x8664
   #error "This kernel requires x86-64 assembly!"
#endif
#define M       %rdi
#define M_w      %di
#define N       %rsi
#define N_w     %si
#define N_l     %esi
#define pA      %rdx
#define lda     %rcx
#define pX      %r8
#define pY      %r9
#define pA1     %rbx
#define pA2     %r11
#define Mr      %rax
#define pA3     %r10
#define FLAGS   %r12 /* bitfield: 0:set if M%8 >= 4;  1:set if Mr is non-zero */
#define Nr      %r14
//#define incII   %r15
#define M0      %r13
/*
 * SSE register assignment
 */
#define rA0     %ymm0
#define rX0     %ymm1
#define rx0     %ymm2
#define rt0     %ymm3
#define rY0r    %ymm4
#define rY0i    %ymm5
#define rY1r    %ymm6
#define rY1i    %ymm7
#define rY2r    %ymm8
#define rY2i    %ymm9
#define rY3r    %ymm10
#define rY3i    %ymm11
// #define rX1     %xmm12
#define rMSK4   %ymm13   /* all ones if N%8 >= 4 */
#define rMASK   %ymm14   /* each word says if N%4 includes that word */
#define rNP1    %ymm15

#define rA0_    %xmm0
#define rX0_    %xmm1
#define rx0_    %xmm2
#define rt0_    %xmm3
#define rY0r_   %xmm4
#define rY0i_   %xmm5
#define rY1r_   %xmm6
#define rY1i_   %xmm7
#define rY2r_   %xmm8
#define rY2i_   %xmm9
#define rY3r_   %xmm10
#define rY3i_   %xmm11
#define rNP1_   %xmm15
/*
 *                      %rdi        %rsi           %rdx          %rcx
 * void ATL_UGEMV(ATL_CINT M, ATL_CINT N, const TYPE *A, ATL_CINT lda,
 *                          %r8      %r9
 *                const TYPE *X, TYPE *Y)
 */
.text
.global ATL_asmdecor(ATL_UGEMV)
ALIGN64
ATL_asmdecor(ATL_UGEMV):
/*
 * Construct {-1,1,-1,1} in memory, and duplicate into rNP1
 */
   mov $0xBF800000, %eax        /* IEEE -1.0 */
   mov %eax, -12(%rsp)
   mov %eax, -4(%rsp)   
   shl $1, %eax
   shr $1, %eax                 /* IEEE 1.0 */
   mov %eax, -16(%rsp)
   mov %eax, -8(%rsp)   
   vbroadcastf128 -16(%rsp), rNP1   
      /* rNP1 = {-1.0,+1.0,-1.0,+1.0,-1.0,+1.0-1.0,+1.0} */
/*
 * Save callee-saved iregs
 */
   movq %rbp, -8(%rsp)
   movq %rbx, -16(%rsp)
   movq %r12, -24(%rsp)
   movq %r13, -32(%rsp)
   movq %r14, -40(%rsp)
   movq %r15, -48(%rsp)

   movslq %edi, %rdi
   movslq %esi, %rsi
   movslq %ecx, %rcx
   mov M, Mr
   andw $0xFFF8, M_w            /* M is now a multiple of 8 */
   sub M, Mr                    /* Mr = M % 8 */
/*
 * 
 */
   xor FLAGS, FLAGS             /* assume M%8 = 0 */
   vxorps rMSK4, rMSK4, rMSK4   /* all-zeros: assuming M%8 < 4 */
   vxorps rMASK, rMASK, rMASK   /* all-zeros: assuming no cleanup */
/*
 * If (Mr >= 4) rMSK4={all 1s}; Mr -= 4
 */
   bt $2, Mr                    /* CF=1 if Mr >= 4 */
   jnc DONE_MSK4
      mov $1, FLAGS             /* will peel first 4 cplx numbers */
      sub $4, Mr
      mov $0xFFFFFFFF, %ebx
      movl %ebx, -56(%rsp)
      vbroadcastss -56(%rsp), rMSK4     /* rMSK4 = all 1s -> peel 4 its */
      add $32, pA                       /* will be peeled */
      add $32, pX                       /* will be peeled */
   DONE_MSK4:                           /* Mr known <= 3 */
   cmp $0, Mr
   je DONE_MASK
/*
 * Construct rMASK based on Mr, rMASK starts out all-zeros (no cleanup)
 * If we reach here, there is some cleanup to do (1 <= Mr <= 3)
 */
   mov $0xFFFFFFFF, %ebx
   movl %ebx, -56(%rsp)
   vbroadcastss -56(%rsp), rt0   /* rt0 = all 1s */
   add $2, FLAGS
/*
 * If Mr == 1, have first two words (imag,real) all 1s, others 0 (from MASK)
 */
   cmp $1, Mr
   jne DONEM1
      vblendps $0x03, rt0, rMASK, rMASK   /* 0b0000 0011 */
      jmp DONE_MASK
   DONEM1:
/*
 * If Mr == 2, have first 4 words all 1s, others 0 (from MASK)
 */
   cmp $2, Mr
   jne DONEM2
      vblendps $0x0F, rt0, rMASK, rMASK   /* 0b0000 1111 */
      jmp DONE_MASK
   DONEM2:
/*
 * If we reach here, Mr == 3
 */
   vblendps $0x3F, rt0, rMASK, rMASK   /* 0b0011 1111 */

DONE_MASK:
   shl $3, lda          /* lda *= sizeof */
/*
 * Jump to special case code if M < 8
 */
   cmp $8, M
   jl MLT8
/*
 * Otherwise, set up for normal NU=4, Mu=8 unrolled loop
 */
   lea (pA,M,8), pA     /* A += M */
   lea (pX,M,8), pX     /* X += M */
   lea (pA,lda), pA1
   lea (pA,lda,2), pA2
   lea (pA1,lda,2), pA3
   shl $3, M            /* M *= sizeof */
   neg M
   mov M, M0
   mov N, Nr
   andl $0xFFFFFFFC, N_l        /* N is now a multiple of 4 */
   jz LOOPN1
   sub N, Nr                    /* Nr = N % 4 */
   LOOPN4:
      bt $0, FLAGS      /* CF=1 if Mr >= 4 */
      jc PEEL4
      vxorps rY0r, rY0r, rY0r
      vxorps rY0i, rY0i, rY0i
      vxorps rY1r, rY1r, rY1r
      vxorps rY1i, rY1i, rY1i
      vxorps rY2r, rY2r, rY2r
      vxorps rY2i, rY2i, rY2i
      vxorps rY3r, rY3r, rY3r
      vxorps rY3i, rY3i, rY3i
      LOOPM8:
         vmovaps (pX,M), rX0   
            /* rX0 = {x3i, x3r, x2i, x2r, x1i, x1r, x0i, x0r} */
         vshufps $0xB1, rX0, rX0, rx0   
            /* rx0 = {x3r, x3i, x2r, x2i, x1r, x1i, x0r, x0i} */
         vmovaps (pA,M), rA0             
            /* rA0 = {a3i, a3r, a2i, a2r, a1i, a1r, a0i, a0r} */
         vmulps rX0, rA0, rt0           
            /* rt0 = {x3i*a3i,x3r*a3r,...,  x0i*a0i, x0r*a0r} */
         vaddps  rt0, rY0r, rY0r               
            /* rY0r= {s3ii, s3rr, s2ii, s2rr, s1ii, s1rr, s0ii, s0rr} */
            prefetchnta 512(pA,M)
         vmulps rx0, rA0, rt0           
            /* rt0 = {x3r*a3i, x3i*a3r, ..., x0r*a0i, x0i*a0r} */
         vaddps  rt0, rY0i, rY0i               
            /* rY0i= {s3ri, s3ir, s2ri, s2ir, s1ri, s1ir, s0ri, s0ir} */

         vmovaps (pA1,M), rA0             
         vmulps rX0, rA0, rt0           
         vaddps  rt0, rY1r, rY1r
            prefetchnta 448(pA1,M)
         vmulps rx0, rA0, rt0           
         vaddps  rt0, rY1i, rY1i               

         vmovaps (pA2,M), rA0
         vmulps rX0, rA0, rt0           
         vaddps  rt0, rY2r, rY2r
            prefetchnta 448(pA2,M)
         vmulps rx0, rA0, rt0           
         vaddps  rt0, rY2i, rY2i               

         vmovaps (pA3,M), rA0
         vmulps rX0, rA0, rt0           
         vaddps  rt0, rY3r, rY3r
            prefetchnta 448(pA3,M)
         vmulps rx0, rA0, rt0           
         vaddps  rt0, rY3i, rY3i               

         vmovaps 32(pX,M), rX0   
            /* rX0 = {x3i, x3r, x2i, x2r, x1i, x1r, x0i, x0r} */
         vshufps $0xB1, rX0, rX0, rx0   
            /* rx0 = {x3r, x3i, x2r, x2i, x1r, x1i, x0r, x0i} */
         vmovaps 32(pA,M), rA0             
            /* rA0 = {a3i, a3r, a2i, a2r, a1i, a1r, a0i, a0r} */
         vmulps rX0, rA0, rt0           
            /* rt0 = {x3i*a3i,x3r*a3r,...,  x0i*a0i, x0r*a0r} */
         vaddps  rt0, rY0r, rY0r               
            /* rY0r= {s3ii, s3rr, s2ii, s2rr, s1ii, s1rr, s0ii, s0rr} */
         vmulps rx0, rA0, rt0           
            /* rt0 = {x3r*a3i, x3i*a3r, ..., x0r*a0i, x0i*a0r} */
         vaddps  rt0, rY0i, rY0i               
            /* rY0i= {s3ri, s3ir, s2ri, s2ir, s1ri, s1ir, s0ri, s0ir} */

         vmovaps 32(pA1,M), rA0             
         vmulps rX0, rA0, rt0           
         vaddps  rt0, rY1r, rY1r               
         vmulps rx0, rA0, rt0           
         vaddps  rt0, rY1i, rY1i               
            prefetcht0 256(pX,M)

         vmovaps 32(pA2,M), rA0             
         vmulps rX0, rA0, rt0           
         vaddps  rt0, rY2r, rY2r               
         vmulps rx0, rA0, rt0           
         vaddps  rt0, rY2i, rY2i               

         vmovaps 32(pA3,M), rA0             
         vmulps rX0, rA0, rt0           
         vaddps  rt0, rY3r, rY3r               
         vmulps rx0, rA0, rt0           
         vaddps  rt0, rY3i, rY3i               

         add $8*8, M
      jnz LOOPM8
/* 
 *    If we have a M%4 remainder, jump to code that will handle it
 */
      bt $1, FLAGS
      jc CLEANN4_M
      DONE_MCU:  /* jump back to here if we left loop for M cleanup */
/*
 *    Sum up all these vecs
 */
      lea (pA,lda,4), pA
      vmulps rNP1, rY0r, rY0r   /* negate all imag*imag entries */
      vhaddps rY0i, rY0r, rY0r  
         /* rY0r = {s0iD,s0iC,s0rD,s0rC,s0iB,s0iA,s0rB,s0rA} */
      lea (pA1,lda,4), pA1
      vmulps rNP1, rY1r, rY1r   /* negate all imag*imag entries */
      vhaddps rY1i, rY1r, rY1r  /* same for 0 but for Y[1] */
      prefetchnta (pA,M0)
      lea (pA2,lda,4), pA2
      vmulps rNP1, rY2r, rY2r   /* negate all imag*imag entries */
      vhaddps rY2i, rY2r, rY2r  /* same as for 0 but for Y[2] */
      lea (pA3,lda,4), pA3
      prefetchnta (pA1,M0)
      vmulps rNP1, rY3r, rY3r   /* negate all imag*imag entries */
      vhaddps rY3i, rY3r, rY3r  /* same as for 0 but for Y[3] */

      prefetchnta (pA2,M0)
      vhaddps rY1r, rY0r, rY0r
           /* rY0r = {s1iCD,s1rCD,s0iCD,s0rCD,s1iAB,s1rAB,s0iAB,s0rAB} */
      prefetchnta (pA3,M0)
      vhaddps rY3r, rY2r, rY2r
           /* rY2r = {s3iCD,s3rCD,s2iCD,s2rCD,s3iAB,s3rAB,s2iAB,s2rAB} */
      prefetchnta 64(pA,M0)
      vperm2f128 $0x20, rY2r, rY0r, rY0i
           /* rY0i = {s3iAB,s3rAB,s2iAB,s2rAB,s1iAB,s1rAB,s0iAB,s0rAB} */
      prefetchnta 64(pA1,M0)
      vperm2f128 $0x31, rY2r, rY0r, rY2i
           /* rY2i = {s3iCD,s3rCD,s2iCD,s2rCD,s1iCD,s1rCD,s0iCD,s0rCD} */
      vaddps rY2i, rY0i, rY0r /* rY0r= {s3i,s3r,s2i,s2r,s1i,s1r,s0i,s0r} */
      prefetchnta 64(pA2,M0)
      mov M0, M
      vaddps (pY), rY0r, rY0r
      vmovaps rY0r, (pY)
      add $32, pY
      prefetchnta 64(pA3,M0)

      sub $4, N
   jnz LOOPN4
/*
 * Do N-loop cleanup if necessary
 */
   cmp $0, Nr
   jne LOOPN1

/*
 * EPILOGUE: restore registers and return
 */
DONE:
   movq -8(%rsp), %rbp
   movq -16(%rsp), %rbx
   movq -24(%rsp), %r12
   movq -32(%rsp), %r13
   movq -40(%rsp), %r14
   movq -48(%rsp), %r15
   ret
/*
 *    Peel first 4 iterations if M%8 >= 4; if M%8 < 4, then this will
 *    zero the Y vectors
 */
PEEL4:
   vmovaps -32(pX,M), rX0   /* rX0 = {x3i, x3r, x2i, x2r, x1i, x1r, x0i, x0r} */
   vshufps $0xB1, rX0, rX0, rx0   
      /* rx0 = {x3r, x3i, x2r, x2i, x1r, x1i, x0r, x0i} */
   vmovaps -32(pA,M), rA0 /* rA0 = {a3i, a3r, a2i, a2r, a1i, a1r, a0i, a0r} */
   vmulps rX0, rA0, rY0r
   vmulps rx0, rA0, rY0i

   vmovaps -32(pA1,M), rA0
   vmulps rX0, rA0, rY1r
   vmulps rx0, rA0, rY1i

   vmovaps -32(pA2,M), rA0
   vmulps rX0, rA0, rY2r
   vmulps rx0, rA0, rY2i

   vmovaps -32(pA3,M), rA0
   vmulps rX0, rA0, rY3r
   vmulps rx0, rA0, rY3i

   jmp LOOPM8
/*
 *    Handles M%4 component for LOOPN4.
 */
CLEANN4_M:
   vmaskmovps (pX), rMASK, rX0   
      /* rX0 = {x3i, x3r, x2i, x2r, x1i, x1r, x0i, x0r} */
   vshufps $0xB1, rX0, rX0, rx0   
      /* rx0 = {x3r, x3i, x2r, x2i, x1r, x1i, x0r, x0i} */
   vmaskmovps (pA), rMASK, rA0             
      /* rA0 = {a3i, a3r, a2i, a2r, a1i, a1r, a0i, a0r} */
   vmulps rX0, rA0, rt0           
      /* rt0 = {x3i*a3i,x3r*a3r,...,  x0i*a0i, x0r*a0r} */
   vaddps  rt0, rY0r, rY0r               
      /* rY0r= {s3ii, s3rr, s2ii, s2rr, s1ii, s1rr, s0ii, s0rr} */
   vmulps rx0, rA0, rt0           
      /* rt0 = {x3r*a3i, x3i*a3r, ..., x0r*a0i, x0i*a0r} */
   vaddps  rt0, rY0i, rY0i               
      /* rY0i= {s3ri, s3ir, s2ri, s2ir, s1ri, s1ir, s0ri, s0ir} */
   vmaskmovps (pA1), rMASK, rA0             
   vmulps rX0, rA0, rt0           
   vaddps  rt0, rY1r, rY1r               
   vmulps rx0, rA0, rt0           
   vaddps  rt0, rY1i, rY1i               
   vmaskmovps (pA2), rMASK, rA0             
   vmulps rX0, rA0, rt0           
   vaddps  rt0, rY2r, rY2r               
   vmulps rx0, rA0, rt0           
   vaddps  rt0, rY2i, rY2i               
   vmaskmovps (pA3), rMASK, rA0             
   vmulps rX0, rA0, rt0           
   vaddps  rt0, rY3r, rY3r               
   vmulps rx0, rA0, rt0           
   vaddps  rt0, rY3i, rY3i               
   jmp DONE_MCU

/*
 * this case can handle any N value, as long as M >= 4
 */
LOOPN1:  /* rolled loop for N cleanup */
   mov M0, M
   bt $0, FLAGS      /* CF=1 if Mr >= 4 */
   jc PEEL4_N1
   vxorps rY0r, rY0r, rY0r
   vxorps rY0i, rY0i, rY0i
   LOOPM4_N1:
      vmovaps (pX,M), rX0   /* rX0 = {x3i, x3r, x2i, x2r, x1i, x1r, x0i, x0r} */
      vshufps $0xB1, rX0, rX0, rx0/* rx0 = {x3r,x3i,x2r,x2i,x1r,x1i,x0r,x0i} */
      vmovaps (pA,M), rA0   /* rA0 = {a3i, a3r, a2i, a2r, a1i, a1r, a0i, a0r} */
      vmulps rX0, rA0, rt0  /* rt0 = {x3i*a3i,x3r*a3r,...,  x0i*a0i, x0r*a0r} */
      vaddps  rt0, rY0r, rY0r               
         /* rY0r= {s3ii, s3rr, s2ii, s2rr, s1ii, s1rr, s0ii, s0rr} */
      vmulps rx0, rA0, rt0           
         /* rt0 = {x3r*a3i, x3i*a3r, ..., x0r*a0i, x0i*a0r} */
         prefetchnta 512(pA,M)
      vaddps  rt0, rY0i, rY0i               
         /* rY0i= {s3ri, s3ir, s2ri, s2ir, s1ri, s1ir, s0ri, s0ir} */
      add $4*8, M
   jnz LOOPM4_N1
/*
 * If M%4 > 0, handle it
 */
   bt $1, FLAGS
   jnc DONE_N1MCU
      vmaskmovps (pX,M), rMASK, rX0   
         /* rX0 = {x3i, x3r, x2i, x2r, x1i, x1r, x0i, x0r} */
      vshufps $0xB1, rX0, rX0, rx0   
         /* rx0 = {x3r, x3i, x2r, x2i, x1r, x1i, x0r, x0i} */
      vmaskmovps (pA,M), rMASK, rA0             
         /* rA0 = {a3i, a3r, a2i, a2r, a1i, a1r, a0i, a0r} */
      vmulps rX0, rA0, rt0           
         /* rt0 = {x3i*a3i,x3r*a3r,...,  x0i*a0i, x0r*a0r} */
      vaddps  rt0, rY0r, rY0r               
         /* rY0r= {s3ii, s3rr, s2ii, s2rr, s1ii, s1rr, s0ii, s0rr} */
      vmulps rx0, rA0, rt0           
         /* rt0 = {x3r*a3i, x3i*a3r, ..., x0r*a0i, x0i*a0r} */
      vaddps  rt0, rY0i, rY0i               
         /* rY0i= {s3ri, s3ir, s2ri, s2ir, s1ri, s1ir, s0ri, s0ir} */
   DONE_N1MCU:
/*
 * sum up results
 */
   movlps (pY), rA0_
   vmulps rNP1, rY0r, rY0r   /* negate all imag*imag entries */
   vhaddps rY0i, rY0r, rY0r  /* rY0r = {s3i,s2i,s3r,s2r,s1i,s0i,s1r,s0r} */
   vextractf128 $1,rY0r,rY0i_/* rY0i = {XXX,XXX,XXX,XXX,s3i,s2i,s3r,s2r} */
   addps rY0i_, rY0r_        /* rY0r = {s31i,s20i,s31r,s20r} */
   haddps rY0r_, rY0r_       /* rY0r = {s0-3i,s0-3r,s0-3i,s0-3r} */
   addps rA0_, rY0r_
   movlps rY0r_, (pY)

   lea (pA,lda), pA
   add $8, pY
   sub $1, Nr
jnz LOOPN1
jmp DONE
/*
 * Handle peel of first iteration if M%8 >= 4
 */
PEEL4_N1:
   vmovaps -32(pX,M), rX0   /* rX0 = {x3i, x3r, x2i, x2r, x1i, x1r, x0i, x0r} */
   vshufps $0xB1, rX0, rX0, rx0/* rx0 = {x3r,x3i,x2r,x2i,x1r,x1i,x0r,x0i} */
   vmovaps -32(pA,M), rA0   /* rA0 = {a3i, a3r, a2i, a2r, a1i, a1r, a0i, a0r} */
   vmulps rX0, rA0, rY0r /* rt0 = {x3i*a3i,x3r*a3r,...,  x0i*a0i, x0r*a0r} */
   vmulps rx0, rA0, rY0i          
      /* rt0 = {x3r*a3i, x3i*a3r, ..., x0r*a0i, x0i*a0r} */
      prefetchnta 512(pA,M)
   jmp LOOPM4_N1

/*
 * Special case code for M < 8; start by loading loop-invariant X
 */
#define rX1 rY3i
#define rx1 rY3r
#define ry0 rY2r_
MLT8:
   xorps ry0, ry0
   bt $0, FLAGS      /* CF=1 if Mr >= 4 */
   jnc MLT4

   vmovaps -32(pX), rX0   
      /* rX0 = {x3i, x3r, x2i, x2r, x1i, x1r, x0i, x0r} */
   vshufps $0xB1, rX0, rX0, rx0   
      /* rx0 = {x3r, x3i, x2r, x2i, x1r, x1i, x0r, x0i} */
   vmulps rNP1, rX0, rX0
      /* rX0 ={-x3i, x3r,-x2i, x2r,-x1i, x1r,-x0i, x0r} */
   vmaskmovps (pX), rMASK, rX1   
      /* rX1 = {x7i, x7r, x6i, x6r, x5i, x5r, x4i, x4r} */
   vshufps $0xB1, rX1, rX1, rx1   
      /* rx1 = {x3r, x3i, x2r, x2i, x1r, x1i, x0r, x0i} */
   vmulps rNP1, rX1, rX1
      /* rX1 ={-x7i, x7r,-x6i, x6r,-x5i, x5r,-x4i, x4r} */
LOOPN1_MLE8:
   vmovaps -32(pA), rA0 
      /* rA0 = {a3i, a3r, a2i, a2r, a1i, a1r, a0i, a0r} */
   vmulps rX0, rA0, rY0r
      movlps (pY), ry0
   vmulps rx0, rA0, rY0i
   vmaskmovps 0(pA), rMASK, rA0 
      /* rA0 = {a7i, a7r, a6i, a6r, a5i, a5r, a4i, a4r} */
   vmulps rX1, rA0, rt0
   vaddps rt0, rY0r, rY0r
      /* rY0r = {yr7, yr6, yr5, yr4, yr3, yr2, yr1, yr0} */
   vmulps rx1, rA0, rt0
   vaddps rt0, rY0i, rY0i
      prefetcht0  (pA,lda,8)
      /* rY0i = {yi7, yi6, yi5, yi4, yi3, yi2, yi1, yi0} */
   vhaddps rY0i, rY0r, rY0r
      /* rY0r = {yi6-7,yi4-5,yr6-7,yr4-5,yi2-3,yi0-1,yr2-3,yr0-1} */
   vextractf128 $1, rY0r, rY0i_ /* y0i_={yi6-7,yi4-5,yr6-7,yr4-5} */
      add lda, pA
   addps rY0i_, rY0r_           /* y0r ={yi2367,yi0145,yr2367,yr0145 */
      add $8, pY
   haddps rY0r_, rY0r_          /* y0r ={yi0-7,y0-7,yi0-7,yr0-7} */
   addps rY0r_, ry0
   movlps ry0, -8(pY)
   sub $1, N
jne LOOPN1_MLE8
jmp DONE
/*
 * Special case code for M < 3
 */
MLT4:
   vmaskmovps (pX), rMASK, rX0
   vshufps $0xB1, rX0, rX0, rx0   
      /* rx0 = {x3r, x3i, x2r, x2i, x1r, x1i, x0r, x0i} */
   vmulps rNP1, rX0, rX0
      /* rX0 ={-x3i, x3r,-x2i, x2r,-x1i, x1r,-x0i, x0r} */

LOOPN1_MLT4:
   vmaskmovps (pA), rMASK, rA0 
      /* rA0 = {a3i, a3r, a2i, a2r, a1i, a1r, a0i, a0r} */
   vmulps rX0, rA0, rY0r
      movlps (pY), ry0
   vmulps rx0, rA0, rY0i
   vhaddps rY0i, rY0r, rY0r
      /* rY0r = {yi6-7,yi4-5,yr6-7,yr4-5,yi2-3,yi0-1,yr2-3,yr0-1} */
      prefetcht0  (pA,lda,8)
   vextractf128 $1, rY0r, rY0i_ /* y0i_={yi6-7,yi4-5,yr6-7,yr4-5} */
      add lda, pA
   addps rY0i_, rY0r_           /* y0r ={yi2367,yi0145,yr2367,yr0145 */
      add $8, pY
   haddps rY0r_, rY0r_          /* y0r ={yi0-7,y0-7,yi0-7,yr0-7} */
   addps rY0r_, ry0
   movlps ry0, -8(pY)
   sub $1, N
jne LOOPN1_MLT4
jmp DONE
@ROUT ATL_cgemvT_8x4_sse3
@extract -b @(topd)/cw.inc lang=c -define cwdate 2010
#include "atlas_asm.h"
/*
 * This file does a 1x4 unrolled mvt_sse with these params:
 *    CL=8, ORDER=clmajor
 */
#ifndef ATL_GAS_x8664
   #error "This kernel requires x86-64 assembly!"
#endif
/*
 * Integer register assignment
 */
#define M       %rdi
#define N       %rsi
#define pA0     %rdx
#define lda     %rax
#define pX      %r8
#define pY      %r9
#define II      %rbx
#define pX0     %r11
#define Mr      %rcx
#define incAXm  %r10
#define incII   %r15
#define incAn   %r14
#define lda3    %r12
#define Ma      %r13
/*
 * SSE register assignment
 */
#define rA0     %xmm0
#define rX0     %xmm1
#define rx0     %xmm2
#define rt0     %xmm3
#define rY0r    %xmm4
#define rY0i    %xmm5
#define rY1r    %xmm6
#define rY1i    %xmm7
#define rY2r    %xmm8
#define rY2i    %xmm9
#define rY3r    %xmm10
#define rY3i    %xmm11
#define NONEPONEOFF -72
#define NONEPONE %xmm15
/*
 * macros
 */
#ifndef MOVA
   #define MOVA movaps
#endif
#define movapd movaps
#define movupd movups
#define xorpd xorps
#define addpd addps
#define mulpd mulps
#define addsd addss
#define mulsd mulss
#define movsd movss
#define haddpd haddps
/*
 * Define macros controlling prefetch
 */
#ifndef PFDIST
   #define PFDIST 256
#endif
#ifndef PFADIST
   #define PFADIST PFDIST
#endif
#ifndef PFYDIST
   #define PFYDIST 64
#endif
#ifndef PFXDIST
   #define PFXDIST 64
#endif
#ifndef PFIY
   #ifdef ATL_3DNow
      #define PFIY prefetchw
   #else
      #define PFIY prefetchnta
   #endif
#endif
#ifndef PFIX
   #define PFIX prefetcht0
#endif
#ifndef PFIA
   #define PFIA prefetchnta
#endif
#if PFADIST == 0                /* flag for no prefetch */
   #define prefA(mem)
#else
   #define prefA(mem) PFIA mem
#endif
#if PFYDIST == 0                /* flag for no prefetch */
   #define prefY(mem)
#else
   #define prefY(mem) PFIY mem
#endif
#if PFXDIST == 0                /* flag for no prefetch */
   #define prefX(mem)
#else
   #define prefX(mem) PFIX mem
#endif
/*
 *                      %rdi        %rsi           %rdx          %rcx
 * void ATL_UGEMV(ATL_CINT M, ATL_CINT N, const TYPE *A, ATL_CINT lda,
 *                          %r8      %r9
 *                const TYPE *X, TYPE *Y)
 */
.text
.global ATL_asmdecor(ATL_UGEMV)
ALIGN64
ATL_asmdecor(ATL_UGEMV):

/*
 * Save callee-saved iregs
 */
   movq %rbp, -8(%rsp)
   movq %rbx, -16(%rsp)
   movq %r12, -24(%rsp)
   movq %r13, -32(%rsp)
   movq %r14, -40(%rsp)
   movq %r15, -48(%rsp)
/*
 * Compute M = (M/MU)*MU, Mr = M - (M/MU)*MU
 * NOTE: Mr is %rcx reg, so we can use jcx to go to cleanup loop
 */
   mov  %rcx, lda       /* move lda to assigned register, rax */
   mov  $1, Mr          /* setup assignment to peel */
   xor  Ma, Ma          /* default to no peel */
   test $0xF, pA0       /* 0 if 16-byte aligned */
   cmovnz Mr, Ma        /* if nonzero, say need 1 iteration peel */
   sub  Ma, M
   mov  M, Mr           /* Mr = M */
   shr $3, M            /* M = M / MU */
   shl $3, M            /* M = (M/MU)*MU */
   sub M, Mr            /* Mr = M - (M/MU)*MU */
/*
 * Construct ponenone = {-1.0,1.0,-1.0,1.0}
 */
   finit
   fld1                                 /* ST =  1.0 */
   fldz                                 /* ST =  0.0 1.0 */
   fsub %st(1), %st                     /* ST = -1.0 1.0 */
   fsts NONEPONEOFF+4(%rsp)
   fstps NONEPONEOFF+12(%rsp)           /* ST = 1.0 */
   fsts NONEPONEOFF(%rsp)
   fstps NONEPONEOFF+8(%rsp)            /* ST=NULL, mem=-1,1,-1,1*/
   movapd NONEPONEOFF(%rsp), NONEPONE
/*
 * Setup constants
 */
   mov lda, incAn       /* incAn = lda */
   sub M, incAn         /* incAn = lda - (M/MU)*MU */
   sub Ma, incAn
   sub Mr, incAn        /* incAn = lda - M */
   shl $3, incAn        /* incAn = (lda-M)*sizeof */
   shl $3, lda          /* lda *= sizeof */
   sub $-128, pA0       /* code compaction by using signed 1-byte offsets */
   sub $-128, pX        /* code compaction by using signed 1-byte offsets */
   mov pX, pX0          /* save for restore after M loops */
   mov $-64, incAXm     /* code comp: use reg rather than constant */
   lea (lda, lda,2), lda3       /* lda3 = 3*lda */
   lea (incAn, lda3), incAn     /* incAn = (4*lda-M)*sizeof */
   mov $8*1, incII      /* code comp: use reg rather than constant */
   mov M, II
   ALIGN32
   LOOPN:
      xorpd rY0r, rY0r
      xorpd rY0i, rY0i
      xorpd rY1r, rY1r
      xorpd rY1i, rY1i
      xorpd rY2r, rY2r
      xorpd rY2i, rY2i
      xorpd rY3r, rY3r
      xorpd rY3i, rY3i
/*
 *    If no peeled iteration, start M-loop, else do peeled iteration
 */
      bt $0, Ma
      jnc LOOPM
         xorps rA0, rA0
         xorps rX0, rX0
         xorps rx0, rx0
         movlps -128(pX), rX0           /* rX0 = {0, 0, Xi, Xr} */
         pshufd $0xB1, rX0, rx0     /* rx0 = {0, 0, Xr, Xi} */
         movlps -128(pA0), rA0          /* rA0 = {0, 0, Ai, Ar} */
         movaps rA0, rt0                /* rt0 = {0, 0, Ai, Ar} */
         mulps rx0, rA0                 /* rA0 = {0, 0, Xr*Ai, Xi*Ar} */
         addps rA0, rY0i
         mulps rX0, rt0                 /* rt0 = {0, 0, Xi*Ai, Xr*Ar} */
         addps rt0, rY0r
         movlps -128(pA0,lda), rA0        /* rA0 = {0, 0, Ai, Ar} */
         movaps rA0, rt0                /* rt0 = {0, 0, Ai, Ar} */
         mulps rx0, rA0                 /* rA0 = {0, 0, Xr*Ai, Xi*Ar} */
         addps rA0, rY1i
         mulps rX0, rt0                 /* rt0 = {0, 0, Xi*Ai, Xr*Ar} */
         addps rt0, rY1r
         movlps -128(pA0,lda,2), rA0        /* rA0 = {0, 0, Ai, Ar} */
         movaps rA0, rt0                /* rt0 = {0, 0, Ai, Ar} */
         mulps rx0, rA0                 /* rA0 = {0, 0, Xr*Ai, Xi*Ar} */
         addps rA0, rY2i
         mulps rX0, rt0                 /* rt0 = {0, 0, Xi*Ai, Xr*Ar} */
         addps rt0, rY2r
         movlps -128(pA0,lda3), rA0        /* rA0 = {0, 0, Ai, Ar} */
         movaps rA0, rt0                /* rt0 = {0, 0, Ai, Ar} */
         mulps rx0, rA0                 /* rA0 = {0, 0, Xr*Ai, Xi*Ar} */
         addps rA0, rY3i
         mulps rX0, rt0                 /* rt0 = {0, 0, Xi*Ai, Xr*Ar} */
         addps rt0, rY3r
         add $8, pX
         add $8, pA0

      LOOPM:
         movapd 0-128(pX), rX0              /* rX0 = Xi,    Xr */
         pshufd $0xB1, rX0, rx0                 /* rx0 = Xr,    Xi */
         MOVA   0-128(pA0), rA0             /* rA0 = Ai,    Ar */
         movapd rA0, rt0                        /* rt0 = Ai,    Ar */
         mulpd rx0, rA0                         /* rA0 = Ai*Xr, Ar*Xi */
         addpd rA0, rY0i
         prefA(PFADIST+0(pA0))
         mulpd rX0, rt0                         /* rt0 = Ai*Xi, Ar*Xr */
         addpd rt0, rY0r

         MOVA   0-128(pA0,lda), rA0           /* rA0 = Ai,    Ar */ 
         movapd rA0, rt0                        /* rt0 = Ai,    Ar */
         mulpd rx0, rA0                         /* rA0 = Ai*Xr, Ar*Xi */
         addpd rA0, rY1i
         prefA(PFADIST+0(pA0,lda))
         mulpd rX0, rt0                         /* rt0 = Ai*Xi, Ar*Xr */
         addpd rt0, rY1r
         MOVA   0-128(pA0,lda,2), rA0           /* rA0 = Ai,    Ar */ 
         movapd rA0, rt0                        /* rt0 = Ai,    Ar */
         mulpd rx0, rA0                         /* rA0 = Ai*Xr, Ar*Xi */
         addpd rA0, rY2i
         prefA(PFADIST+0(pA0,lda,2))
         mulpd rX0, rt0                         /* rt0 = Ai*Xi, Ar*Xr */
         addpd rt0, rY2r
         MOVA   0-128(pA0,lda3), rA0           /* rA0 = Ai,    Ar */ 
         movapd rA0, rt0                        /* rt0 = Ai,    Ar */
         mulpd rx0, rA0                         /* rA0 = Ai*Xr, Ar*Xi */
         addpd rA0, rY3i
         prefA(PFADIST+0(pA0,lda3))
         mulpd rX0, rt0                         /* rt0 = Ai*Xi, Ar*Xr */
         addpd rt0, rY3r

         movapd 16-128(pX), rX0              /* rX0 = Xi,    Xr */
         pshufd $0xB1, rX0, rx0                 /* rx0 = Xr,    Xi */
         MOVA   16-128(pA0), rA0             /* rA0 = Ai,    Ar */
         movapd rA0, rt0                        /* rt0 = Ai,    Ar */
         mulpd rx0, rA0                         /* rA0 = Ai*Xr, Ar*Xi */
         addpd rA0, rY0i
         mulpd rX0, rt0                         /* rt0 = Ai*Xi, Ar*Xr */
         addpd rt0, rY0r

         MOVA   16-128(pA0,lda), rA0           /* rA0 = Ai,    Ar */ 
         movapd rA0, rt0                        /* rt0 = Ai,    Ar */
         mulpd rx0, rA0                         /* rA0 = Ai*Xr, Ar*Xi */
         addpd rA0, rY1i
         mulpd rX0, rt0                         /* rt0 = Ai*Xi, Ar*Xr */
         addpd rt0, rY1r
         MOVA   16-128(pA0,lda,2), rA0           /* rA0 = Ai,    Ar */ 
         movapd rA0, rt0                        /* rt0 = Ai,    Ar */
         mulpd rx0, rA0                         /* rA0 = Ai*Xr, Ar*Xi */
         addpd rA0, rY2i
         mulpd rX0, rt0                         /* rt0 = Ai*Xi, Ar*Xr */
         addpd rt0, rY2r
         MOVA   16-128(pA0,lda3), rA0           /* rA0 = Ai,    Ar */ 
         movapd rA0, rt0                        /* rt0 = Ai,    Ar */
         mulpd rx0, rA0                         /* rA0 = Ai*Xr, Ar*Xi */
         addpd rA0, rY3i
         mulpd rX0, rt0                         /* rt0 = Ai*Xi, Ar*Xr */
         addpd rt0, rY3r

         movapd 32-128(pX), rX0              /* rX0 = Xi,    Xr */
         pshufd $0xB1, rX0, rx0                 /* rx0 = Xr,    Xi */
         MOVA   32-128(pA0), rA0             /* rA0 = Ai,    Ar */
         movapd rA0, rt0                        /* rt0 = Ai,    Ar */
         mulpd rx0, rA0                         /* rA0 = Ai*Xr, Ar*Xi */
         addpd rA0, rY0i
         mulpd rX0, rt0                         /* rt0 = Ai*Xi, Ar*Xr */
         addpd rt0, rY0r

         MOVA   32-128(pA0,lda), rA0           /* rA0 = Ai,    Ar */ 
         movapd rA0, rt0                        /* rt0 = Ai,    Ar */
         mulpd rx0, rA0                         /* rA0 = Ai*Xr, Ar*Xi */
         addpd rA0, rY1i
         mulpd rX0, rt0                         /* rt0 = Ai*Xi, Ar*Xr */
         addpd rt0, rY1r
         MOVA   32-128(pA0,lda,2), rA0           /* rA0 = Ai,    Ar */ 
         movapd rA0, rt0                        /* rt0 = Ai,    Ar */
         mulpd rx0, rA0                         /* rA0 = Ai*Xr, Ar*Xi */
         addpd rA0, rY2i
         mulpd rX0, rt0                         /* rt0 = Ai*Xi, Ar*Xr */
         addpd rt0, rY2r
         MOVA   32-128(pA0,lda3), rA0           /* rA0 = Ai,    Ar */ 
         movapd rA0, rt0                        /* rt0 = Ai,    Ar */
         mulpd rx0, rA0                         /* rA0 = Ai*Xr, Ar*Xi */
         addpd rA0, rY3i
         mulpd rX0, rt0                         /* rt0 = Ai*Xi, Ar*Xr */
         addpd rt0, rY3r

         movapd 48-128(pX), rX0              /* rX0 = Xi,    Xr */
         pshufd $0xB1, rX0, rx0                 /* rx0 = Xr,    Xi */
         MOVA   48-128(pA0), rA0             /* rA0 = Ai,    Ar */
         movapd rA0, rt0                        /* rt0 = Ai,    Ar */
         mulpd rx0, rA0                         /* rA0 = Ai*Xr, Ar*Xi */
         addpd rA0, rY0i
         mulpd rX0, rt0                         /* rt0 = Ai*Xi, Ar*Xr */
         addpd rt0, rY0r

         MOVA   48-128(pA0,lda), rA0           /* rA0 = Ai,    Ar */ 
         movapd rA0, rt0                        /* rt0 = Ai,    Ar */
         mulpd rx0, rA0                         /* rA0 = Ai*Xr, Ar*Xi */
         addpd rA0, rY1i
         mulpd rX0, rt0                         /* rt0 = Ai*Xi, Ar*Xr */
         addpd rt0, rY1r
         MOVA   48-128(pA0,lda,2), rA0           /* rA0 = Ai,    Ar */ 
         movapd rA0, rt0                        /* rt0 = Ai,    Ar */
         mulpd rx0, rA0                         /* rA0 = Ai*Xr, Ar*Xi */
         addpd rA0, rY2i
         mulpd rX0, rt0                         /* rt0 = Ai*Xi, Ar*Xr */
         addpd rt0, rY2r
         MOVA   48-128(pA0,lda3), rA0           /* rA0 = Ai,    Ar */ 
         movapd rA0, rt0                        /* rt0 = Ai,    Ar */
         mulpd rx0, rA0                         /* rA0 = Ai*Xr, Ar*Xi */
         addpd rA0, rY3i
         mulpd rX0, rt0                         /* rt0 = Ai*Xi, Ar*Xr */
         addpd rt0, rY3r

         sub incAXm, pX
         sub incAXm, pA0
      sub incII, II
      jnz LOOPM

      cmp $0, Mr
      jz  MCLEANED

      mov Mr, II
      xorps rA0, rA0
      xorps rX0, rX0
      xorps rx0, rx0
      LOOPMCU:
         movlps -128(pX), rX0           /* rX0 = {0, 0, Xi, Xr} */
         pshufd $0xB1, rX0, rx0     /* rx0 = {0, 0, Xr, Xi} */
         movlps -128(pA0), rA0          /* rA0 = {0, 0, Ai, Ar} */
         movaps rA0, rt0                /* rt0 = {0, 0, Ai, Ar} */
         mulps rx0, rA0                 /* rA0 = {0, 0, Xr*Ai, Xi*Ar} */
         addps rA0, rY0i
         mulps rX0, rt0                 /* rt0 = {0, 0, Xi*Ai, Xr*Ar} */
         addps rt0, rY0r
         movlps -128(pA0,lda), rA0        /* rA0 = {0, 0, Ai, Ar} */
         movaps rA0, rt0                /* rt0 = {0, 0, Ai, Ar} */
         mulps rx0, rA0                 /* rA0 = {0, 0, Xr*Ai, Xi*Ar} */
         addps rA0, rY1i
         mulps rX0, rt0                 /* rt0 = {0, 0, Xi*Ai, Xr*Ar} */
         addps rt0, rY1r
         movlps -128(pA0,lda,2), rA0        /* rA0 = {0, 0, Ai, Ar} */
         movaps rA0, rt0                /* rt0 = {0, 0, Ai, Ar} */
         mulps rx0, rA0                 /* rA0 = {0, 0, Xr*Ai, Xi*Ar} */
         addps rA0, rY2i
         mulps rX0, rt0                 /* rt0 = {0, 0, Xi*Ai, Xr*Ar} */
         addps rt0, rY2r
         movlps -128(pA0,lda3), rA0        /* rA0 = {0, 0, Ai, Ar} */
         movaps rA0, rt0                /* rt0 = {0, 0, Ai, Ar} */
         mulps rx0, rA0                 /* rA0 = {0, 0, Xr*Ai, Xi*Ar} */
         addps rA0, rY3i
         mulps rX0, rt0                 /* rt0 = {0, 0, Xi*Ai, Xr*Ar} */
         addps rt0, rY3r
         add $8, pX
         add $8, pA0
      dec II
      jnz LOOPMCU

MCLEANED:
                                /* rYr0 = {-rY0d, rY0c, -rY0b, rY0a} */
                                /* rYi0 = { iY0d, iY0c,  iY0b, iY0a} */
      mulps NONEPONE, rY0r   /* rYr = {rY0d,  rY0c,    rY0b,   rY0a} */
      mulps NONEPONE, rY1r   /* rYr = {rY1d,  rY1c,    rY1b,   rY1a} */
      haddps rY0i, rY0r   /* rYr = {iY0cd  ,iY0ab,  rY0cd,  rY0ab} */
      haddps rY1i, rY1r   /* rYr = {iY1cd  ,iY1ab,  rY1cd,  rY1ab} */
      haddps rY1r, rY0r   /* rYr = {iY1abcd,rY1abcd,iY0abcd,rY0abcd} */
      #ifndef BETA0
         addpd 0(pY), rY0r
      #endif
      movaps rY0r, 0(pY)
      mulps NONEPONE, rY2r   /* rYr = {rY0d,  rY0c,    rY0b,   rY0a} */
      mulps NONEPONE, rY3r   /* rYr = {rY1d,  rY1c,    rY1b,   rY1a} */
      haddps rY2i, rY2r   /* rYr = {iY0cd  ,iY0ab,  rY0cd,  rY0ab} */
      haddps rY3i, rY3r   /* rYr = {iY1cd  ,iY1ab,  rY1cd,  rY1ab} */
      haddps rY3r, rY2r   /* rYr = {iY1abcd,rY1abcd,iY0abcd,rY0abcd} */
      #ifndef BETA0
         addpd 16(pY), rY2r
      #endif
      movaps rY2r, 16(pY)
      prefY(4*8+PFYDIST(pY))
      add $4*8, pY
      add incAn, pA0
      mov pX0, pX
      mov M, II
   sub $4, N
   jnz LOOPN
/*
 * EPILOGUE: restore registers and return
 */
   movq -8(%rsp), %rbp
   movq -16(%rsp), %rbx
   movq -24(%rsp), %r12
   movq -32(%rsp), %r13
   movq -40(%rsp), %r14
   movq -48(%rsp), %r15
   ret
