@ROUT atlas_ttypes.h
/*
 * This file defines the types for the new threaded routines, without using
 * any specific thread info so that they can be safely included prior to
 * building & tuning the threading system
 */
#ifdef TYPE
#ifndef ATLAS_TTYPES_H
   #define ATLAS_TTYPES_H
#include "atlas_amm.h"
typedef volatile int VINT;
/*
 * This structure is for a TRSM that uses ATL_trsm as its compute kernel,
 * and only divides N (RHS) matrix.  Works best for tiny TRiangular matrix.
 */
typedef struct ATL_ttrsm_tTR_t ATL_ttrsm_tTR_t;
struct ATL_ttrsm_tTR_t
{
   void *rhsBlkCtr;        /* deals out RHS, [1-nrblks] */
   size_t lda, ldb;        /* leading dims of A & B */
   const TYPE *A;          /* triangular matrix */
   TYPE *B;                /* MxN input/output matrix of right-hand sides */
   #ifdef TREAL
      TYPE alpha;          /* scalar for B */
   #else
      const TYPE *alpha;   /* scalar for B */
   #endif
   enum ATLAS_SIDE side;   /* Whether B on Left or Right of A */
   enum ATLAS_TRANS TA;    /* Whether A is transposed or not */
   enum ATLAS_DIAG uplo;   /* Whether triangle stored Upper of Lower */
   enum ATLAS_DIAG diag;   /* Unit or non-unit diagonal */
   ATL_INT M, N;           /* rows (cols) of array B */
   ATL_INT rb;             /* # of RHS to solve in a given parallel job */
   int rbF;                /* last chunk or RHS dealt out (can be partial) */
   int nrblks;             /* number of RHS blocks dealt out */
};

typedef struct ATL_ttrsm_amm ATL_ttrsm_amm_t;
struct ATL_ttrsm_amm
{
   ammkern_t amm_b0, amm_b1;
   ablk2cmat_t blk2c;
   cm2am_t a2blk, b2blk;
   void *AblkCtr;      /* deals out copy of A blocks column-wise */
   void *rhsCtr;       /* deals out column panels of Right Hand Side (B/X) */
   int *AcpyBV;        /* nablks-len BV; 0: A blk not yet copied */
   void *Acpymut;      /* mutex protecting AcpyBV */
   TYPE *wA;           /* ptr to start of shared A workspace */
   TYPE *w;            /* nthr separate blkszB+panszB workspace */
   const TYPE *A;      /* M-order triangular matrix */
   TYPE *X;            /* MxN matrix of right hand sides (RHS) */
   size_t lda, ldx;
   size_t blkszB;      /* mb*nb, use size_t to prevent overflow with mul */
   size_t blkszA;      /* mb*mb */
   size_t panszC;      /* size of col panel of C : (nmblks-1)*blkszB */
   size_t wsL;         /* # of elts of each core's workspace */
   TYPE alpha;         /* scale to apply to X before access */
   ATL_INT M, N;       /* M: size of A, N: NRHS */
   VINT AcpyDone;      /* set to 1 when copy of entire A complete */
   int nmblks;         /* number of row panels B is split into */
   int nnblks;         /* number of AMM blocks in a row panel */
   int nablks;         /* # of non-diagonal blocks in A */
   int nxblks;         /* # of blocks in X, C has nnblks less */
   int nbf;            /* N%nb, if 0, nb */
   int nnuf;           /* (nbf+nu-1)/nu */
   int mb0;            /* size of 1st diag blk: M%mb, if 0, mb */
   int MB0;            /* for k-vector kernels, (mb0+ku-1)/ku*ku, else mb0 */
   int mb;             /* row & col blocking of A (mb & kb of amm) */
   int nb;             /* column blocking of X/B/C (nb of amm) */
   int mu;             /* M unrolling for amm kernel */
   int nu;             /* N unrolling for amm kernel */
   int ku;             /* K unrolling for amm kernel */
   int nnu, nmu, nmu0;  /* I think can kill nmu0 -> test then KILL */
   enum ATLAS_TRANS TA; 
   enum ATLAS_DIAG uplo;
   enum ATLAS_DIAG diag;
};
/*
 * Function prototypes 
 */
int Mjoin(PATL,tGetTrsmInfo)(ATL_ttrsm_amm_t *pd, int P, enum ATLAS_TRANS TA, 
                             ATL_CSZT M, ATL_CSZT N, const SCALAR beta);
#endif
#endif
@rout atlas_threads.h
#ifndef ATLAS_THREADS_H
   #define ATLAS_THREADS_H
#include "atlas_ttypes.h"
/*
 * Unless told otherwise, use the polling threadpool.  This makes parallel
 * operations faster, but may slow down serial operations due to polling on
 * same core.  Systems where the ATLAS threadpool is distinct from other
 * threadpools will want to make sure ATLAS does not poll, and so will
 * set ATL_SLEEPTPOOL (or make ATLAS to use the shared pool)
 */
#if !defined(ATL_POLLTPOOL) && !defined(ATL_SLEEPTPOOL)
   #define ATL_POLLTPOOL 1
#endif
/*
 * Need to fix this later by using a probe.  ATL_PHI_SORTED being defined
 * asserts that the first P/4 cores in the main list use context 0, the
 * next context 1, and so on.  This is the default sorting by ATLAS, and
 * we'll presently assume it true as long as the NTHR%4 == 0
 */
#if !defined(ATL_PHI_SORTED) && !defined(ATL_PHI_UNSORTED)
   #if defined(ATL_ARCH_XeonPHI) && ATL_NTHREADS%4 == 0
      #define ATL_PHI_SORTED 1
   #else
      #define ATL_PHI_UNSORTED 1
   #endif
#endif
#ifndef ATL_PTMAXMALLOC
   #ifndef ATL_PTMAXMALLOC_MB
      #define ATL_PTMAXMALLOC_MB 256
   #endif
   #define ATL_PTMAXMALLOC (((size_t)(ATL_PTMAXMALLOC_MB)<<20)>>ATL_NTHRPOW2)
#endif
/*
 * If we don't have thread affinity, then the thread I'm waiting on may share
 * the core with me.  In this case, yield my time slice when polling
 */
#include "atlas_tsumm.h"
#include <limits.h>
#if defined(ATL_TAFFINITY) && ATL_TAFFINITY
   #define ATL_POLL
#else
   #define ATL_POLL ATL_thread_yield()
#endif

#if defined(ATL_OS_Win64) || defined(ATL_OS_WinNT)
   #ifdef ATL_USE64BITS
      #define ATL_WIN64THREADS 1
      #define ATL_WINTHREADS 1
   #else
      #define ATL_WIN32THREADS 1
      #define ATL_WINTHREADS 1
   #endif
#endif
#include "atlas_pthreads.h" /* gened file defs ATL_NTHREADS & ATL_NTHRPOW2 */
#ifdef ATL_WINTHREADS
   #include <windows.h>
   typedef struct
   {
      void *vp;      /* ptr to extra info */
      HANDLE thrH;   /* handle to thread */
      int rank;      /* my rank */
      int P;         /* # of processors in this call */
      int affID;     /* affinity id for this core */
   } ATL_thread_t;
   #ifndef CREATE_SUSPENDED
      #define CREATE_SUSPENDED 0x00000004
   #endif
   #ifndef WAIT_FAILED
      #define WAIT_FAILED (~0)
   #endif
#elif defined(ATL_OMP_THREADS)
   #include <omp.h>
   typedef struct
   {
      void *vp;      /* ptr to extra info */
      int rank;      /* my rank */
      int P;         /* # of processors in this call */
      int affID;     /* < 0: not set, affID=1-affID, else affID */
   } ATL_thread_t;
#else
   #define ATL_USE_POSIXTHREADS 1
   #ifndef ATL_USE_THREADPOOL
      #define ATL_USE_THREADPOOL 1 /* by default pthreads uses thread pool*/
   #endif
   #include <pthread.h>
   typedef struct
   {
      pthread_t thrH;/* handle of thread */
      void *vp;      /* ptr to extra info */
      int rank;      /* my rank */
      int P;         /* # of processors in this call */
      int affID;     /* < 0: not set, affID=1-affID, else affID */
   } ATL_thread_t;
#endif

typedef struct ATL_LaunchStruct ATL_LAUNCHSTRUCT_t;
struct ATL_LaunchStruct
{
   ATL_thread_t *rank2thr;              /* index by rank to get thread handle */
   void *opstruct;
   int (*OpStructIsInit)(void*);        /* Query to see if struct is valid */
   void (*DoWork)(ATL_LAUNCHSTRUCT_t*, void*);
   void (*DoComb)(void*, const int, const int);  /* combine func */
   void *vp;                            /* misc. extra info ptr */
   volatile int *chkin;                 /* nthr-len checkin array */
   void **acounts;                      /* var-len array of atomic counters */
};

/*
 * Constants for use in chkin array
 */
#define ATL_CHK_DONE_OP   -2048
#define ATL_CHK_DONE_COMB -2049

/*
 * The following info is all for when a thread pool is being used, rather
 * than launch & join paradigm.
 */
#ifdef ATL_USE_THREADPOOL
@iexp ib 0 1 +
/*
 * Thread pool flag macros
 */
@whiledef nm MICSORTED DIE DYNCOMB ZEROWAKES POLL
#define ATL_TPF_@(nm)(p_) ((p_)->pflag & @(ib))
#define ATL_TPF_SET_@(nm)(p_) ((p_)->pflag |= @(ib))
#define ATL_TPF_UNSET_@(nm)(p_) ((p_)->pflag &= ~@(ib))
   @iexp ib @(ib) 2 *
@endwhile

#define VUINT volatile unsigned int
/*
 * Function pointer taking the pool as an argument that does the work.
 */
typedef void (*ATL_tpjfunc_t)(void *pp, int rank, int vrank);
/*
 * A thread pool takes can combine results at end of run if needed.
 * It takes everything the jobfunc does, + the vranks to combine
 */
typedef void (*ATL_tpjcomb_t)
   (void *pp, int rank, int vrank, int hisvrank);
/*
 * Definition of an ATLAS thread pool
 */
typedef struct ATL_ThreadPool ATL_tpool_t;
struct ATL_ThreadPool
{
   VUINT jobID;           /* count of jobs, wraps at top of uint range */
   VUINT WORKDONE;        /* zeroed to start job, set by last worker done */
   VUINT NOWORK;          /* optionally set when all work dealt out */
   VUINT nthr;            /* # of threads total in this thread pool */
   VUINT nsleep;          /* # of threads that have gone to sleep at start */
   VUINT nworkers;        /* # of thr supposed to wake up and work */
   VUINT wcnt;            /* count incremented as workers wake up */
   VUINT nwdone;          /* # of workers that have completed the task */
   VUINT pflag;           /* bitvector of options */
   void *wcond;           /* cond var for work pool sleep/wake */
   #ifdef ATL_PHI_SORTED
      void *wcond2;       /* cond vars 2nd context sleeps on */
      void *wcond3;       /* cond vars 3rd context sleeps on */
      void *wcond4;       /* cond vars 4th context sleeps on */
   #endif
   void *mcond;           /* cond for master process sleep/wake */
   ATL_thread_t *threads; /* array of thread ptrs */
   volatile int *icomm;   /* nthr-long integer communication array */
   void *combmut;         /* mutex for doing optional combine op */
   int *combReadyBV;      /* 1: thr of that rank is ready to do combine */
   int *combDoneBV;       /* 1: thr of that rank's data already combined */
   void *tpmut;           /* mutex protecting above pool info */
/* 
 * variables below here manipulated only when threads known asleep 
 */
   ATL_tpjfunc_t jobf;    /* ptr to job function to execute */
   ATL_tpjcomb_t combf;   /* NULL: don't combine, else combine func */
   void *PD;              /* problem def to give to jobf & combf */
   void *extra;           /* extra info for jobf & combf */
};

/*
 * Declare the beautiful global variables used by thread pool
 */
   #ifdef ATL_TP_DECL
      double ATL_POLLTIME=0.01;  /* poll for 10 millisecond */
      ATL_tpool_t *ATL_TP_PTR=NULL, *ATL_TP1_PTR=NULL;
   #else
      extern double ATL_POLLTIME;
      extern ATL_tpool_t *ATL_TP_PTR, *ATL_TP1_PTR;
   #endif
   void *ATL_threadpool(void *tp);
   void *ATL_threadpool_launch(void *tp);
   void ATL_InitThreadPoolStartup(int P, void *pd, void *extra);
   void ATL_goParallel (const unsigned int P, void *DoWork, void *DoComb,
                        void *PD, void *extra);
   ATL_tpool_t *ATL_NewThreadPool(const int P, int ICOM, void *vp);
   void ATL_FreeThreadPool(ATL_tpool_t *pp);
   void ATL_oldjobwrap(void *vpp, int rank, int vrank);
   void ATL_oldcombwrap(void *vpp, int rank, int vrank, int vhisrank);
   int ATL_tpool_dojob(ATL_tpool_t *pp, const int rank, const int CallFrWrk);
   void *ATL_threadpool(void *vp);
#endif
   #undef VUINT
/* Sets up ATL_LAUNCHSTRUCT_t var and ATL_thread_t array & starts threads*/
void ATL_thread_launch(void *opstruct, int opstructstride, void *OpStructIsInit,
                       void *DoWork, void *CombineOpStructs);
void ATL_goparallel(const unsigned int P, void *DoWork, void *opstruct, void*);
void ATL_goparallel_prank(const unsigned int P, void *DoWork, void *opstruct,
                          void *DoComb);
/*  Starts a thread running, and sets its affinity to proc if possible */
int ATL_thread_start(ATL_thread_t *thr, int proc, int JOINABLE, 
                     void *(*rout)(void*), void*);
int ATL_thread_join(ATL_thread_t*); /* waits on completion of thread */
void ATL_thread_exit(void*);        /* exits currently executing thread */
void *ATL_log2tlaunch(void *vp);    /* min spanning tree launch */
void *ATL_lin0tlaunch(void *vp);    /* 0 linear launches all threads */
void *ATL_dyntlaunch(void *vp);     /* launch done as workpool */
/*
 * Atomic count functions; may be less overhead than mutex on some systems
 */
void *ATL_SetAtomicCount(int cnt);   /* allocates acnt, sets acnt=cnt */
int   ATL_ResetAtomicCount(void *vp, int cnt);  /* reset vp to cnt */
int   ATL_DecAtomicCount(void *vp);  /* returns acnt-- (not --acnt!) */
int   ATL_GetAtomicCount(void *vp);  /* returns acnt */
void  ATL_FreeAtomicCount(void *vp); /* free acount resources */
/*
 * Global count functions, built out of P local counters for good scaling
 */
void *ATL_SetGlobalAtomicCount(int P, int cnt, int percLoc); 
void  ATL_ResetGlobalAtomicCount(void *vp, int cnt, int percLoc);
int   ATL_DecGlobalAtomicCount(void *vp, int rank); 
int   ATL_GetGlobalAtomicCount(void *vp, int rank);
void  ATL_FreeGlobalAtomicCount(void *vp);
/*
 * Countdown funcs: same as above Glob, but guarantee 1 is last non-zero # ret
 */
void *ATL_SetGlobalAtomicCountDown(int P, int cnt);
int ATL_DecGlobalAtomicCountDown(void *vp, int rank);
void ATL_FreeGlobalAtomicCountDown(void *vp);

/*
 * If using pthreads, just wrapper around pthread mutex funcs, else written
 * in terms of the AtomicCount funcs with init value set to 1
 */
void *ATL_mutex_init(void);       /* returns mutex pointer */
void ATL_mutex_free(void *vp);    /* frees mutex vp */
void ATL_mutex_lock(void *vp);
void ATL_mutex_unlock(void *vp);
int  ATL_mutex_trylock(void *vp); /* opp pthreads: 0 means lock NOT acquired */
void ATL_thread_yield(void);      /* gives up time slice */
/*
 * Condition variables only used for thread pool, not implemented yet on
 * anything but pthreads (Windows & OpenMP missing)
 */
#if ATL_USE_THREADPOOL
   void *ATL_cond_init(void);
   void ATL_cond_free(void *vp);
   void ATL_cond_signal(void *cond);
   void ATL_cond_bcast(void *cond);
   void ATL_cond_wait(void *cond, void *mut);
#endif

#define MindxT(A_,i_) ((void*)( ((char*)(A_)) + ((size_t)i_) ))
#define ATL_tlaunch ATL_log2tlaunch   /* may want linear launch later */
void ATL_tDistMemTouch(size_t N, void *vp);

#endif   /* end of #ifdef protecting include file from redundant inclusion */

@ROUT atlas_tlevel3.h
#ifndef ATLAS_TLEVEL3_H
   #define  ATLAS_TLEVEL3_H
   #define DMM_H 1
   #define SMM_H 1
   #define CMM_H 1
   #define ZMM_H 1
/*
 * ========================================
 * Threaded routines in all four precisions
 * ========================================
 */
@multidef styp double@^*  float@^* double@^ float@^
@multidef typ double float double float
@whiledef pre z c d s
int ATL_@(pre)threadMM(const enum ATLAS_TRANS TA, const enum ATLAS_TRANS TB, 
                  size_t M, size_t N, size_t K);
@whiledef rt gemm
void ATL_@(pre)t@(rt)
   (const enum ATLAS_TRANS TA, const enum ATLAS_TRANS TB, ATL_CINT M, 
    ATL_CINT N, ATL_CINT K, const @(styp)alpha, const @(typ) *A, ATL_CINT lda, 
    const @(typ) *B, ATL_CINT ldb, const @(styp)beta, @(typ) *C, ATL_CINT ldc);
@endwhile
void ATL_@(pre)tsymm
   (const enum ATLAS_SIDE Side, const enum ATLAS_UPLO Uplo,
    ATL_CINT M, ATL_CINT N, const @(styp)alpha, const @(typ) *A, ATL_CINT lda,
    const @(typ) *B, ATL_CINT ldb, const @(styp)beta, @(typ) *C, ATL_CINT ldc);
   @whiledef rt trmm trsm
void ATL_@(pre)t@(rt)
   (const enum ATLAS_SIDE side, const enum ATLAS_UPLO uplo,
    const enum ATLAS_TRANS TA, const enum ATLAS_DIAG diag, 
    ATL_CINT M, ATL_CINT N, const @(styp)alpha, const @(typ) *A, ATL_CINT lda, 
    @(typ) *B, ATL_CINT ldb);
   @endwhile
void ATL_@(pre)tsyr2k
   (const enum ATLAS_UPLO Uplo, const enum ATLAS_TRANS Trans,
    ATL_CINT N, ATL_CINT K, const @(styp)alpha, const @(typ) *A, ATL_CINT lda,
    const @(typ) *B, ATL_CINT ldb, const @(styp)beta, @(typ) *C, ATL_CINT ldc);
void ATL_@(pre)tsyrk
   (const enum ATLAS_UPLO Uplo, const enum ATLAS_TRANS Trans, ATL_CINT N,
    ATL_CINT K, const @(styp)alpha, const @(typ) *A, ATL_CINT lda,
    const @(styp)beta, @(typ) *C, ATL_CINT ldc);
   @undef typ
   @undef styp
@endwhile

/*
 * =======================================================
 * Threaded routines appearing only for complex precisions
 * =======================================================
 */
@multidef sty2 double@^ float@^
@multidef styp double@^*  float@^*
@multidef typ double float
@whiledef pre z c
void ATL_@(pre)themm
   (const enum ATLAS_SIDE Side, const enum ATLAS_UPLO Uplo,
    ATL_CINT M, ATL_CINT N, const @(styp)alpha, const @(typ) *A, ATL_CINT lda,
    const @(typ) *B, ATL_CINT ldb, const @(styp)beta, @(typ) *C, ATL_CINT ldc);
void ATL_@(pre)ther2k
   (const enum ATLAS_UPLO Uplo, const enum ATLAS_TRANS Trans,
    ATL_CINT N, ATL_CINT K, const @(styp)alpha, const @(typ) *A, ATL_CINT lda,
    const @(typ) *B, ATL_CINT ldb, const @(sty2)beta, @(typ) *C, ATL_CINT ldc);
void ATL_@(pre)therk
   (const enum ATLAS_UPLO Uplo, const enum ATLAS_TRANS Trans, ATL_CINT N,
    ATL_CINT K, const @(sty2)alpha, const @(typ) *A, ATL_CINT lda,
    const @(sty2)beta, @(typ) *C, ATL_CINT ldc);
   @undef typ
   @undef styp
   @undef sty2
@endwhile
#endif
@ROUT atlas_tlvl3.h
#ifndef atlas_tlvl3_H
   #define atlas_tlvl3_H
   #define DMM_H 1
   #define SMM_H 1
   #define CMM_H 1
   #define ZMM_H 1

#include "atlas_threads.h"
#ifdef TYPE
   #include "atlas_lvl3.h"
   #include "atlas_amm.h"
#endif
#ifndef ATL_XOVER_L3
   #ifdef TREAL
      #define ATL_XOVER_L3 2   /* number of NBxNB blocks */
   #else
      #define ATL_XOVER_L3 1
   #endif
#endif

#ifndef ATL_TGEMM_XOVER
   #define ATL_TGEMM_XOVER ATL_XOVER_L3
#endif
#ifndef ATL_TGEMM_ADDP
   #define ATL_TGEMM_ADDP 1
#endif
/*
 * Number of blocks per proc for GEMM to divide M only
 */
#ifndef ATL_TMMMINMBLKS
   #define ATL_TMMMINMBLKS 4
#endif
#ifndef ATL_TGEMM_THRESH_MF
   #define ATL_TGEMM_THRESH_MF \
      ((((2.0*(ATL_TGEMM_XOVER))*ATL_AMM_LLCMU)*ATL_AMM_LLCMU)*ATL_AMM_LLCMU)
#endif
/*
 * This is the minimal number of flops each thread requires once THRESH
 * is exceeded
 */
#ifndef ATL_TGEMM_PERTHR_MF
   #define ATL_TGEMM_PERTHR_MF \
      ((((2.0*ATL_TGEMM_ADDP)*ATL_AMM_LLCMU)*ATL_AMM_LLCMU)*ATL_AMM_LLCMU)
#endif
/*
 * For debugging, can define ATL_SERIAL_COMBINE, and then it any required
 * workspaces of C will be allocated before beginning parallel operations,
 * and all required combined will happen after parallel operations are
 * done.
 */
// #define ATL_SERIAL_COMBINE
#ifdef ATL_SERIAL_COMBINE
typedef struct ATL_CombNode ATL_combnode_t;
struct ATL_CombNode
{
   ATL_INT M, N, ldw, ldd;
   void *W, *D;                 /* Work and Destination */
   ATL_combnode_t *next;
};
#endif
/*
 * The array Cinfp holds C partitioning information.  This array holds a
 * list of pointers to nodes whose data I have not been able to combine
 * with my native C partition.  The first nCw entries contain the pointers
 * to the MMNODE of allocated C workspaces that I have not been able to
 * combine.  If my node has C in workspace, I am the first entry in this array.
 * Sometimes, a child thread has been combined with me that owned a piece of
 * the original C.  These values do not need to be combined (they were written
 * to the original C), but we need to combine the range of "owned" workspaces
 * so that we know when it is legal for a parent node to add into the space.
 * The final nCp entries of Cinfp entries of Cinfp hold these original pieces
 * that need to be combined to create larger owned partitions (starting from 
 * the end of the array).  If the C ptr is NULL, that means that entry has
 * been subsumed into a new entry.
 */
typedef struct ATL_TMMNode ATL_TMMNODE_t;
struct ATL_TMMNode
{
   ATL_TMMNODE_t *Cinfp[ATL_NTHREADS];
   void (*gemmK)(ATL_CINT, ATL_CINT, ATL_CINT, const void*, const void *,
                 ATL_CINT, const void*, ATL_CINT, const void*, void*, ATL_CINT);
   const void *A, *B;
   void *C, *Cw;
   void *alpha, *beta;
   void *zero, *one;
   ATL_INT ldcw, M, N, K, lda, ldb, ldc;
   int mb, nb, kb;
   int eltsz, eltsh; /* element size, and shift (eg. log_2(eltsz)) */
   int rank;         /* the rank of my thread ([0,P-1]) */
   int nCw;          /* # of workspace entries in 1st nCw elts of Cinfp array */
   int nCp;          /* # of orig. C pieces last nCp elts of Cinfp */
   int ownC;         /* do I own my piece of C, or only wrkspace? */
};
/*
 * This data structure used for dynamically scheduled rank-K update
 * It is needed only by routines that are typed, and thus define TYPE
 */
#ifdef TYPE
typedef struct
{
   void *aNcnt;           /* count on col-panels of C */
   void *aMcnt;           /* count row-panels of A */
   void **aMcnts;         /* P-len array of counts on row-blks of C */
   void **Mlocks;         /* mutexes protecting init of aMcnts */
   int *Js;               /* current C col for each node */
   int Sync0;             /* 0: no sync at end; else thr 0 waits til all done */
   volatile int *chkin;   /* ATL_NTHREAD-len checkin array */
   TYPE **Bws;            /* preallocated thread copy areas */
   TYPE *Aw;              /* workspace for common A */
   const TYPE *A, *B;     /* original input matrices */
   TYPE *C;               /* original output matrix */
   #ifdef TREAL
      TYPE alpha;          
      TYPE beta;
   #else
      const TYPE *alpha;          
      const TYPE *beta;
   #endif
   ATL_INT nKb, kr, kr8;
   ATL_INT nMb, mr, nNb, nr;
   ATL_INT M, N, K, lda, ldb, ldc;
   enum ATLAS_TRANS TA, TB;
} ATL_TGEMM_RKK_t;
#endif

/*
 * This data structure is used when we split K for SYRK
 */
typedef struct ATL_SyrkK ATL_TSYRK_K_t;
struct ATL_SyrkK
{
   ATL_TSYRK_K_t *Cinfp[ATL_NTHREADS];
@beginskip
   void (*gemmT)(ATL_CINT, ATL_CINT, ATL_CINT, const void*, const void *,
                 ATL_CINT, const void*, ATL_CINT, const void*, void*, ATL_CINT);
@endskip
   void (*gemmT)(const enum ATLAS_TRANS, const enum ATLAS_TRANS,
                  ATL_CINT, ATL_CINT, ATL_CINT, const void *,
                  const void *, ATL_CINT, const void *, ATL_CINT,
                  const void *, void *, ATL_CINT);
   void (*tvsyrk)(const enum ATLAS_UPLO, const enum ATLAS_TRANS, ATL_CINT,
                  ATL_CINT, const void*, const void*, ATL_CINT, const void*,
                  void*, ATL_CINT);
   const void *A;
   void *C, *Cw;
   void *DoComb;
   ATL_LAUNCHSTRUCT_t *lp;
   const void *alpha, *beta;
   const void *zero, *one;
   ATL_INT ldcw, N, K, nb, lda, ldc;
   int eltsh, rank, nCw;
   enum ATLAS_UPLO Uplo;
   enum ATLAS_TRANS Trans, TB;
};
#ifdef TYPE
   #define VINT volatile int
/*
 * This structure used when K <= MAXKB, and M and N are large
 */
typedef struct ATL_tamm_rkK ATL_tamm_rkK_t;
struct ATL_tamm_rkK
{
   ammkern_t amm_b0;
   cm2am_t a2blk;       /* block copy for A */
   cm2am_t b2blk;       /* block copy for B, applies alpha */
   ablk2cmat_t blk2c;   /* copy that applies beta  */
   const TYPE *A;       /* input A matrix */
   const TYPE *B;       /* input B matrix */
   TYPE *C;             /* output matrix */
   TYPE *w;             /* nthr wsz-len thread-local workspaces */
   const TYPE *alpha;   /* ptr to alpha */
   const TYPE *beta;    /* ptr to beta */
   void *B1cpyAsgCtr;   /* 1: 1st B blk not assgnd, caller copies,0: cpy done */
   void *B1cpyDonCtr;   /* 1: 1st B blk not copied yet, 0: cpy done */
   void *AcpyCtr;       /* when 0, all of A has been copied */
   void **MbCtr;        /* nnblk-len array of Mblk ctrs */
   void *BAssgBV;       /* 1 means being copied, 0: not assigned */
   void *BDoneBV;       /* 1 means already copied, 0: not yet copied */
   void *cpBmut;        /* protects B[assg,done]BV */
   size_t wsz;          /* size of local workspace */
   int BCPYDONE;        /* if 1, all of B has been copied */
   int ACPYDONE;        /* if 1, all of A has been copied */
   int bsz;             /* size of common workspace for B */
   int TA;              /* 0: noTrans; 1: Trans */
   int TB;              /* 0: noTrans; 1: Trans */
   int N;               /* # cols of C; N <= MAXNB */
   int K;               /* common dim A&B; K <= MAXKB */
   int nmblks;          /* # of M blocks, including any partial block */
   int mr;              /* M%mu */
   int nbm;             /* # of M blocks 1 MbCtr gives out (1st col always 1) */
   int nmu;             /* CEIL(mb/mu) */
   int nmuL;            /* # of mus in final block */
   int mb;              /* block size for all blocks but last */
   int mbL;             /* block size for last block */
   int nnu;             /* CEIL(N/nu) */
   int KB0;             /* if kmajor, it is CEIL(K/ku)*ku, else K */
   int lda;             /* leading dim of A */
   int ldb;             /* leading dim of B */
   int ldc;             /* leading dim of C */
};
/*
 * This structure used when M <= MAXMB && N <= MAXNB (K large)
 */
typedef struct ATL_tamm_tMN ATL_tamm_tMN_t;
struct ATL_tamm_tMN
{
   ammkern_t amm_b0, amm_b1, ammK_b0, ammK_b1;
   cm2am_t a2blk;       /* block copy for A */
   cm2am_t b2blk;       /* block copy for B */
   const TYPE *A;       /* input A matrix */
   const TYPE *B;       /* input B matrix */
   TYPE *C;             /* output matrix */
   TYPE *w;             /* ATL_NTHREADS wsz-len thread-local workspaces */
   void *KbCtr;         /* which K block */
   int TA;              /* 0: noTrans; 1: Trans */
   int TB;              /* 0: noTrans; 1: Trans */
   int nC;              /* nmu*nnu*mu*nu */
   int M;               /* # of rows of C; M <= MAXMB */
   int N;               /* # cols of C; N <= MAXNB */
   int nmu;             /* CEIL(M/mu) */
   int nnu;             /* CEIL(N/nu) */
   int kb;              /* K blocking factor for all but first block */
   int kb0;             /* K blocking factor for first block */
   int KB0;             /* if k-vectorized, it is CEIL(kb0/ku)*ku, else  */
   int szA, szB;        /* mb*kb, nb*kb */
   size_t szW;          /* size of local workspace */
   size_t K;            /* common dim A&B; K is large & is parallelized */
   size_t nkblks;       /* # of K blocks, including any partial block */
   size_t lda;          /* leading dim of A */
   size_t ldb;          /* leading dim of B */
   size_t ldc;          /* leading dim of C */
};
/*
 * This structure used when N <= MAXNB && K <= MAXKB
 */
typedef struct ATL_tamm_tNK ATL_tamm_tNK_t;
struct ATL_tamm_tNK
{
   ammkern_t amm_b0;
   cm2am_t a2blk;       /* block copy for A */
   cm2am_t b2blk;       /* block copy for B, applies alpha */
   ablk2cmat_t blk2c;   /* copy for beta=1, applies beta  */
   const TYPE *A;       /* input A matrix */
   const TYPE *B;       /* input A matrix */
   TYPE *C;             /* output matrix */
   TYPE *w;             /* ATL_NTHREADS wsz-len thread-local workspaces */
   const TYPE *alpha;   /* ptr to alpha */
   const TYPE *beta;    /* ptr to beta */
   void *MbCtr;         /* which M block */
   void *BassgCtr;      /* 1 means must be copied, 0 assigned */
   void *BdoneCtr;      /* 0 means must be copied, 1 not ready */
   size_t wsz;          /* size of local workspace */
   int bsz;             /* size of common workspace for B */
   int TA;              /* 0: noTrans; 1: Trans */
   int TB;              /* 0: noTrans; 1: Trans */
   int N;               /* # cols of C; N <= MAXNB */
   int K;               /* common dim A&B; K <= MAXKB */
   int nmblks;          /* # of M blocks, including any partial block */
   int mr;              /* M%mu */
   int nmu;             /* CEIL(mb/mu) */
   int nmuL;            /* # of mus in final block */
   int mb;              /* block size for all blocks but last */
   int mbL;             /* block size for last block */
   int nnu;             /* CEIL(N/nu) */
   int KB0;             /* if kmajor, it is CEIL(K/ku)*ku, else K */
   int lda;             /* leading dim of A */
   int ldb;             /* leading dim of B */
   int ldc;             /* leading dim of C */
};
/*
 * This structure used when N <= MIN(MAXNB,MAXMB), so we deal out only
 * K blocks using the global counter KbCtr.  
 */
typedef struct ATL_tsyrk_ammK ATL_tsyrk_ammK_t;
struct ATL_tsyrk_ammK
{
   ammkern_t amm_b0, amm_b1, ammK_b0, ammK_b1;
   cm2am_t a2blk;        /* no-transpose copy */
   cm2am_t b2blk;        /* transpose copy */
   ablk2cmat_t blk2c_b0;/* copy for beta=0 */
   ablk2cmat_t blk2c_b1;/* copy for beta=1 */
   const TYPE *A;       /* input matrix */
   TYPE *C;             /* output matrix */
   TYPE *w;             /* 2*ATL_NTHREADS thread-local mb*nb workspaces */
   const TYPE *alpha;   /* ptr to alpha */
   const TYPE *beta;    /* ptr to beta */
   void *KbCtr;         /* which k block */
   void *Cmut;          /* mutex lock for block of C */
   size_t wsz;          /* size of local workspace */
   VINT BETA_APPLIED;
   int LOWER;           /* set true if lower triangular C */
   int TA;              /* 0: noTrans; 1: Trans */
   int nkblks;          /* # of k blocks, including any partial block */
   int N;               /* total size of C, known to be <= MAXNB */
   int mb;              /* ((N+mu-1)/mu)*mu */
   int nb;              /* ((N+nu-1)/nu)*nu */
   int mbnb;            /* mb * nb */
   int nmu;             /* CEIL(N/mu) */
   int nnu;             /* CEIL(N/nu) */
   int kb;              /* blocking used for K */
   int kb0;             /* K%kb, if 0, kb */
   int KB0;             /* if kmajor, it is CEIL(kb0/ku)*ku, else kb0 */
   int lda;             /* leading dim of A */
   int ldc;             /* leading dim of C */
};
/*
 * This structure used by dynamic access-major SYRK
 * In the first phase, we work only on diagonal blocks, while copying both
 * A & A'.  For diag work, we parallelize both N & K dims so that the copy
 * is done as quickly as possible.  Threads coming in first choose differing
 * diag blks; diagonal blocks are dealt out cheaply using the dCtr global
 * counter (which starts at nnblks == ndiag).
 * Once all diagonal blocks are dealt out, new threads will start using
 * the atomic ctr array KbegCtr array to share K work for each diagonal.
 * both KbegCtr & KdonCtr are nnblk-len arrays of atomic counters.  Each
 * counter starts at nkblks.  Once the block pointed to by KbegCtr is
 * completely copied, the copying array increments the KdonCtr.  Only one
 * core per diag will get KdonCtr == 0 after doing his copy, and this
 * core will seize cdmut mutex in order to set the appropriate bit in
 * cpydonBV, which is a nnblks-length bit vector.  If the kth bit is set,
 * that means the ith row of A & jth col of A' has been copied.
 * Once a thread gets KbegCtr for a particular diag of 0, it means there's
 * no more work for this block of C, and so it will seize the appropriate
 * Cdmuts mutex which protects each diagonal block of C, and write its
 * finished contribution out to C.  The first such thread to ever seize
 * the mutex will scope dbetaBV to find this diagonal block needs beta applied;
 * while later threads will use beta=1.
 * Eventually, all diagonal work is finished, and the first processor to
 * get 0 for all dCtr & KbegCtr requests will set NODWORK=1, so later
 * threads don't have to query all the counters to know they should proceed
 * to non-diagonal work.
 *
 * For non-diagonal work, we count the number of non-diagonal blocks of C,
 * which is initially stored in the ncblks variable, which is protected
 * the cwmut mutex, which also protects the cblkBV, which is a ncblk-len BV.
 * A unset bit means that particular non-diagonal block has not yet been 
 * assigned to a thread, while a 1 means it has.  The mutex also protects
 * the cpydone variable, which is set to 1 when cpydonBV has all bits set.
 * So, threads wanting to do non-diagonal work will find the first unset
 * bit in cblkBV, and then translate that to a (i,j) C block coordinate.
 * If the ith & jth bits are both set in cpydonBV (or cpydone is set), then
 * they will take that block as their own to do (in this phase, each thread
 * gets an individual block of C to do) by setting the bit in cblkBV.
 * When all bits are set in cblkBV, then all work has been dealt out, and
 * threads will exit once they say there is no more work to do.
 * The master process joins all created threads, and can delete data structures
 * safely after all joins succeed.
 */
typedef struct ATL_tsyrk_ammN ATL_tsyrk_ammN_t;
struct ATL_tsyrk_ammN
{
   ammkern_t amm_b0, amm_b1, ammK;
   cm2am_t a2blk;    /* no-transpose copy */
   cm2am_t b2blk;    /* transpose copy */
   ablk2cmat_t blk2d;/* copy for diagonal blocks */
   ablk2cmat_t blk2c;/* copy for non-diagonal blocks */
   const TYPE *A;    /* input matrix */
   TYPE *C;          /* output matrix */
   TYPE *wC;         /* ATL_NTHREADS thread-local nb*nb workspaces */
   TYPE *wA, *wAt;   /* workspaces for storing A & A' */
   const TYPE *alpha;/* ptr to alpha */
   const TYPE *beta; /* ptr to beta */
   int *cpydonBV;    /* nnblks BV set means A & A' copy is done */
   int *cblkBV;      /* ncblks BV for dealing out cblks */
   int *dbetaBV;     /* nnblks BV: unset means beta not yet applied */
   int *cbetaBV;     /* ncblks BV: unset means beta not yet applied */
   void *cdmut;      /* mutex protecting cpydonBV */
   void *cwmut;      /* mutex protecting non-diag work */
   void *dCtr;       /* AtomicCtr for dealing out diagonal blocks */
   void **KbegCtr;   /* K-counters for dealing out diagonal blocks */
   void **KdonCtr;   /* K-counters for completed diagonal blocks */
   void **Cdmuts;    /* mutex locks for each diagonal block of C */
@skip   void *CdonCtr;    /* non-diag computation finished when we reach 0 */
   size_t panszA;    /* nkblks * blkszA */
   #ifdef ATL_PHI_SORTED
      VINT *chkin;   /* ncores*ATL_Cachelen check-in array */
      int ncores;    /* ncores, on PHI it is nthr/4 */
      int ncntxts;   /* ncontexts to use per core */
   #endif
   VINT cpydone;     /* set when all A/A' copying complete */
   int LOWER;        /* set true if lower triangular C */
   int TA;           /* 0: noTrans; 1: Trans */
   int ndiag;        /* # of diagonal blocks */
   int ncblks;       /* # of non-diagonal blocks left to be assigned */
   int nkblks;       /* # of k blocks, including any partial block */
   int NODWORK;      /* set to 1 by first thread to find all work started */
   int nb;           /* N blocking used for all but remainder block */
   int nbnb;         /* nb * nb */
   int nmu;          /* nb/mu */
   int nnu;          /* nb/nu */
   int nbf;          /* N%nb, if 0, nb */
   int nnuf;         /* CEIL(nbf/nu) */
   int nmuf;         /* CEIL(nbf/mu) */
   int kb;           /* blocking used for K */
   int kb0;          /* K%kb, if 0, kb */
   int KB0;          /* if kmajor, it is CEIL(kb0/ku)*ku, else kb0 */
   int blkszA;       /* nb*kb */
   int Mf;           /* ((nbf+mu-1)/mu)*mu */
   int Nf;           /* ((nbf+nu-1)/nu)*nu */
   int lda;          /* leading dim of A */
   int ldc;          /* leading dim of C */
};
/*
 * This data structure for doing access-major threaded gemm for moderately
 * sized problems where no dimension is <= to the blocking factor, and
 * the problem is not too large to prevent us from copying all of A & B
 * up front.  Copying up front allows us to compute the blocks of C 
 * in any order.  For large problems, will have to recur (mainly on K) to
 * get workspace down to reasonable levels.  This case will not work well
 * if the number of C blocks is not quite a bit larger than the nthreads.
 */

typedef struct ATL_tgemm_ammG ATL_tgemm_ammG_t;
struct ATL_tgemm_ammG
{
   ammkern_t amm_b0, amm_b1, ammK_b0, ammK_b1;
   cm2am_t a2blk;    /* copy/transpose for A */
   cm2am_t b2blk;    /* copy/tranpose for B */
   ablk2cmat_t blk2c,/* access-major to column-major copy/scale for C */
     blk2c_b1;       /* access-major to column-major copy for C, BETA=1 */
   const TYPE *A;    /* input matrix */
   const TYPE *B;    /* input matrix */
   TYPE *C;          /* output matrix */
   TYPE *wC;         /* ATL_NTHREADS thread-local mb*nb workspaces */
   TYPE *wA, *wB;    /* workspaces for storing A & B */
   SCALAR beta;      /* beta */
   SCALAR alpA;      /* alpha to apply to A */
   SCALAR alpB;      /* alpha to apply to B */
   SCALAR alpC;      /* alpha to apply to C */
   int *cpyAdBV;     /* nmblks BV, set means K-panel of A has been copied */
   int *cpyBdBV;     /* nnblks BV, set means K-panel of A has been copied */
   int *cCblkBV;     /* nMNblks BV for dealing C blks while copying */
   int *cbetaBV;     /* nMNblks BV: unset means beta not yet applied */
   void *cpmut;      /* mutex protecting cpyA/BdBV & cCblkBV */
   void *ccCtr;      /* nMNblks ctr for dealing out (diagonal) blocks wt copy */
   void *cCtr;       /* ncblks ctr for dealing out blocks w/o copy */
   void **KbegCtr;   /* counters for dealing out K blocks for copying */
   void **KdonCtr;   /* K-counters for copied blocks */
   void **Cmuts;     /* MNblks mutex locks for copy-blocks of C */
   size_t panszA;    /* nkblks * blkszA */
   size_t panszB;    /* nkblks * blkszB */
   size_t lda;       /* leading dim of A */
   size_t ldb;       /* leading dim of B */
   size_t ldc;       /* leading dim of C */
   #ifdef ATL_PHI_SORTED
      VINT *chkin;   /* ncores*ATL_Cachelen check-in array */
      int ncores;    /* ncores, on PHI it is nthr/4 */
      int ncntxts;   /* ncontexts to use per core */
   #endif
   VINT cpyAdone;    /* set when all of A has been copied */
   VINT cpyBdone;    /* set when all of B has been copied */
   VINT NOCPWORK;    /* set to 1 by 1st thread to find all copy work started */
   int TA;           /* 0: noTrans; 1: Trans */
   int TB;           /* 0: noTrans; 1: Trans */
   int nCblks;       /* nmblks * nnblks */
   int nMNblks;      /* MAX(nmblks,nnblks) */
   int nmblks;       /* CEIL(M/mb) */
   int nnblks;       /* CEIL(N/nb) */
   int nkblks;       /* CEIL(K/kb) */
   int mb;           /* M blocking used for all but remainder block */
   int nb;           /* N blocking used for all but remainder block */
   int nmu;          /* mb/mu */
   int nnu;          /* nb/nu */
   int mbf;          /* M%mb, if 0, mb */
   int nbf;          /* N%nb, if 0, nb */
   int nmuf;         /* CEIL(mbf/mu) */
   int nnuf;         /* CEIL(nbf/nu) */
   int kb;           /* blocking used for K */
   int kb0;          /* K%kb, if 0, kb */
   int KB0;          /* if kmajor, it is CEIL(kb0/ku)*ku, else kb0 */
   int blkszA;       /* mb*kb */
   int blkszB;       /* nb*kb */
   int blkszC;       /* mb*nb */
};
   #undef VINT
#endif
/*
 * This data structure used when we divide N only, and NTHREAD is a power of 2
 */
typedef struct 
{
   void (*gemmK)(ATL_CINT, ATL_CINT, ATL_CINT, const void*, const void *,
                 ATL_CINT, const void*, ATL_CINT, const void*, void*, ATL_CINT);
   void (*tvsyrk)(const enum ATLAS_UPLO, const enum ATLAS_TRANS, ATL_CINT,
                  ATL_CINT, const void*, const void*, ATL_CINT, const void*,
                  void*, ATL_CINT);
   void *T;             /* Triangular matrix to do SYRK into*/
   void *C;             /* rect matrix to do GEMM into */
   const void *A0;      /* input matrix for syrk, */
   const void *A;       /* 1st input matrix for GEMM */
   const void *B;       /* 2nd input matrix for GEMM */
   const void *alpha, *beta;
   ATL_INT M;           /* size of SYRK and 1st dim of GEMM */
   ATL_INT N;           /* size of 2nd dim of N */
   ATL_INT K;           /* K of original problem */
   ATL_INT lda, ldc;
   int nb, eltsh;       /* shift to do equivalant of *sizeof */
   enum ATLAS_UPLO Uplo;
   enum ATLAS_TRANS TA, TB;
} ATL_TSYRK_M_t;

@beginskip
#if ATL_NTHREADS == (1<<ATL_NTHRPOW2)
typedef struct
{
   void (*gemmK)(ATL_CINT, ATL_CINT, ATL_CINT, const void*, const void *,
                 ATL_CINT, const void*, ATL_CINT, const void*, void*, ATL_CINT);
   void (*tvsyrk)(const enum ATLAS_UPLO, const enum ATLAS_TRANS, ATL_CINT,
                  ATL_CINT, const void*, const void*, ATL_CINT, const void*,
                  void*, ATL_CINT);
   const void *A0;  /* input matrix, split only along N */
   const void *A1;  /* A of 2nd triangular matrix (C), or B of gemm */
   void *T;         /* 1st triangular matrix */
   void *C;         /* if (T), 2nd triangular mat, else rect matrix */
   const void *alpha, *beta;
   ATL_INT M;      /* if (T) order of 1st triang mat, else 1st dim of C */
   ATL_INT N;      /* if (T) order of 2nd triang mat, else 2nd dim of C */
   ATL_INT K;      /* size of K dim (2nd dim of A, first of A^T) */
   ATL_INT lda, ldc;
   int nb, eltsh;  /* shift to do equivalant of *sizeof */
   enum ATLAS_UPLO Uplo;
   enum ATLAS_TRANS TA, TB;
} ATL_TSYRK_N_t;
/*
 * sets ATL_TSYRK_N_t sy_[i] = sy[j]
 */
#define McpSYN(sy_, i_, j_) \
@skip   memcpy((sy_)+(i_), (sy_)+(j_), sizeof(ATL_TSYRK_N_t));
{ \
   (sy_)[(i_)].gemmK  = (sy_)[(j_)].gemmK; \
   (sy_)[(i_)].tvsyrk = (sy_)[(j_)].tvsyrk; \
   (sy_)[(i_)].numthr = (sy_)[(j_)].numthr; \
   (sy_)[(i_)].A0     = (sy_)[(j_)].A0; \
   (sy_)[(i_)].A1     = (sy_)[(j_)].A1; \
   (sy_)[(i_)].T      = (sy_)[(j_)].T; \
   (sy_)[(i_)].C      = (sy_)[(j_)].C; \
   (sy_)[(i_)].alpha  = (sy_)[(j_)].alpha; \
   (sy_)[(i_)].beta   = (sy_)[(j_)].beta; \
   (sy_)[(i_)].M      = (sy_)[(j_)].M; \
   (sy_)[(i_)].N      = (sy_)[(j_)].N; \
   (sy_)[(i_)].K      = (sy_)[(j_)].K; \
   (sy_)[(i_)].nb     = (sy_)[(j_)].nb; \
   (sy_)[(i_)].lda    = (sy_)[(j_)].lda; \
   (sy_)[(i_)].ldc    = (sy_)[(j_)].ldc; \
   (sy_)[(i_)].eltsh  = (sy_)[(j_)].eltsh; \
   (sy_)[(i_)].Uplo   = (sy_)[(j_)].Uplo; \
   (sy_)[(i_)].TA     = (sy_)[(j_)].TA; \
   (sy_)[(i_)].TB     = (sy_)[(j_)].TB; \
}
#endif
@endskip

typedef struct
{
   const void *A, *alpha;
   void *B;
@skip   void (*trsmK)(ATL_TTRSM_t*);
   ATL_INT M, N, lda, ldb;
@skip   int eltsh;                   /* shift for element size */
   enum ATLAS_SIDE side;
   enum ATLAS_UPLO uplo;
   enum ATLAS_TRANS TA;
   enum ATLAS_DIAG  diag;
} ATL_TTRSM_t;

typedef struct
{
   const void *A, *B, *alpha, *beta;
   void *C;
   ATL_INT M, N, lda, ldb, ldc, nb;
   enum ATLAS_SIDE side;
   enum ATLAS_UPLO uplo;
} ATL_TSYMM_t;
typedef struct
{
   const void *alpha, *alpha2, *beta, *one, *zero;
   void (*tvgemm)(const enum ATLAS_TRANS, const enum ATLAS_TRANS,
                  ATL_CINT, ATL_CINT, ATL_CINT, const void *,
                  const void *, ATL_CINT, const void *, ATL_CINT,
                  const void *, void *, ATL_CINT);
   void (*tvApAt)(const enum ATLAS_UPLO, ATL_CINT, const void *, ATL_CINT, 
                  const void *, void *, ATL_CINT);

   ATL_INT K, lda, ldb, ldc;
   int nb, eltsh;
   enum ATLAS_UPLO Uplo;
   enum ATLAS_TRANS trans, TA, TB,  /* trans for syr2k, TA,TB are for GEMM */
                    TA2, TB2;       /* transpose of TA,TB */
} ATL_SYR2K_t;

@beginskip
typedef struct
{
   const void *alpha, *beta, *one, *zero;
   void (*tvgemm)(const enum ATLAS_TRANS, const enum ATLAS_TRANS,
                  ATL_CINT, ATL_CINT, ATL_CINT, const void *,
                  const void *, ATL_CINT, const void *, ATL_CINT,
                  const void *, void *, ATL_CINT);
   void (*tvsyrk)(const enum ATLAS_UPLO, const enum ATLAS_TRANS, ATL_CINT,
                  ATL_CINT, const void*, const void*, ATL_CINT, const void*,
                  void*, ATL_CINT);
   const void *A, *B;
   void *C;
   ATL_INT T, M, N, K, nb, ia, ja, ib, jb, ic, jc, eltsh, lda, ldc;
   enum ATLAS_UPLO Uplo;
   enum ATLAS_TRANS Trans;
} ATL_TSYRK_t;
@endskip
/*
 * For triangular matrices, diagonal blocks are handled specially, but we
 * get dense square blocks above/below the diagonal.  We consider Upper
 * triangular the transpose of Lower, allowing us to only handle Lower.
 * Our AtomicCtr routines are 1-D counters, not 2-D, so we linearize the
 * blocks beneath the diagonal by counting them column-wise.  So, a 4x4
 * matrix of blocks would look like:
 *    |X X X X|
 *    |1 X X X|
 *    |2 4 X X|
 *    |3 5 7 X|
 */
/*
 * Translates (i,j) coordinate of lower triangular matrix to number between
 * [0,n), where n = number of non-diagonal block.  nm_ is the number of
 * diagonal blocks in the matrix (4 in example above).  
 */
#define Mcoord2tblk(nm_, i_, j_) \
   ((nm_)*(j_) - (((1+(j_))*(j_))>>1) + (i_)-(j_)-1)

/*
 * In a lower matrix of with NB_ diagonal blocks, translate the linearized
 * block number B_ back into rectangular (I,J) coordinates.  The difficulty
 * is finding J.  Would like to do it with an equation, like we do when
 * converting from coordinates to block number.  Tony Castaldo came up with
 *    J = (int)((nm-0.5-0.5*sqrt(4*nm*nm-4*nm+1-8*b)))
 * but sqrt is a function call which does a Newtonian iteration on floats
 * (therefore, has a relatively slow loop).  Finding the J column is indeed
 * the hard part, and in theory we can use binary search to find in 
 * O(log_2(NB_)) time.  However, this algorithm requires multiplication
 * inside the loop, and so it is never competitive for the range we are
 * interested in (NB_ usually < 10, and almost never larger than 2000).
 * So instead do a linear search to find j, but optimize by first
 * constraining j to a 128-column region, then a 8-column region, and
 * then find the actual column.  So worst-case loop counts are
 *  CEIL(NB_/128) + 16 + 8
 *
 * nblkC = # of blocks in a [128,8]-column Chunk
 */
#define Mtblk2coord(NB_, B_, I_, J_) \
{ \
   unsigned int j_=0; \
   unsigned int n_ = (NB_), b_=(B_), nblksC_; \
   KEEP_LOOKING128: /* find 128-col region where J is */ \
      nblksC_ = (n_<<7)-8256; \
      if (b_ < nblksC_ || n_ < 128) \
         goto FOUND128; \
      b_ -= nblksC_; \
      n_ -= 128; \
      j_ += 128; \
   goto KEEP_LOOKING128; \
   FOUND128: \
   KEEP_LOOKING8: /* find 8-col region where J is */ \
      nblksC_ = (n_<<3)-36; \
      if (b_ < nblksC_ || n_ < 8) \
         goto FOUND8; \
      b_ -= nblksC_; \
      n_ -= 8; \
      j_ += 8; \
   goto KEEP_LOOKING8; \
   FOUND8: \
      for (n_--; b_ >= n_; j_++) \
         b_ -= n_--; \
   (J_) = j_; \
   (I_) = j_ + b_ + 1; \
}
@beginskip
/*
 * buffer i, j in case they aren't type int; hopefully compiler will eliminate
 * when they are through copy propogation
 */
#define Mtblk2coord(NM_, B_, I_, J_) \
{ \
   int i_, j_; \ 
   ATL_tblk2coord(NM_, B_,  &(i_), &(j_)); \
   I_ = i_; \
   I_ = i_; \
}
#define Mtblk2coord(NM_, B_, I_, J_) \
{ \
   register int n_ = (NM_)-1, b_=(B_), j_; \
   for (j_=0; b_ >= n_; j_++) \
   { \
      b_ -= n_; \
      n_--; \
   } \
   (J_) = j_; \
   (I_) = j_ + b_ + 1; \
}
@endskip

/*
 * =============================================================================
 * Function prototypes
 * =============================================================================
 */
void ATL_EnforceNonPwr2LO(ATL_TMMNODE_t *ptmms, const int P);
int Mjoin(PATL,threadMM)(const enum ATLAS_TRANS TA, const enum ATLAS_TRANS TB, 
                         size_t M, size_t N, size_t K);
@skip int Mjoin(PATL,tNumGemmThreads)(ATL_CINT M, ATL_CINT N, ATL_CINT K);
void ATL_tvsyr2k_rec(ATL_SYR2K_t *syp, ATL_CINT Nblks, ATL_CINT nr, 
                     const void *A, const void *B, void *C);
#ifdef TYPE
void Mjoin(PATL,tsyrk)
   (const enum ATLAS_UPLO Uplo, const enum ATLAS_TRANS Trans, ATL_CINT N,
    ATL_CINT K, const SCALAR alpha, const TYPE *A, ATL_CINT lda,
    const SCALAR beta, TYPE *C, ATL_CINT ldc);
#ifdef TCPLX
void Mjoin(PATL,therk)
   (const enum ATLAS_UPLO Uplo, const enum ATLAS_TRANS Trans, ATL_CINT N,
    ATL_CINT K, const TYPE alpha, const TYPE *A, ATL_CINT lda,
    const TYPE beta, TYPE *C, ATL_CINT ldc);
#endif
void Mjoin(PATL,tsymm)
   (const enum ATLAS_SIDE Side, const enum ATLAS_UPLO Uplo, 
    ATL_CINT M, ATL_CINT N, const SCALAR alpha, const TYPE *A, ATL_CINT lda, 
    const TYPE *B, ATL_CINT ldb, const SCALAR beta, TYPE *C, ATL_CINT ldc);
#ifdef TCPLX
void Mjoin(PATL,themm)
   (const enum ATLAS_SIDE Side, const enum ATLAS_UPLO Uplo, 
    ATL_CINT M, ATL_CINT N, const SCALAR alpha, const TYPE *A, ATL_CINT lda, 
    const TYPE *B, ATL_CINT ldb, const SCALAR beta, TYPE *C, ATL_CINT ldc);
#endif
void Mjoin(PATL,tsyr2k)
   (const enum ATLAS_UPLO Uplo, const enum ATLAS_TRANS Trans,
    ATL_CINT N, ATL_CINT K, const SCALAR alpha, const TYPE *A, ATL_CINT lda,
    const TYPE *B, ATL_CINT ldb, const SCALAR beta, TYPE *C, ATL_CINT ldc);
void Mjoin(PATL,tsyr2k)
   (const enum ATLAS_UPLO Uplo, const enum ATLAS_TRANS Trans,
    ATL_CINT N, ATL_CINT K, const SCALAR alpha, const TYPE *A, ATL_CINT lda,
    const TYPE *B, ATL_CINT ldb, const SCALAR beta, TYPE *C, ATL_CINT ldc);
#ifdef TCPLX
void Mjoin(PATL,ther2k)
   (const enum ATLAS_UPLO Uplo, const enum ATLAS_TRANS Trans,
    ATL_CINT N, ATL_CINT K, const SCALAR alpha, const TYPE *A, ATL_CINT lda,
    const TYPE *B, ATL_CINT ldb, const TYPE beta, TYPE *C, ATL_CINT ldc);
#endif

void Mjoin(PATL,ttrsm)(const enum ATLAS_SIDE side, const enum ATLAS_UPLO uplo, 
                       const enum ATLAS_TRANS TA, const enum ATLAS_DIAG diag,
                       ATL_CINT M, ATL_CINT N, const SCALAR alpha,
                       const TYPE *A, ATL_CINT lda, TYPE *B, ATL_CINT ldb);
void Mjoin(PATL,ttrmm)(const enum ATLAS_SIDE side, const enum ATLAS_UPLO uplo, 
                       const enum ATLAS_TRANS TA, const enum ATLAS_DIAG diag,
                       ATL_CINT M, ATL_CINT N, const SCALAR alpha,
                       const TYPE *A, ATL_CINT lda, TYPE *B, ATL_CINT ldb);
void Mjoin(PATL,tgemm)(const enum ATLAS_TRANS TA, const enum ATLAS_TRANS TB,
                       ATL_CINT M, ATL_CINT N, ATL_CINT K, const SCALAR alpha,
                       const TYPE *A, ATL_CINT lda, const TYPE *B, ATL_CINT ldb,
                       const SCALAR beta, TYPE *C, ATL_CINT ldc);
void Mjoin(PATL,tvgemm)(const enum ATLAS_TRANS TA, const enum ATLAS_TRANS TB,
                        ATL_CINT M, ATL_CINT N, ATL_CINT K, const void *alpha,
                        const void *A, ATL_CINT lda, const void *B,ATL_CINT ldb,
                        const void *beta, void *C, ATL_CINT ldc);
int Mjoin(PATL,ammm_REC)
   (const enum ATLAS_TRANS TA, const enum ATLAS_TRANS TB, 
    ATL_CINT M, ATL_CINT N, ATL_CINT K, const SCALAR alpha,
    const TYPE *A, ATL_CINT lda, const TYPE *B, ATL_CINT ldb,
    const SCALAR beta, TYPE *C, ATL_CINT ldc,
    int (*amm)(enum ATLAS_TRANS,enum ATLAS_TRANS, ATL_CINT, ATL_CINT, ATL_CINT,
               const SCALAR, const TYPE*, ATL_CINT,  const TYPE*, ATL_CINT,
               const SCALAR, TYPE*, ATL_CINT));
@whiledef rt tammm_G tammm_tNK tammm_tMN
int Mjoin(PATL,@(rt))
   (const enum ATLAS_TRANS TA, const enum ATLAS_TRANS TB, 
    ATL_CINT M, ATL_CINT N, ATL_CINT K, const SCALAR alpha,
    const TYPE *A, ATL_CINT lda, const TYPE *B, ATL_CINT ldb,
    const SCALAR beta, TYPE *C, ATL_CINT ldc);
@endwhile

@whiledef ds rec rkK bigMN_Kp
int Mjoin(PATL,tgemm_@(ds))
   (const enum ATLAS_TRANS TA, const enum ATLAS_TRANS TB, 
    ATL_CINT M, ATL_CINT N, ATL_CINT K, const SCALAR alpha,
    const TYPE *A, ATL_CINT lda, const TYPE *B, ATL_CINT ldb,
    const SCALAR beta, TYPE *C, ATL_CINT ldc);
@endwhile
@whiledef TA T N C
   @whiledef TB T N C
@mif TA = "C
#ifdef TCPLX
@endmif
@mif TB = "C
   @mif TA ! "C
#ifdef TCPLX
   @endmif
@endmif
void Mjoin(PATL,tgemm@(TA)@(TB))
   (ATL_CINT M, ATL_CINT N, ATL_CINT K, const SCALAR alpha,
    const TYPE *A, ATL_CINT lda, const TYPE *B, ATL_CINT ldb,
    const SCALAR beta, TYPE *C, ATL_CINT ldc);
void Mjoin(PATL,tsvgemm@(TA)@(TB))
   (ATL_CINT M, ATL_CINT N, ATL_CINT K, const void* alpha,
    const void *A, ATL_CINT lda, const void *B, ATL_CINT ldb,
    const void *beta, void *C, ATL_CINT ldc);
@mif TA = "C
#endif  /* end ifdef TCPLX */
@endmif
@mif TB = "C
   @mif TA ! "C
#endif  /* end ifdef TCPLX */
   @endmif
@endmif
   @endwhile
@endwhile
#endif  /* end ifdef TYPE */

#ifdef ATL_ESTNCTR
   #ifdef __GNUC__
   static inline int ATL_EstNctr    /* RETURNS: good # of global ctrs */
   #else
   static int ATL_EstNctr           /* RETURNS: good # of global ctrs */
   #endif
   (
      int N,   /* Total count you are going to use */
      int P    /* nthreads to be used */
   )
   {
      int ncnt, ncntP;
/*
 *    Want at least 32 elements per counter
 */
      ncnt = (N > 64) ? (N >> 5) : 1;
/*
 *    allow up to 8-way contention
 */
      if (P >= 16)
         ncntP = (P>>3);
      else 
         ncntP = (P >= 4) ? (P>>1) : 1;
      return(Mmin(ncnt,ncntP));
   }
#endif

#endif
@ROUT ATL_set_ucnt
/*
 * This routines provide the basis of ATLAS's fast and almost-contention-free
 * partitioning algorithm.  Before spawning the threads, the problem is
 * divided into n partitions, and the master thread allocates an unordered
 * counter wt call to ATL_set_ucnt(n).  
 * After threading is complete, the space is freed by ATL_free_ucnt(void*vp);
 * Now, threads call ATL_get_ucnt(void *vp), which returns 0 if all partitions
 * have been handled; a number between 1 & n says that that partition of the
 * problem remains to be done.
 * We can then use an assembly command like XCHG to determine if a given
 * set is available for use.
 * To ease the amount of cache coherence message, each thread gets his
 * own region in each space, which looks like:
 * <p> <n1> <rk1 off> ... <nP> <rkP off> <rk1 region>...<rkP region>
 * beginning of space aligned to CL, all regions start on CL boundary.  Each
 * region is n/p long, with any remainders stuck in the first n%P regions.
 * Each thread will pass his rank in and therefore will do the majority of
 * writing on his own region (avoiding ping-ponging lines thru cache).  
 * Only once all his sets are exhausted will he search other thread's sets
 * (thus starting ping-pong).
 * All we need to implement is something like XCHG, which everybody has:
 * SPARC: LDSTUB: ld/store unsigned byte loads value from memory, rights 0xff
 *                to byte atomically 
 *           http://developers.sun.com/solaris/articles/atomic_sparc/
 * x86  : XCHG
 * PPC  : lwarx/stwcx, see: http://www.ibm.com/developerworks/library/pa-atom/
 * MIPS : ll/sc (load linked store conditional); I think similar to PPC
 *
 * In some systems, we can actually using a simple counter:
 *  SPARC: cas (compare & swap), v9 only
 *   x86 : CMPXCHG
 *   PPC : lwarx/stwcx
 *   MIPS: ll/sc
 *
 * So, can use cntr for all harware we now about, so can implement an assembly
 *    int GetAtomicCount(void *vp)
 * In systems wt cntr support, simply use directly, quitting when count is 0
 * or negative.  For systems that can only handle a boolean (like XCHG/LDSTUB),
 * store the boolean immediately after the counter.  This file will be tested
 * in assembly for compile & use, and we set a Make.inc macro to point at
 * something like: GetAtomicCount_[ppc,sparc,x86,mips,mutex].o, all of
 * which appear in the ATLAS/src/threads directory.
 */
void *ATL_set_ucnt(int nsets, int nranks)
{
}
void ATL_free_ucnt(void *vp)
{
}
int ATL_get_ucnt(void *vp, int rank)
/* 
 * RETURNS: 0 if all sets taken, else a number between 1...nsets
{
}
@ROUT ATL_thread_yield
#include "atlas_misc.h"
#include "atlas_threads.h"
#ifndef ATL_WINTHREADS
   #include <sched.h>
#endif
void ATL_thread_yield(void)
{
   #ifdef ATL_WINTHREADS
      Sleep(0);
   #elif defined(ATL_ARCH_XeonPHI)
       __asm__ __volatile__ ("delay %0" :: "r"(64) : "0" );
   #else
      sched_yield();
   #endif
}
@ROUT ATL_cond_init ATL_cond_free ATL_cond_signal ATL_cond_bcast ATL_cond_wait
#include "atlas_misc.h"
#include "atlas_threads.h"
#if ATL_USE_THREADPOOL
   #if defined(ATL_WINTHREADS) || defined(ATL_OMP_THREADS)
      #error "Thread pool stuff not yet written for non-pthreads!"
   #endif
@ROUT ATL_cond_init
void *ATL_cond_init(void)
{
   void *vp;
   vp = malloc(sizeof(pthread_cond_t));
   ATL_assert(vp);
   ATL_assert(!pthread_cond_init(vp, NULL));
   return(vp);
}
@ROUT ATL_cond_free
void ATL_cond_free(void *vp)
{
   ATL_assert(!pthread_cond_destroy(vp));
   free(vp);
}
@ROUT ATL_cond_bcast
void ATL_cond_bcast(void *cond)
{
   ATL_assert(!pthread_cond_broadcast(cond));
}
@ROUT ATL_cond_signal
void ATL_cond_signal(void *cond)
{
   ATL_assert(!pthread_cond_signal(cond));
}
@ROUT ATL_cond_wait
void ATL_cond_wait(void *cond, void *mut)
{
   ATL_assert(!pthread_cond_wait(cond, mut));
}
@ROUT ATL_cond_init ATL_cond_free ATL_cond_signal ATL_cond_bcast ATL_cond_wait
#endif
@ROUT ATL_mutex_init
#include "atlas_misc.h"
#include "atlas_threads.h"
void *ATL_mutex_init(void)
{
/*
 * On Windows, use known-good x86 code.  OS X's mutex have horrible scaling,
 * so use homebrewed code instead
 */
#if defined(ATL_WINTHREADS) || (defined(ATL_OS_OSX) && defined(ATL_SSE1))
   return(ATL_SetAtomicCount(1));
#elif defined(ATL_OMP_THREADS)
   void *vp;
   vp = malloc(sizeof(omp_lock_t));
   ATL_assert(vp);
   omp_init_lock(vp);
   return(vp);
#else
   void *vp;
   vp = malloc(sizeof(pthread_mutex_t));
   ATL_assert(vp);
   ATL_assert(!pthread_mutex_init(vp, NULL));
   return(vp);
#endif
}
@ROUT ATL_mutex_free
#include "atlas_misc.h"
#include "atlas_threads.h"
void ATL_mutex_free(void *vp)
{
#if defined(ATL_WINTHREADS) || (defined(ATL_OS_OSX) && defined(ATL_SSE1))
   ATL_FreeAtomicCount(vp);
#elif defined(ATL_OMP_THREADS)
   omp_destroy_lock(vp);
   free(vp);
#else
   ATL_assert(!pthread_mutex_destroy(vp));
   free(vp);
#endif
}
@ROUT ATL_mutex_free
@ROUT ATL_mutex_lock
   @define rt @lock@
@ROUT ATL_mutex_unlock
   @define rt @unlock@
@ROUT ATL_mutex_lock ATL_mutex_unlock
#include "atlas_misc.h"
#include "atlas_threads.h"

void ATL_mutex_@(rt)(void *vp)
{
@ROUT ATL_mutex_lock
#ifdef ATL_WINTHREADS  /* if not using pthreads, use AtomicCount to sim mut */
   while(!ATL_DecAtomicCount(vp));
#elif defined(ATL_OS_OSX) && defined(ATL_SSE1)
   while(!ATL_DecAtomicCount(vp))
      ATL_thread_yield();
@ROUT ATL_mutex_unlock
#if defined(ATL_WINTHREADS) || (defined(ATL_OS_OSX) && defined(ATL_SSE1))
   ATL_ResetAtomicCount(vp, 1);
@ROUT ATL_mutex_lock ATL_mutex_unlock
#elif defined(ATL_OMP_THREADS)
@ROUT ATL_mutex_unlock `   omp_unset_lock(vp);`
@ROUT ATL_mutex_lock `   omp_set_lock(vp);`
#else
   ATL_assert(!pthread_mutex_@(rt)(vp));
#endif
}
@ROUT ATL_mutex_trylock
#include "atlas_misc.h"
#include "atlas_threads.h"

int ATL_mutex_trylock(void *vp)
/*
 * return 0 if lock not required, else return non-zero 
 */
{
#if defined(ATL_WINTHREADS) || (defined(ATL_OS_OSX) && defined(ATL_SSE1))
   return(ATL_DecAtomicCount(vp));
#elif defined(ATL_OMP_THREADS)
   return(omp_test_lock(vp));
#else
   return(!pthread_mutex_trylock(vp));
#endif
}
@ROUT ATL_SetAtomicCount_mut
#include "atlas_misc.h"
#include "atlas_threads.h"

void *ATL_SetAtomicCount(int cnt)
{
#if defined(ATL_OMP_THREADS)
   char *cp;
   omp_lock_t *mp;
   int *cntp;
   cp = malloc(256+sizeof(int) + sizeof(omp_lock_t));
   ATL_assert(cp);
   cntp = (int*)(cp+128);  /* avoid false sharing wt 128-byte guard */
   mp = (omp_lock_t*)(cntp+2);
   omp_init_lock(mp);
   *cntp = cnt;
   return((void*)cp);
#else
   char *cp;
   pthread_mutex_t *mp;
   int *cntp;
   cp = malloc(256+sizeof(int) + sizeof(pthread_mutex_t));
   ATL_assert(cp);
   cntp = (int*)(cp+128);  /* avoid false sharing wt 128-byte guard */
   mp = (pthread_mutex_t*)(cntp+2);
   ATL_assert(!pthread_mutex_init(mp, NULL));
   *cntp = cnt;
   return((void*)cp);
#endif
}

@ROUT ATL_SetGlobalAtomicCountDown ATL_DecGlobalAtomicCountDown @\
      ATL_FreeGlobalAtomicCountDown
#include "atlas_misc.h"
#include "atlas_threads.h"
/*
 * GlobalAtomicCountDowns are same as GlobalAtomicCount, except that they
 * guarantee that their last non-zero value is 1 (they don't guarantee 
 * a sequential count down, despite the name).  They are used over Counts
 * when it is vital that the someone atomically knows they are the last
 * non-zero return (i.e. the last worker frees resources, etc).
 */
@ROUT ATL_SetGlobalAtomicCountDown 
void *ATL_SetGlobalAtomicCountDown
(
   int P,               /* # of Local counters to use to make global ctr */
   int cnt              /* total count to start global count at */
)
{
   void **va;
   va = malloc(2*sizeof(void*));
   ATL_assert(va);
   if (cnt > 2 && P > 1)
   {
      va[0] = ATL_SetGlobalAtomicCount(P, cnt-1, 0);
      va[1] = ATL_SetAtomicCount(1);
   }
   else
   {
      va[0] = NULL;
      va[1] = ATL_SetAtomicCount(cnt);
   }
   return(va);
}

@ROUT ATL_DecGlobalAtomicCountDown
int ATL_DecGlobalAtomicCountDown(void *vp, int rank)
{
   void **va=vp;
   if (va[0])
   {
      int i;
      i = ATL_DecGlobalAtomicCount(va[0], rank);
      if (i)
         return(i+1);
   }
   return(ATL_DecAtomicCount(va[1]));
}

@ROUT ATL_FreeGlobalAtomicCountDown
void ATL_FreeGlobalAtomicCountDown(void *vp)
{
   void **va=vp;
   if (va[0])
      ATL_FreeGlobalAtomicCount(va[0]);
   ATL_FreeAtomicCount(va[1]);
   free(vp);
}
@ROUT ATL_SetGlobalAtomicCount ATL_ResetGlobalAtomicCount
#include "atlas_misc.h"
#include "atlas_threads.h"

@ROUT ATL_ResetGlobalAtomicCount
void ATL_ResetGlobalAtomicCount(void *vp, int cnt, int percLoc)
/*
 * This routine resets the global atomic counter vp to cnt
 */
{
   int *ip=vp, *lcnts = ip+4;
   const int P = *ip;
   void **cnts = (void**)(lcnts+(((P+3)>>2)<<2));
   int i, b, extra, nL, nG;

@ROUT ATL_SetGlobalAtomicCount
void *ATL_SetGlobalAtomicCount
(
   int P,               /* # of Local counters to use to make global ctr */
   int cnt,             /* total count to start global count at */
   int percLoc          /* fraction of local work to reserve for callers */
)                       /* whose rank is exactly equal to the cnt index */
/*
 * This routine counts down from cnt to 0 for in a thread-safe way.
 * For scalability, the count is split up into P different counters
 * (this minimizes contention on atomic counters).  Further, if percLoc
 * is non-zero, then .01*fracLoc*local(cnt) numbers will be reserved exclusively
 * for callers that set their rank to the local counter index.  This allows
 * us to force a particular node to do at least that many columns, and for
 * those column accesses we can do it a non-rentrant read, which means it
 * runs much faster.  However, this means the caller will need to be sure
 * in this case that two processors cannot call with the same rank!
 */
{
   void **cnts;
   int *ip, *lcnts;
   int i, b, extra, nL, nG;

@ROUT ATL_SetGlobalAtomicCount ATL_ResetGlobalAtomicCount
   b = cnt / P;
   extra = cnt - b*P;
   nL = (percLoc > 0) ? percLoc*.01*b : 0;
   nG = b - nL;
@ROUT ATL_SetGlobalAtomicCount
   i = ((P+3)>>2)<<2;
   ip = malloc(P*sizeof(void*)+(i+4)*sizeof(int));
   ATL_assert(ip);
   lcnts = ip+4;
   cnts = (void**)(lcnts + i);
@ROUT ATL_SetGlobalAtomicCount ATL_ResetGlobalAtomicCount
   ip[0] = P;
   ip[1] = b;
   ip[2] = extra;
   ip[3] = nL;

   for (i=0; i < P; i++)
   {
      int n = nG;
      if (i < extra) n++;
@ROUT ATL_SetGlobalAtomicCount   `      cnts[i] = ATL_SetAtomicCount(n);`
@ROUT ATL_ResetGlobalAtomicCount `      ATL_ResetAtomicCount(cnts[i], n);`
      lcnts[i] = nL;
   }
@ROUT ATL_SetGlobalAtomicCount `   return((void*)ip);`
}

@ROUT ATL_FreeGlobalAtomicCount
#include "atlas_misc.h"
#include "atlas_threads.h"

void ATL_FreeGlobalAtomicCount(void *vp)
{
   int *ip=vp;
   const int P=ip[0];
   void **acnts = (void**)(ip+4+(((P+3)>>2)<<2));
   int i;
   for (i=0; i < P; i++)
      ATL_FreeAtomicCount(acnts[i]);
   free(vp);
}

@ROUT ATL_DecGlobalAtomicCount
   @define op @Dec@
@ROUT ATL_GetGlobalAtomicCount
   @define op @Get@
@ROUT ATL_DecGlobalAtomicCount ATL_GetGlobalAtomicCount
#include "atlas_misc.h"
#include "atlas_threads.h"
int ATL_@(op)GlobalAtomicCount(void *vp, int rank)
/*
 * This routine returns a global counter that has been distributed over
 * P local counters
 */
{
   int i, j, P, b, icnt, extra, nL, *ip=vp, *iloc;
   void **acnts;

   P = ip[0];
   b = ip[1];
   extra = ip[2];
   nL = ip[3];
   iloc = ip+4;
/*
 * See if I can get the index from purely local information
 */
   if (rank < P && rank >= 0 && nL)
   {
      j = iloc[rank];
      if (j)
      {
@ROUT ATL_DecGlobalAtomicCount `         iloc[rank]--;`
         j += b * rank + Mmin(rank, extra);
//fprintf(stderr, "%d: j=%d, LRET\n", rank, j);
         return(j);
      }
   }
   acnts = (void**) (ip+4+(((P+3)>>2)<<2));
/*
 * Otherwise, find an atomic counter that still has count
 */
   for (i=0; i < P; i++)
   {
/* 
 *    If I got a counter value, convert it from local to global
 */
      icnt = (rank+i)%P;
      if (j = ATL_@(op)AtomicCount(acnts[icnt]))
      {
         j += nL + b*icnt + Mmin(icnt,extra);
         break;
      }
   }
//fprintf(stderr, "%d: j=%d, icnt=%d, b=%d P=%d, e=%d\n", rank, j, icnt, b, P, extra);
@skip      j = (b) ? ((j-1)/b)*P*b + icnt*b + (j-1)%b + 1 : icnt+1;
   return(j);
}
@ROUT ATL_IsFirstThreadedCall
#include "atlas_threads.h"
/*
 * A kludge for initializing thread pool related stuff on first call
 * in portable fashion.  
 * OpenMP just uses the OpenMP thread pool, so should not call here.
 * pthreads uses standard initializer.
 * Otherwise, I assume Windows, where DecAtomicCtr should work.
 */
#ifdef ATL_USE_THREADPOOL

#if ATL_USE_POSIXTHREADS
   static pthread_mutex_t initlock=PTHREAD_MUTEX_INITIALIZER;
   static int atlinit=1;
#else   /* non-pthreads is Windows, where AtomicCtrs work */
   volatile static int ilck[65] = {1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,
      1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,
      1,1,1,1,1,1,1};
#endif
int ATL_IsFirstThreadedCall(void)
{
#ifdef ATL_OMP_THREADS
   fprintf(stderr, "%s: OpenMP should never call this, ATLAS dying!\n", 
   __FILE__);
   ATL_assert(0);
#elif ATL_USE_POSIXTHREADS
   int iret;
   pthread_mutex_lock(&initlock);
   iret = atlinit;
   atlinit = 0;
   pthread_mutex_unlock(&initlock);
   return(iret);
#else   /* non-pthreads is Windows, where AtomicCtrs work */
   return(ATL_DecAtomicCtr((void*)ilck);
#endif
}
/*
 * This function not thread safe, so user must make sure only called once
 */
void ATL_ResetThreadPoolInit(void)
{
#if ATL_USE_POSIXTHREADS
   pthread_mutex_lock(&initlock);
   atlinit = 1;
   pthread_mutex_unlock(&initlock);
#else
   ATL_ResetAtomicCount((void*)ilck, 1);
#endif
}
#endif
@ROUT ATL_FreeAtomicCount_mut
#include <stdlib.h>
#ifdef ATL_OMP_THREADS
   #include <omp.h>
#else
   #include <pthread.h>
#endif
#include "atlas_misc.h"
void ATL_FreeAtomicCount(void *vp)
{
   char *cp=vp;

#ifdef ATL_OMP_THREADS
   omp_destroy_lock((omp_lock_t*)(cp+2*sizeof(int)+128));
#else
   ATL_assert(!pthread_mutex_destroy((pthread_mutex_t*)(cp+2*sizeof(int)+128)));
#endif
   free(vp);
}
@ROUT ATL_ResetAtomicCount_mut ATL_DecAtomicCount_mut
#ifdef ATL_OMP_THREADS
   #include <omp.h>
#else
   #include <pthread.h>
#endif
@ROUT ATL_DecAtomicCount_mut
int ATL_DecAtomicCount(void *vp)
@ROUT ATL_ResetAtomicCount_mut
int ATL_ResetAtomicCount(void *vp, int cnt)
@ROUT ATL_ResetAtomicCount_mut ATL_DecAtomicCount_mut
{
   char *cp=vp;
   #ifdef ATL_OMP_THREADS
      omp_lock_t *mp;
   #else
      pthread_mutex_t *mp;
   #endif
   int *cntp;
   int iret;

   cntp = (int*)(cp+128);
   #ifdef ATL_OMP_THREADS
      mp = (omp_lock_t*)(cntp+2);
      omp_set_lock(mp);
   #else
      mp = (pthread_mutex_t*)(cntp+2);
      pthread_mutex_lock(mp);
   #endif
   iret = *cntp;
@ROUT ATL_DecAtomicCount_mut   `   if (iret) (*cntp)--;`
@ROUT ATL_ResetAtomicCount_mut `   *cntp = cnt;`
   #ifdef ATL_OMP_THREADS
      omp_unset_lock(mp);
   #else
      pthread_mutex_unlock(mp);
   #endif
   return(iret);
}
@ROUT ATL_SetAtomicCount_arch
#include "atlas_misc.h"

void *ATL_SetAtomicCount(int cnt)
{
   int *ip;

   ip = malloc(260); /* make false sharing unlikely by */
   ATL_assert(ip);   /* putting a 128 byte guard on */
   ip[32] = cnt;     /* both sides of counter */
   return((void*)ip);
}
@ROUT ATL_FreeAtomicCount_arch
#include <stdlib.h>
void ATL_FreeAtomicCount(void *vp)
{
   free(vp);   /* could just do #define ATL_FreeAtomicCount free */
}              /* but do this so compiles same as _mut version */
@ROUT ATL_GetAtomicCount
int ATL_GetAtomicCount(void *vp)
{
   volatile int *ip = vp;
   return(ip[32]);
}
@ROUT ATL_ResetAtomicCount_ia32 ATL_ResetAtomicCount_amd64 ATL_ResetAtomicCount_win64
#include "atlas_asm.h"
/*
@ROUT ATL_ResetAtomicCount_ia32 ATL_ResetAtomicCount_amd64
 * rax                       rdi         rsi
@ROUT ATL_ResetAtomicCount_win64
 * rax                       rcx         rdx
@ROUT ATL_ResetAtomicCount_ia32 ATL_ResetAtomicCount_amd64 ATL_ResetAtomicCount_win64
 * int ATL_ResetAtomicCount(void *vp, int cnt)
 * Sets vp's acnt=cnt.
 * RETURNS: acnt before the reset
 */
.text
.global ATL_asmdecor(ATL_ResetAtomicCount)
ATL_asmdecor(ATL_ResetAtomicCount):
@ROUT ATL_ResetAtomicCount_ia32
   @define mm @%edx@
   @define ct @%ecx@
   movl 4(%esp), @(mm)
   movl 8(%esp), @(ct)
@ROUT ATL_ResetAtomicCount_amd64
   @define mm @%rdi@
   @define ct @%esi@
@ROUT ATL_ResetAtomicCount_win64
   @define mm @%rcx@
   @define ct @%edx@
@ROUT ATL_ResetAtomicCount_ia32 ATL_ResetAtomicCount_amd64 ATL_ResetAtomicCount_win64
   sub $-128, @(mm)            /* skip false sharing guard zone */
   ATOMIC_LOOP:
      movl (@(mm)), %eax       /* read acnt from memory */
      lock                    /* make cmpxchg atomic */
      cmpxchg @(ct), (@(mm))   /* put cnt in mem if mem still == acnt in eax */
      je DONE                 /* ZF set if cmpxchg wrote to mem */
   jmp ATOMIC_LOOP            /* ZF=0 means cmpxch failed, try again */
DONE:
   ret
@ROUT ATL_DecAtomicCount_amd64 ATL_DecAtomicCount_ia32 ATL_DecAtomicCount_win64
#include "atlas_asm.h"
/* rax                  %rdi/rcx/4  */
/* int ATL_DecAtomicCount(void *vp) */
.text
.global ATL_asmdecor(ATL_DecAtomicCount)
ATL_asmdecor(ATL_DecAtomicCount):
@ROUT ATL_DecAtomicCount_ia32
   movl 4(%esp), %edx
   @define mm @%edx@
@ROUT ATL_DecAtomicCount_amd64 
   @define mm @%rdi@
@ROUT ATL_DecAtomicCount_win64 
   @define mm @%rdx@
   movq %rcx, %rdx
@ROUT ATL_DecAtomicCount_amd64 ATL_DecAtomicCount_ia32 ATL_DecAtomicCount_win64
   sub $-128, @(mm)            /* skip false sharing guard zone */
   ATOMIC_LOOP:
      movl (@(mm)), %eax       /* read cnt from memory */
      movl %eax, %ecx         /* ecx = cnt */
      subl $1, %ecx           /* ecx = cnt-1 */
      jl ZERO_RET             /* return 0 if count already below 1 */
      lock                    /* make cmpxchg atomic */
      cmpxchg %ecx, (@(mm))    /* put cnt-1 in mem if mem still == cnt in eax */
      je DONE                 /* ZF set if cmpxchg wrote to mem */
   jmp ATOMIC_LOOP            /* ZF=0 means cmpxch failed, try again */

ZERO_RET:
@ROUT ATL_DecAtomicCount_amd64 ATL_DecAtomicCount_win64 `   xor %rax, %rax`
@ROUT ATL_DecAtomicCount_ia32  `   xor %eax, %eax`
@skip   movl %eax, (@(mm))  /* safety to ensure no roll from neg back to pos */
DONE:
   ret
@ROUT ATL_ResetAtomicCount_ppc
   @define op @Reset@
@ROUT ATL_DecAtomicCount_ppc
   @define op @Dec@
@ROUT ATL_DecAtomicCount_ppc ATL_ResetAtomicCount_ppc
#include "atlas_asm.h"
.text
#ifdef ATL_AS_OSX_PPC
   .globl _ATL_@(op)AtomicCount
   _ATL_@(op)AtomicCount:
#else
   #if defined(ATL_USE64BITS)
/*
 *      Official Program Descripter section, seg fault w/o it on Linux/PPC64
 */
        .section        ".opd","aw"
        .align 2
	.globl  ATL_USERMM
        .align  3
ATL_@(op)AtomicCount:
        .quad   Mjoin(.,ATL_@(op)AtomicCount),.TOC.@tocbase,0
        .previous
        .type   Mjoin(.,ATL_@(op)AtomicCount),@function
        .text
	.globl  Mjoin(.,ATL_@(op)AtomicCount)
.ATL_@(op)AtomicCount:
   #else
	.globl  ATL_@(op)AtomicCount
ATL_@(op)AtomicCount:
   #endif
#endif
@ROUT ATL_ResetAtomicCount_ppc
/* r3                                 r3       r4 */
/* int int ATL_ResetAtomicCount(void *vp, int cnt) */
@ROUT ATL_DecAtomicCount_ppc
#error "Code is not reliable on PPC, don't know why"
/* r3                           r3  */
/* int ATL_DecAtomicCount(void *vp) */
@ROUT ATL_DecAtomicCount_ppc ATL_ResetAtomicCount_ppc
RETRY:
   lwarx r5, 0, r3    /* Read int from mem, place reservation */
@ROUT ATL_DecAtomicCount_ppc
   addi  r5, r5, -1   /* decrement value */
   stwcx. r5, 0, r3   /* attempt to store decremented value back to mem */
@ROUT ATL_ResetAtomicCount_ppc
   stwcx. r4, 0, r3   /* attempt to store new value back to mem */
@ROUT ATL_DecAtomicCount_ppc ATL_ResetAtomicCount_ppc
   bne-  RETRY        /* If store failed, retry */
   mr r3, r5
   blr
@ROUT ATL_DecAtomicCount_sparc ATL_ResetAtomicCount_sparc @\
      ATL_DecAtomicCount_mips ATL_ResetAtomicCount_mips
#error "not implemented"
@ROUT ATL_thread_launch
#include "atlas_misc.h"
#include "atlas_threads.h"

/*
 * These redefinitions allow us to try various launch structures
 */
#ifdef ATL_TUNE_LIN
   #ifdef ATL_tlaunch
      #undef ATL_tlaunch
   #endif
   #ifdef ATL_NOAFFINITY
      #define ATL_tlaunch ATL_lin0tlaunch_noaff
      #define ATL_thread_launch ATL_tllin_noaff
      #define ATL_thread_start ATL_thread_start_noaff
   #else
      #define ATL_tlaunch ATL_lin0tlaunch
      #define ATL_thread_launch ATL_tllin
   #endif
#elif defined(ATL_TUNE_LG2)
   #ifdef ATL_tlaunch
      #undef ATL_tlaunch
   #endif
   #ifdef ATL_NOAFFINITY
      #define ATL_tlaunch ATL_log2tlaunch_noaff
      #define ATL_thread_launch ATL_tllg2_noaff
      #define ATL_thread_start ATL_thread_start_noaff
   #else
      #define ATL_tlaunch ATL_log2tlaunch
      #define ATL_thread_launch ATL_tllg2
   #endif
#elif defined(ATL_TUNE_DYN)
   #ifdef ATL_tlaunch
      #undef ATL_tlaunch
   #endif
   #ifdef ATL_NOAFFINITY
      #define ATL_tlaunch ATL_dyntlaunch_noaff
      #define ATL_thread_launch ATL_tldyn_noaff
      #define ATL_thread_start ATL_thread_start_noaff
   #else
      #define ATL_tlaunch ATL_dyntlaunch
      #define ATL_thread_launch ATL_tldyn
   #endif
#endif
void *ATL_tlaunch(void *vp); /* _noaff versions not protoed in threads.h */

/*
 * This routine can be called by any threaded routine to autoset all needed
 * data structures for ATLAS to launch the threads for a parallel operation
 */
void ATL_thread_launch
(
   void *opstruct,              /* P-len struct given to each launched thread */
   int opstructstride,          /* sizeof(opstruct) */
   void *OpStructIsInit,        /* NULL, or test to see if thread is spawned */
   void *DoWork,                /* computation to call (rout launched) */
   void *CombineOpStructs       /* NULL, or combine func */
)
{
   ATL_thread_t tp[ATL_NTHREADS];
   ATL_LAUNCHSTRUCT_t ls;
   void *vp;
   int i;

   ls.opstruct = opstruct;
   ls.opstructstride = opstructstride;
   ls.CombineOpStructs = CombineOpStructs;
   ls.OpStructIsInit = OpStructIsInit;
   ls.DoWork = DoWork;
   ls.rank2thr = tp;
   #ifdef ATL_TUNE_DYN
      ls.acounts = &vp;
      ls.acounts[0] = ATL_SetGlobalAtomicCount(ATL_NTHREADS>>1, 
                                               ATL_NTHREADS-1, 0);
      ls.chkin = calloc(ATL_NTHREADS, sizeof(int));
      ATL_assert(ls.chkin);
   #endif

   for (i=0; i < ATL_NTHREADS; i++)
   {
      tp[i].vp = &ls;
      tp[i].rank = i;
   }
   ATL_thread_start(tp, 0, 1, ATL_tlaunch, tp);
   ATL_thread_join(tp);
   #ifdef ATL_TUNE_DYN
       ATL_FreeGlobalAtomicCount(ls.acounts[0]);
       free((void*)ls.chkin);
   #endif
}
@ROUT ATL_thread_start
#ifndef ATL_NOAFFINITY
   #include "atlas_taffinity.h"  /* include this file first! */
#elif defined(ATL_TUNING)
   #define ATL_thread_start ATL_thread_start_noaff
#endif
#include "atlas_misc.h"
#include "atlas_threads.h"
int ATL_thread_start(ATL_thread_t *thr, int proc, int JOINABLE,
                     void *(*rout)(void*), void *arg)
/*
 * Creates a thread that will run only on processor proc.
 * RETURNS: 0 on success, non-zero on error
 * NOTE: present implementation dies on error, so 0 is always returned.
 */
{
#ifdef ATL_WINTHREADS
   #ifdef ATL_WIN32THREADS
      DWORD thrID;
   #else
      unsigned thrID;
   #endif
      
   #ifdef ATL_NOAFFINITY
      #ifdef ATL_WIN32THREADS
         thr->thrH = CreateThread(NULL, 0, rout, arg, 0, &thrID);
      #else
         thr->thrH = (HANDLE)_beginthreadex(NULL, 0, rout, arg, 0, &thrID);
      #endif
      ATL_assert(thr->thrH);
   #else
      thr->rank = proc;
      #ifdef ATL_WIN32THREADS
         thr->thrH = CreateThread(NULL, 0, rout, arg, CREATE_SUSPENDED, &thrID);
      #else
         thr->thrH = (HANDLE)_beginthreadex(NULL, 0, rout, arg, 
                                            CREATE_SUSPENDED, &thrID);
      #endif
      ATL_assert(thr->thrH);
      #ifdef ATL_RANK_IS_PROCESSORID
         ATL_assert(SetThreadAffinityMask(thr->thrH, ((long long)1)<<proc)));
         thr->affID = proc;
      #else
         thr->affID = ATL_affinityIDs[proc%ATL_AFF_NUMID];
         ATL_assert(SetThreadAffinityMask(thr->thrH, 
                    (((long long)1)<<(thr->affID))));
      #endif
      ATL_assert(ResumeThread(thr->thrH) == 1);
   #endif
#elif defined(ATL_OMP_THREADS)
   fprintf(stderr, "Should not call thread_start when using OpenMP!");
   ATL_assert(0);
#elif 0 && defined(ATL_OS_OSX)  /* unchecked special OSX code */
/* http://developer.apple.com/library/mac/#releasenotes/Performance/RN-AffinityAPI/_index.html */
   pthread_attr_t attr;
   #define ATL_OSX_AFF_SETS 2       /* should be probed for */
   thread_affinity_policy ap;

   ap.affinity_tag = proc % ATL_OSX_AFF_SETS;
   ATL_assert(!pthread_attr_init(&attr));
   if (JOINABLE)
      ATL_assert(!pthread_attr_setdetachstate(&attr, PTHREAD_CREATE_JOINABLE));
   else
      ATL_assert(!pthread_attr_setdetachstate(&attr, PTHREAD_CREATE_DETACHED));
   pthread_attr_setscope(&attr, PTHREAD_SCOPE_SYSTEM); /* no chk, OK to fail */

   ATL_assert(!pthread_create(&thr->thrH, &attr, rout, arg));
   ATL_assert(!thread_policy_set(thr->thrH, THREAD_AFFINITY_POLICY, 
                                 (integer_t*)&ap, 
                                 THREAD_AFFINITY_POLICY_COUNT));
   ATL_assert(!pthread_attr_destroy(&attr));
#else
   pthread_attr_t attr;
   #ifndef ATL_NOAFFINITY
      #if defined(ATL_PAFF_SETAFFNP) || defined(ATL_PAFF_SCHED)
         cpu_set_t cpuset;
      #elif defined(ATL_PAFF_PLPA)
         plpa_cpu_set_t cpuset;
      #elif defined(ATL_PAFF_CPUSET) /* untried FreeBSD code */
         cpuset_t mycpuset;
      #endif
      #ifdef ATL_RANK_IS_PROCESSORID
         const int affID = proc;
      #else
         const int affID = ATL_affinityIDs[proc%ATL_AFF_NUMID];
      #endif
      #ifdef ATL_PAFF_SELF
         thr->affID = -affID-1; /* affinity must be set by created thread */
      #endif
   #endif
   thr->rank = proc;
   ATL_assert(!pthread_attr_init(&attr));
   if (JOINABLE)
   {
      #ifdef IBM_PT_ERROR
         ATL_assert(!pthread_attr_setdetachstate(&attr, 
                                                 PTHREAD_CREATE_UNDETACHED));
      #else
         ATL_assert(!pthread_attr_setdetachstate(&attr, 
                                                 PTHREAD_CREATE_JOINABLE));
      #endif
   }
   else
      ATL_assert(!pthread_attr_setdetachstate(&attr, PTHREAD_CREATE_DETACHED));
   pthread_attr_setscope(&attr, PTHREAD_SCOPE_SYSTEM); /* no chk, OK to fail */
   #ifdef ATL_PAFF_SETAFFNP
      CPU_ZERO(&cpuset);
      CPU_SET(affID, &cpuset);
      ATL_assert(!pthread_attr_setaffinity_np(&attr, sizeof(cpuset), &cpuset));
   #elif defined(ATL_PAFF_SETPROCNP)
      ATL_assert(!pthread_attr_setprocessor_np(&attr, (pthread_spu_t)affID, 
                                               PTHREAD_BIND_FORCED_NP)); 
   #endif
   ATL_assert(!pthread_create(&thr->thrH, &attr, rout, arg));
   #if defined(ATL_PAFF_PBIND)
      ATL_assert(!processor_bind(P_LWPID, thr->thrH, affID, NULL));
      thr->affID = affID;  /* affinity set by spawner */
   #elif defined(ATL_PAFF_BINDP)
      ATL_assert(!bindprocessor(BINDTHREAD, thr->thrH, affID));
      thr->affID = affID;  /* affinity set by spawner */
   #elif defined(ATL_PAFF_CPUSET)  /* untried FreeBSD code */
      CPU_ZERO(&mycpuset);         /* no manpage, so guess works like linux */
      CPU_SET(affID, &mycpuset);
      if (!cpuset_setaffinity(CPU_LEVEL_WHICH, CPU_WHICH_TID, thr->thrH,
                              sizeof(mycpuset), &mycpuset));
         thr->affID = affID;  /* affinity set by spawner */
   #endif
   ATL_assert(!pthread_attr_destroy(&attr));
#endif
   return(0);
}
@ROUT ATL_thread_join
#include "atlas_misc.h"
#include "atlas_threads.h"
int ATL_thread_join(ATL_thread_t *thr)   /* waits on completion of thread */
{
#ifdef ATL_WINTHREADS
   ATL_assert(WaitForSingleObject(thr->thrH, INFINITE) != WAIT_FAILED);
   ATL_assert(CloseHandle(thr->thrH));
#elif defined(ATL_OMP_THREADS)
   fprintf(stderr, "Cannot call thread_join using OpenMP!!\n");
   ATL_assert(0);  /* should never enter this rout when using OMP */
#else
   ATL_assert(!pthread_join(thr->thrH, NULL));
#endif
   return(0);
}
@ROUT ATL_thread_exit
#include "atlas_misc.h"
#include "atlas_threads.h"
void ATL_thread_exit(void *retval)
{
#ifdef ATL_WINTHREADS
   ExitThread((DWORD)(retval));
#elif defined(ATL_OMP_THREADS)
   fprintf(stderr, "Cannot call thread_exit using OpenMP!!\n");
   ATL_assert(0);  /* should never enter this rout when using OMP */
#else
   pthread_exit(retval);
#endif
}
@ROUT ATL_log2tlaunch ATL_lin0tlaunch ATL_dyntlaunch ATL_goparallel
#ifndef ATL_NOAFFINITY
   #include "atlas_taffinity.h"  /* include this file first! */
#endif
#include "atlas_misc.h"
#include "atlas_threads.h"

@ROUT ATL_goparallel
#if !defined(ATL_NOAFFINITY) && defined(ATL_PAFF_SELF) && defined(ATL_USEOPENMP)
static int ATL_setmyaffinity()
@ROUT ATL_log2tlaunch ATL_lin0tlaunch ATL_dyntlaunch
#if !defined(ATL_NOAFFINITY) && defined(ATL_PAFF_SELF)
static int ATL_setmyaffinity(ATL_thread_t *me)
@ROUT ATL_log2tlaunch ATL_lin0tlaunch ATL_dyntlaunch ATL_goparallel
/*
 * Attempts to sets the affinity of an already-running thread.  The 
 * aff_set flag is set to true whether we succeed or not (no point in
 * trying multiple times).
 * RETURNS: 0 on success, non-zero error code on error
 */
{
@ROUT ATL_goparallel
   int bindID;
   bindID = omp_get_thread_num();
   #ifdef ATL_RANK_IS_PROCESSORID
      bindID = bindID % ATL_AFF_NUMID;
   #else
      bindID = ATL_affinityIDs[bindID%ATL_AFF_NUMID];
   #endif
@ROUT ATL_log2tlaunch ATL_lin0tlaunch ATL_dyntlaunch
   #ifdef ATL_RANK_IS_PROCESSORID
      const int bindID = me->rank % ATL_AFF_NUMID;
   #else
      const int bindID = ATL_affinityIDs[me->rank%ATL_AFF_NUMID];
   #endif
@ROUT ATL_log2tlaunch ATL_lin0tlaunch ATL_dyntlaunch ATL_goparallel
#ifdef ATL_PAFF_PLPA
   plpa_cpu_set_t cpuset;
   PLPA_CPU_ZERO(&cpuset);
   PLPA_CPU_SET(bindID, &cpuset);
   if (me->affID >= 0)
      return(0);
   me->affID = bindID;
   return(plpa_sched_setaffinity((pid_t)0, sizeof(cpuset), &cpuset));
#elif defined(ATL_PAFF_PBIND)
   return(processor_bind(P_LWPID, P_MYID, bindID, NULL));
#elif defined(ATL_PAFF_SCHED)
   cpu_set_t cpuset;
   CPU_ZERO(&cpuset);
   CPU_SET(bindID, &cpuset);
   if (me->affID >= 0)
      return(0);
   me->affID = bindID;
   return(sched_setaffinity(0, sizeof(cpuset), &cpuset));
#elif defined (ATL_PAFF_RUNON)
   if (me->affID >= 0)
      return(0);
   me->affID = bindID;
   return(pthread_setrunon_np(bindID));
#elif defined(ATL_PAFF_BINDP)
   if (me->affID >= 0)
      return(0);
   me->affID = bindID;
   return(bindprocessor(BINDTHREAD, thread_self(), bindID));
#elif defined(ATL_PAFF_CPUSET)  /* untried FreeBSD code */
   cpuset_t mycpuset;
   CPU_ZERO(&mycpuset);         /* no manpage, so guess works like linux */
   CPU_SET(bindID, &mycpuset);
   if (me->affID >= 0)
      return(0);
   me->affID = bindID;
   return(cpuset_setaffinity(CPU_LEVEL_WHICH, CPU_WHICH_TID, -1,
                             sizeof(mycpuset), &mycpuset));
#endif
   return(0);
}
#endif
@ROUT ATL_lin0tlaunch
#ifdef ATL_NOAFFINITY
   #define ATL_tDoWorkWrap ATL_tDoWorkWrap_noaff
#endif
void *ATL_tDoWorkWrap(void *vp)
{
   ATL_thread_t *tp = vp;
   ATL_LAUNCHSTRUCT_t *lp = tp->vp;
   lp->DoWork(lp, tp);
   return(NULL);
}

#if defined(ATL_TUNING) && defined(ATL_NOAFFINITY)
   #define ATL_lin0tlaunch ATL_lin0tlaunch_noaff
   #define ATL_thread_start ATL_thread_start_noaff
#endif
void *ATL_lin0tlaunch(void *vp)
{
   ATL_thread_t *tp = vp;
   ATL_LAUNCHSTRUCT_t *lp;
   const int P = tp->P;
   int i;
/*
 * Set my affinity if I haven't already
 */
   #ifdef ATL_PAFF_SELF
      if (tp->affID < 0)
          ATL_setmyaffinity(tp);
   #endif
/*
 * Spawn DoWork to all nodes
 */
   lp = tp->vp;
   for (i=1; i < P; i++)
   {
      ATL_thread_start(tp+i, i, 1, ATL_tDoWorkWrap, tp+i);
   }
/*
 * Thread 0 must also do the operation
 */
   lp->DoWork(lp, tp);
/*
 * Await completion of each task, and do combine (linear!) if requested
 */
   for (i=1; i < P; i++)
   {
      ATL_thread_join(tp+i);
      if (lp->DoComb)  /* do combine if necessary */
         lp->DoComb(lp->opstruct, 0, i);
   }
   return(NULL);
}
@ROUT ATL_dyntlaunch
#if defined(ATL_TUNING) && defined(ATL_NOAFFINITY)
void *ATL_dyntlaunch_noaff(void *vp)
#else
void *ATL_dyntlaunch(void *vp)
#endif
{
   ATL_thread_t *tp = vp, *btp;
   ATL_LAUNCHSTRUCT_t *lp;
   const int iam = tp->rank, P = tp->P;
   int i, src, dest, nthrP2, mask, abit;
   void *acnt;

   lp = tp->vp;
   acnt = lp->acounts[0];
   btp = tp - iam;
/*
 * Set my affinity if I haven't already
 */
   #ifdef ATL_PAFF_SELF
      if (tp->affID < 0)
          ATL_setmyaffinity(tp);
   #endif
   dest = ATL_DecGlobalAtomicCount(acnt, iam);
   while(dest)
   {
      dest = tp->P - dest;
      ATL_thread_start(btp+dest, dest, 0, ATL_dyntlaunch, btp+dest);
      dest = ATL_DecGlobalAtomicCount(acnt, iam);
   }
/*
 * Do the operation
 */
   lp->DoWork(lp, tp);
/*
 * Do combine in minimum spanning tree, combining results as required
 */
   for (i=0; (1<<i) < P; i++);
   nthrP2 = i;
   mask = 0;
   for (i=0; i < nthrP2; i++)
   {
      if (!(iam & mask))
      {
         abit = (1<<i);
         if (!(iam & abit))
         {
            src = iam ^ abit;
            if (src < P)
            {
               while (lp->chkin[src] != ATL_CHK_DONE_OP)
                  ATL_POLL;
               if (lp->DoComb)
                  lp->DoComb(lp->opstruct, iam, src);
            }
         }
         else
         {
            lp->chkin[iam] = ATL_CHK_DONE_OP;
            ATL_thread_exit(NULL);
         }
      }
      mask |= abit;
   }
@beginskip
/* 
 * This code does not work, because combine routs assume combine is always
 * right-to-left
 */
/*
 * Node 0 awaits completion of each task, and do combine (linear!) if requested
 * Later on, replace this code with pair-wise summup using AtomicCtrs=2
 */
   if (iam == 0)
   {
      int ndone=1;
      while (ndone < P)
      {
/*
 *       Find an uncombined completed result
 */
         for (i=1; i < P && lp->chkin[i] != ATL_CHK_DONE_OP; i++);
         if (i == P)
         {
            ATL_POLL;
            continue;
         }
/*
 *       Do the combine if necessary, and mark that thread as completely done
 */
         if (lp->DoComb)  /* do combine if necessary */
            lp->DoComb(lp->opstruct, iam, i);
         ndone++;
         lp->chkin[i] = ATL_CHK_DONE_COMB;
      }
   }
@endskip
   return(NULL);
}
@ROUT ATL_ranktlaunch_noaff
static void *ATL_GoToWork(ATL_thread_t *tp, ATL_LAUNCHSTRUCT_t *lp, int iam)
{
/*
 * Do the operation
 */
   lp->DoWork(lp, lp->opstruct+lp->opstructstride*iam);
/*
 * Node 0 awaits completion of each task, and do combine (linear!) if requested
 */
   if (iam == 0)
   {
      for (i=1; i < ATL_NTHREADS; i++)
      {
         if (!lp->OpStructIsInit || 
             lp->OpStructIsInit(lp->opstruct+i*lp->opstructstride))
         {
            ATL_thread_join(tp+i);
            if (lp->CombineOpStructs)  /* do combine if necessary */
               lp->CombineOpStructs(lp->opstruct,
                                    lp->opstruct+lp->opstructstride*i);
         }
      }
   }
   return(NULL);
}
static unsigned int ATL_coreID(void)
{

  int myRetn=-1;
  __asm__ __volatile__ ("\n"
    "movl $1, %%eax\n"
    "cpuid\n"
    "shrl $24, %%ebx\n"
    "movl %%ebx, %0\n"
    : "=m" (myRetn)
    :
    :"%rax", "%rbx"
    );
  
  return(myRetn);
} // end *** ATL_coreId ***
static void ATL_CreateThread(void *vp)
{
   pthread_attr_t attr;
   pthread_t pt;
   void *ATL_ranktlaunch_noaff(void *vp);

   ATL_assert(!pthread_attr_setdetachstate(&attr, PTHREAD_CREATE_DETACHED));
   pthread_attr_setscope(&attr, PTHREAD_SCOPE_SYSTEM); /* no chk, OK to fail */
   ATL_assert(!pthread_create(&pt, &attr, ATL_ranktlaunch_noaff, vp));
}

static void *ATL_KeepLaunching(ATL_thread_t tp, ATL_LAUNCHSTRUCT_t *lp)
{
   void *ranklock = lp->acounts[0];
   int i;
   do
   {
/*
 *    If all cores have gotten a thread, or if any thread has finished the
 *    problem, then stop launching and exit
 */
      if (lp->acounts[1])
         return(NULL);
      for (i=0; i < ATL_NTHREADS; i++)
      {
         if (lp->impdone[i] == -1)
            return(NULL);
      }
      iam = ATL_coreID();      /* may have changed if I've slept */
      if (!lp->rank2thr[iam])
      {
         int SUCCESS=0;
         ATL_mutex_lock(ranklock);
         if (!lp->rank2thr[iam])
         {
            int n;
            tp->rank = iam;
            lp->rank2thr[iam].thrH = pthread_self();
            for (i=0; i < ATL_NTHREADS && !lp->rank2thr[i].thrH; i++);
            if (i == ATL_NTHREADS)         /* if threads on all cores */
               lp->acounts[1] = (void*)1;  /* set flag saying launch complete */
            SUCCESS = 1;
         }
         ATL_mutex_unlock(ranklock);
         if (SUCCESS)
            return(ATL_GoToWork(tp, lp, iam));
      }
/*
 *    If there are still threads that need to run, spawn a new one, and go
 *    to sleep to yield to computation
 */
      ATL_CreateThread(tp);
      sched_yield();  /* go to sleep if no success */
   }
   while(1);
   return(NULL);
}
/*
 * This routine is for launching on platforms w/o affinity that have some
 * way for a running process to establish what core they are on.  Threads
 * are continually launched until there is one on each core, or the first
 * core runs out of work, whichever comes first.
 * Any process that is launched with this technique must not barrier,
 * since the number of cores is unknown, and the will certainly enter
 * the program at very different times.
 * NOTE: This routine calls pthreads directly, because ATLAS supports only
 *       Windows threads and pthreads, and Windows threads have affinity.
 */
void *ATL_ranktlaunch_noaff(void *vp)
{
   ATL_thread_t *tp = vp;
   ATL_LAUNCHSTRUCT_t *lp=tp->vp;

   return(ATL_KeepLaunching(vp, lp));

@ROUT ATL_log2tlaunch
#if defined(ATL_TUNING) && defined(ATL_NOAFFINITY)
void *ATL_log2tlaunch_noaff(void *vp)
#else
void *ATL_log2tlaunch(void *vp)
#endif
{
   ATL_thread_t *tp = vp, *btp;
   ATL_LAUNCHSTRUCT_t *lp;
   int i, iam, abit, mask, src, dest, nthrP2;
   const int P=tp->P;

   iam = tp->rank;
   for (i=0; (1<<i) < P; i++);
   nthrP2 = i;
/*
 * Set my affinity if I haven't already
 */
   #ifdef ATL_PAFF_SELF
      if (tp->affID < 0)
          ATL_setmyaffinity(tp);
   #endif
   btp = tp - iam;
   lp = tp->vp;
   mask = (1<<nthrP2) - 1;   /* no threads are in at beginning */
/*
 * Take log_2(NTHR) steps to do log_2 launch 
 */
   for (i=nthrP2-1; i >= 0; i--)
   {
      abit = (1<<i);
      mask ^= abit;   /* double the # of threads participating */
      if (!(iam & mask))
      {
         if (!(iam & abit))
         {
            dest = iam ^ abit;
            if ( dest < P)
               ATL_thread_start(btp+dest, dest, 1, ATL_log2tlaunch, btp+dest);
         }
      }
   }
   lp->DoWork(lp, tp);   /* do the operation */
/*
 * Join tree back up, combining results as required
 */
   mask = 0;
   for (i=0; i < nthrP2; i++)
   {
      if (!(iam & mask))
      {
         abit = (1<<i);
         if (!(iam & abit))
         {
            src = iam ^ abit;
            if (src < P)
            {
               ATL_thread_join(btp+src);
               if (lp->DoComb)
                  lp->DoComb(lp->opstruct, iam, src);
            }
         }
         else
            ATL_thread_exit(NULL);
      }
      mask |= abit;
   }
   return(NULL);
}
@ROUT ATL_goparallel
#if ATL_USE_THREADPOOL
/*
 * Started wt pthread_create on affID==0, this guy will create the
 * rest of the work queue
 */
void *ATL_threadpool_launch(void *vp)
{
   ATL_thread_t *tp = vp;
   ATL_tpool_t *pp = ATL_TP_PTR;
   const int P = pp->nthr;
   int i;
/*
 * Set my affinity if I haven't already
 */
   #ifdef ATL_PAFF_SELF
      if (tp->affID < 0)
          ATL_setmyaffinity(tp);
   #endif
   for (i=1; i < P; i++)
   {
      ATL_thread_start(tp+i, i, 0, ATL_threadpool, tp+i);
   }
/*
 * Thread 0 now participates in pool
 */
   ATL_threadpool(tp);
/*
 * When pool finished, await everyone to quit, then free the pool
 */
   while(pp->nthr)
      ATL_thread_yield();
   
   ATL_TP_PTR = NULL;
   ATL_FreeThreadPool(pp);
   if (ATL_TP1_PTR)
   {
      ATL_FreeThreadPool(ATL_TP1_PTR);
      ATL_TP1_PTR = NULL;
   }
}

@beginskip
ATL_tpool_t *ATL_TP_start(int P, ATL_LAUNCHSTRUCT_t *lp)
{
   ATL_tpool_t *pp;
   ATL_thread_t *tp;
   int i;

   pp = calloc(1, sizeof(ATL_tpool_t));
   ATL_assert(pp);
   tp = pp->threads = malloc(P*sizeof(ATL_thread_t));
   pp->icomm = calloc(P, sizeof(int));
   ATL_assert(tp && pp->icomm);
   for (i=0; i < P; i++)
   {
        tp[i].vp = lp;
        tp[i].rank = i;
        tp[i].P = P;
   }
   lp->vp = pp;
   #ifdef DEBUG
     fprintf(stderr, "MASTER LAUNCHING\n");
   #endif
   pp->tpmut = ATL_mutex_init();
   pp->mcond = ATL_cond_init();
   ATL_mutex_lock(pp->tpmut);
   ATL_thread_start(tp, 0, 0, ATL_threadpool_launch, tp);
   do
   {
      #ifdef DEBUG
          fprintf(stderr, "MASTER SLEEPING WD=%u\n", pp->WORKDONE);
      #endif
      ATL_cond_wait(pp->mcond, pp->tpmut);
      #ifdef DEBUG
          fprintf(stderr, "MASTER SLEEPING WD=%u\n", pp->WORKDONE);
      #endif
   }
   while (!pp->WORKDONE);
   pp->nworkers = 0;
   ATL_mutex_unlock(pp->tpmut);
   return(pp);
}
@endskip
#endif
#if defined(ATL_TUNING) && defined(ATL_NOAFFINITY)
   void *ATL_log2tlaunch_noaff(void *vp);
   #define ATL_goparallel ATL_goparallel_noaff
   #define ATL_dyntlaunch ATL_log2tlaunch_noaff
   #define ATL_USE_DYNAMIC 0
#elif defined(ATL_TUNING)
   #if defined(ATL_LAUNCH_LINEAR)
      #define ATL_goparallel ATL_goparallel_lin
      #define ATL_dyntlaunch ATL_lin0tlaunch
      #define ATL_USE_DYNAMIC 0
   #elif defined(ATL_LAUNCH_DYNAMIC)
      #define ATL_goparallel ATL_goparallel_dyn
      #define ATL_USE_DYNAMIC 1
   #else
      #ifdef ATL_LAUNCH_LOG2
         #define ATL_goparallel ATL_goparallel_log2
      #endif
      #define ATL_dyntlaunch ATL_log2tlaunch
      #define ATL_USE_DYNAMIC 0
   #endif
#else
   #define ATL_USE_DYNAMIC 1
#endif
void ATL_goparallel
/*
 * This function is used when you pass a single opstruct to all threads;  
 * In this case, we stash opstruct in launchstruct's vp, and then use the
 * rank array as opstruct during the spawn.  Therefore, these routines
 * should expect to get their problem def from ls.vp, and their rank from
 * the second argument.  The DoWork function is the function that should
 * be called from each thread to do the parallel work.  This function should
 * look like:
 * void DoWork_example(ATL_LAUNCHSTRUCT_t *lp, void *vp)
 * {
 *    ATL_thread_t *tp = vp;
 *    const int myrank = tp->rank;
 *    my_prob_def_t *pd = lp->opstruct;
 *    ... do work based on info in struct pointed to by lp->opstruct ...
 * }
 * Your DoWork should perform any needed combine before finishing execution,
 * and any return values can be passed in the problem definition structure
 * that you define.
 */
(
   const unsigned int P, /* # of cores to use */
   void *DoWork,         /* func ptr to work function */
   void *opstruct,       /* structure giving tasks to threads */
   void *DoComb          /* function to combine two opstructs */
)
{
   ATL_thread_t *tp;
   int *chkin;
   void *vp, *lc;
   int i;
   ATL_LAUNCHSTRUCT_t ls;

   ls.OpStructIsInit = NULL;
   ls.DoWork = DoWork;
   ls.DoComb = DoComb;
   ls.opstruct = opstruct;
#ifdef ATL_OMP_THREADS
   tp = malloc(sizeof(ATL_thread_t)*P);
   ATL_assert(tp);
   for (i=0; i < P; i++)
   {
      tp[i].vp = &ls;
      tp[i].rank = i;
      tp[i].P = P;
   }
   ls.rank2thr = tp;
   omp_set_num_threads(P);
   #pragma omp parallel
   {
/*
 *    Make sure we got the requested nodes, and set affinity if supported
 */
      ATL_assert(omp_get_num_threads() == P);
      #ifdef ATL_PAFF_SELF
         ATL_setmyaffinity();
      #endif
      i = omp_get_thread_num();
      ls.DoWork(&ls, tp+i);
   }
/*
 * Do combine (linear) if requested
 */
   if (DoComb)
      for (i=1; i < P; i++)
         ls.DoComb(ls.opstruct, 0, i);
/*
 * If using threadpool, use invoke the newer goParallel with old-skool wrappers
 */
#elif ATL_USE_THREADPOOL
   if (DoComb)
      ATL_goParallel(P, ATL_oldjobwrap, ATL_oldcombwrap, &ls, NULL);
   else
      ATL_goParallel(P, ATL_oldjobwrap, NULL, &ls, NULL);
@beginskip
   ATL_tpool_t *pp;
   int n, *ip;
   int JUST_STARTED=0;
/*
 * If thread pool not currently active, must spawn it. 
 */
   if (!ATL_TP_PTR)
   {
      if (ATL_IsFirstThreadedCall())
      {
         JUST_STARTED = 1;
         ATL_InitThreadPoolStartup(P, void *pd, void *extra)
      }
   }
   pp = ATL_TP_PTR;
   if (!JUST_STARTED)
   {
/*
 *    Tell the threads the job is in this launchstruct, wake them up
 */
      ATL_mutex_lock(ATL_TP_MUT);
      ATL_mutex_lock(pp->tpmut);
      tp = pp->threads;
      pp->PD = &ls;
      pp->jobf = ATL_oldjobwrap;
      if (DoComb)
      {
         pp->combf = ATL_oldcombwrap;
         ATL_UnsetAllBitsBV(pp->combReadyBV);
         ATL_UnsetAllBitsBV(pp->combDoneBV);
      }
      pp->nworkers = P;
      pp->nwdone = pp->wcnt = 0;
      pp->WORKDONE = pp->NOWORK = 0;
      #ifdef DEBUG
        fprintf(stderr, "MASTER WAKES WORKERS\n");
      #endif
      #ifdef ATL_TP_FORCEBCAST
         #if (ATL_TP_FORCEBCAST)
            ATL_TPF_UNSET_ZEROWAKES(pp);
         #else
            ATL_TPF_SET_ZEROWAKES(pp);
         #endif
      #else
         if (P == pp->nthr)
            ATL_TPF_UNSET_ZEROWAKES(pp);
         else
            ATL_TPF_SET_ZEROWAKES(pp);
      #endif
   
      if (ATL_TPF_ZEROWAKES(pp))
         ATL_cond_signal(ATL_TP_WCOND);
      else
         ATL_cond_bcast(ATL_TP_WCOND);
   }
/*
 * Threads have started, so this routine awaits completion of task
 */
   do
   {
      ATL_mutex_unlock(pp->tpmut);   /* allow threads to mess wt tpool */
      ATL_cond_wait(ATL_TP_MCOND, ATL_TP_MUT);
      ATL_mutex_lock(pp->tpmut);   /* allow threads to mess wt tpool */
   }
   while (!pp->WORKDONE);
   pp->nworkers = 0;     /* make sure spurious wakeup will go back to sleep */
   ATL_mutex_unlock(pp->tpmut);
   ATL_mutex_unlock(ATL_TP_MUT);
/*
 * For first launch, it is critical that we ensure all threads have started
 * before returning, to avoid threads awake when they shouldn't be
 */
   if (JUST_STARTED)
   {
      while(pp->nthr != ATL_NTHREADS)
         ATL_thread_yield();
   }
   
@endskip
/*
 * Otherwise, I'm using explicit launch & join paradigm of some type
 */
#else
   #if ATL_USE_DYNAMIC
      ls.acounts = &lc;
      ls.acounts[0] = ATL_SetGlobalAtomicCount(P>>1, P-1, 0);
      vp = malloc(P*(sizeof(ATL_thread_t)+sizeof(int)) + ATL_Cachelen);
      ATL_assert(vp);
      chkin = vp;
      tp = (ATL_thread_t*)(chkin+P);
      tp = ATL_AlignPtr(tp);
   #else
      vp = malloc(P*(sizeof(ATL_thread_t)) + ATL_Cachelen);
      tp = ATL_AlignPtr(vp);
   #endif
   ls.rank2thr = tp;

   for (i=0; i < P; i++)
   {
      tp[i].vp = &ls;
      tp[i].rank = i;
      tp[i].P = P;
      #if ATL_USE_DYNAMIC
         chkin[i] = 0;
      #endif
   }
   #if ATL_USE_DYNAMIC
      ls.chkin = (volatile int*) chkin;
   #endif
   ATL_thread_start(tp, 0, 1, ATL_dyntlaunch, tp);
   ATL_thread_join(tp);
   #if ATL_USE_DYNAMIC
      ATL_FreeGlobalAtomicCount(ls.acounts[0]);
   #endif
   free(vp);
#endif
}
@ROUT ATL_goparallel_prank
#include "atlas_misc.h"
#include "atlas_threads.h"
#if defined(ATL_GAS_x8664) || defined(ATL_GAS_x8632)
   #define ATL_HAS_COREID
static unsigned int ATL_coreID(void)
{

  int myRetn=-1;
  __asm__ __volatile__ ("\n"
    "movl $1, %%eax\n"
    "cpuid\n"
    "shrl $24, %%ebx\n"
    "movl %%ebx, %0\n"
    : "=m" (myRetn)
    :
#if defined(ATL_GAS_x8632)
    :"%eax", "%ebx", "%edx", "%ecx"
#elif defined(ATL_GAS_x8664)
    :"%rax", "%rbx", "%rdx", "%rcx"
#endif
    );
  return(myRetn);
}
#else
   #define ATL_HAS_COREID
   #define _GNU_SOURCE 1
   #define __USE_GNU   1
   #include <sched.h>
   #define ATL_coreID sched_getcpu
#endif
#ifndef ATL_HAS_COREID
void ATL_goparallel_prank
(
   const unsigned int P, /* # of cores to use */
   void *DoWork,         /* func ptr to work function */
   void *opstruct,       /* will be stashed in launchstruct's vp */
   void *DoComb
)
{
   fprintf(stderr, "Hey chief, you are screwed:\n");
   fprintf(stderr, 
      "  someone called goparallel_ptrank wt no way to determine prank!\n");
   exit(-1);
}
#else

typedef struct
{
   volatile int *coreIDs;   /* NTHR-len array providing non-unique coreIDs */
   volatile int *thrrnks;   /* NP-len array of chosen thread ranks */
   void *Tcnt;              /* atomic counter of # of threads launched */
   void *Trankcnt;          /* atomic ctr providing thread rank */
   int NT, NP;              /* # of threads & processors */
   int NLC;                 /* # of local cntrs in global acnts Tcnt/Trankcnt */
   pthread_attr_t attr;     /* attribute for pthread_create */
   ATL_LAUNCHSTRUCT_t *lp;  /* to pretend we've been launched normally */
} ATL_ranklaunch_t;


/*
 * Selects an ID from list A which does not appear in C
 * RETURNS: first unique ID, or -1 if no such ID found
 */
static int GetUniqueID
(
   int Na,   /* number of accepted unique IDs in U */
   int *A,   /* list of accepted unique IDs found so far */
   int Nc,   /* number of candidates left */
   int *C    /* non-unique candidates */
)
{
   int ic, ia;
   for (ic=0; ic < Nc; ic++)
   {
      for (ia=0; ia < Na && A[ia] != C[ic]; ia++);
      if (ia == Na)  /* found unique one */
         return(ic);
   }
   return(-1);
}

void *ATL_DoRankLaunch(void *vp)
{
   ATL_ranklaunch_t *rl = vp;
   ATL_LAUNCHSTRUCT_t *lp = rl->lp;
   const int T = rl->NT, P = rl->NP;
   int trank;   /* thread rank between 0 and NT-1 */
   int prank;   /* processor rank between 0 and NP-1 */
   int i, coreID;
   pthread_t pt;
/*
 * Cooperate with master to launch NT threads
 */
   coreID = ATL_coreID();
   trank = coreID % rl->NLC;
   #ifdef ATL_GLOBAL
   while(ATL_DecGlobalAtomicCount(rl->Tcnt, trank))
   #else
   while(ATL_DecAtomicCount(rl->Tcnt))
   #endif
      pthread_create(&pt, &rl->attr, ATL_DoRankLaunch, rl);
/*
 * Get my coreID, and tell master I'm alive by writing it to coreID array
 */
   #ifdef ATL_GLOBAL
      trank = T - ATL_DecGlobalAtomicCount(rl->Trankcnt, trank);
   #else
      trank = T - ATL_DecAtomicCount(rl->Trankcnt);
   #endif
   rl->coreIDs[trank] = coreID;
/*
 * Wait on master to signal ranking have been established
 */
   while(rl->coreIDs[0] == -1)
      ATL_thread_yield();
/* 
 * See if my core has been selected for survival
 */
   for (i=0; i < P; i++)
      if (rl->thrrnks[i] == trank)
         break;
/*
 * If I'm not in worker list, signal completion by writing -2 to coreID array,
 * and quit
 */
   if (i == P)
   {
      rl->coreIDs[trank] = -2;  /* signal thread has completed */
      pthread_exit(NULL);
   }
/* 
 * i is actually now my processor rank, fill my thread info in
 */
   prank = i;
/*   lp->rank2thr[prank].thrH = pthread_self(); */ /* don't need this */
   lp->rank2thr[prank].rank = prank;

   lp->DoWork(lp, lp->rank2thr+prank);  /* do work */

   rl->coreIDs[trank] = -2 ;    /* signal thread completion for master */
}

/*
 * This function is used when we don't have affinity, but do have some way
 * to determine the coreID, which must be a unique non-negative int.
 * It will launch 4*P threads in a detached state; all those threads that
 * start on unique cores will work on the problem, as will some on non-unique
 * cores that are necessary to get P threads out of the 4P created
 */
void ATL_goparallel_prank
(
   const unsigned int P, /* # of cores to use */
   void *DoWork,         /* func ptr to work function */
   void *opstruct,       /* will be stashed in launchstruct's vp */
   void *DoComb
)
{
   ATL_LAUNCHSTRUCT_t ls;
   ATL_ranklaunch_t rl;
   int T, t, i, j, prank, nunique, coreID;
   int *uids;    /* unique coreIDs */
   volatile int *coreIDs;
   pthread_t pt;
   pthread_attr_t *attr;
   void *vp;

   attr = &rl.attr;
   ls.DoWork = DoWork;
   ls.vp = opstruct;
   ls.DoComb = NULL;
@skip   ls.CombineOpStructs = NULL;
@skip   ls.opstructstride = 0;
   ls.chkin = NULL;
   ls.acounts = NULL;
   T = (P >= 8) ? P<<2 : P+P;
   rl.NT = T;
   rl.NP = P;
   #ifdef ATL_GLOBAL
      rl.NLC = P;
      rl.Tcnt = ATL_SetGlobalAtomicCount(rl.NLC, T-1, 0);
      rl.Trankcnt = ATL_SetGlobalAtomicCount(rl.NLC, T-1, 0);
   #else
      rl.NLC = T >> 2;
      rl.Tcnt = ATL_SetAtomicCount(T-1);
      rl.Trankcnt = ATL_SetAtomicCount(T-1);
   #endif
   rl.lp = &ls;
   uids = malloc(ATL_Cachelen+sizeof(int)*(T+P+P)+sizeof(ATL_thread_t)*P);
   ATL_assert(uids);
   coreIDs = rl.coreIDs = (volatile int*) (uids + P);
   rl.thrrnks = (volatile int*) (rl.coreIDs + T);
   vp = (void*) (rl.thrrnks + P);
   ls.rank2thr = ATL_AlignPtr(vp);
/*
 * Initialize attribute: detached with system scope 
 */
   ATL_assert(!pthread_attr_init(attr));
   ATL_assert(!pthread_attr_setdetachstate(attr,PTHREAD_CREATE_DETACHED));
   pthread_attr_setscope(attr, PTHREAD_SCOPE_SYSTEM); /* no chk, OK to fail */
/*
 * Initialize rank arrays with -1; negative #s are codes, -1 means not init,
 * -2: started and then died.
 */
   for (i=0; i < P; i++)
      coreIDs[i] = rl.thrrnks[i] = -1;
   for (i=P; i < T; i++)
      coreIDs[i] = -1;
/*
 * Cooperate with worker threads to spawn all T threads
 */
   #ifdef ATL_GLOBAL
      while(ATL_DecGlobalAtomicCount(rl.Tcnt, 0))
   #else
      while(ATL_DecAtomicCount(rl.Tcnt))
   #endif
         pthread_create(&pt, attr, ATL_DoRankLaunch, &rl);
/*
 * Wait for all created threads to checkin; worker threads will all write
 * their coreID to their entry in the coreIDs array.  Their index in this array
 * is therefore their thread rank, which everyone agrees on due to using
 * the atomic counter.
 */
   coreID = ATL_coreID();  /* get my core ID */
   for (i=1; i < T; i++)
      while(coreIDs[i] == -1)
         ATL_thread_yield();
/*
 * All workers are spinning on coreIDs[0] awaiting my OK, so it is safe
 * to build all processor/thread ranking arrays 
 */
   uids[0] = coreID;
   rl.thrrnks[0] = 0;   /* master is always first worker */
   for (i=1; i < P; i++)
   {
      j = GetUniqueID(i, uids, T-1, (int*)coreIDs+1) + 1;
      if (!j)
        break;
      rl.thrrnks[i] = j;
      uids[i] = coreIDs[j];
   }
   nunique = i;
/*
 * We didn't get P unique coreIDs, so choose some coreIDs to get extra threads
 * Try to map all extra threads to same cores as much as possible, to make
 * it more likely OS gets off its ass and reschedules; also, since we are
 * using dynamically scheduled ops, only a few processors will be running
 * at low speeds.
 */
   while (i < P)
   {
      int k;
      for (k=0; k < i; k++)
      {
         for (j=0; j < T; j++)
            if (coreIDs[j] == uids[k])
               break;
         if (j < T)
         {
            rl.thrrnks[i] = j;
            uids[i] = coreIDs[j];
            i++;
            break;
         }
      }
   }
/*
 * Signal to workers that thread mapping is complete, then do my portion of work
 */
   coreIDs[0] = coreID;
   ls.rank2thr[0].rank = 0;
   ls.DoWork(&ls, ls.rank2thr);  /* do work */
/*
 * I could have freed these resources after first checkin, but have delayed
 * until now to avoid possible context switch due to system call.  Free some
 * resources I'm no longer using
 */
   ATL_assert(!pthread_attr_destroy(attr));  /* spawning complete, release */
   #ifdef ATL_GLOBAL
      ATL_FreeGlobalAtomicCount(rl.Tcnt);
      ATL_FreeGlobalAtomicCount(rl.Trankcnt);
   #else
      ATL_FreeAtomicCount(rl.Tcnt);
      ATL_FreeAtomicCount(rl.Trankcnt);
   #endif
/*
 * Wait for all threads to complete before returning
 */
   if (nunique < P)
      printf("Node 0 awaits completion on %d unique cores; P=%d\n", nunique, P);
   for (i=1; i < T; i++)
      while(coreIDs[i] != -2)
        ATL_thread_yield();
   free(uids);
}
#endif
@ROUT ATL_Xtgemm
#include "atlas_misc.h"
@skip #define ATL_LAUNCHORDER         /* we want static ATL_launchorder array */
#include "atlas_threads.h"
#include "atlas_tlvl3.h"
/* 
 * =========================================================================
 * This file contains support routines for TGEMM that are not type-dependent
 * =========================================================================
 */
@ROUT ATL_thrdecompMM ATL_Xtgemm
int ATL_thrdecompMM_rMN
   (ATL_TMMNODE_t *ptmms, const enum ATLAS_TRANS TA, const enum ATLAS_TRANS TB,
    ATL_CINT Mblks, const int mr, ATL_CINT Nblks, const int nr, ATL_CINT Kblks,
    const int kr, const void *A, ATL_INT lda, const void *B, const ATL_INT ldb,
    const void *C, ATL_CINT ldc, const int P, const int indx, const int COPYC)
/*
 * This routine recursively splits the M & N dimensions over P processors
 */
{
   int pR, pL, rL, rR, np, eltsh;
   ATL_INT nblksL, nblksR, j;
   size_t i;
   double d;

/*
 * Choose to split either M or N.  Want M < N always, so require
 * N to be twice as big before splitting (or be out of M blocks)
 */
   if (P > 1 && Nblks > 1 && (Mblks < 2 || Nblks >= Mblks+Mblks))
   {
      eltsh = ptmms[indx].eltsh;
      pR = P>>1;    /* on right, take P/2 threads */
      pL = P - pR;  /* on left, take remaining threads */
      d = (pR == pL) ? 0.5 : ((double)pL)/((double)P);    /* percent on left */
      #ifdef DEBUG
         fprintf(stderr, "Cut N\n");
      #endif
      nblksL = (d * Nblks);
      nblksR = Nblks - nblksL;
      if (nblksR < nblksL)
      {
         rL = 0;
         rR = nr;
      }
      else
      {
         rL = nr;
         rR = 0;
      }
      i = (nblksL*ptmms[indx].nb+rL) << eltsh;
      np = ATL_thrdecompMM_rMN(ptmms, TA, TB, Mblks, mr, nblksL, rL, Kblks, kr, 
                               A, lda, B, ldb, C, ldc, pL, indx, COPYC);
      np += ATL_thrdecompMM_rMN(ptmms, TA, TB, Mblks, mr, nblksR, rR, Kblks, kr,
                                A,  lda, (TB == AtlasNoTrans) ? 
                                MindxT(B,i*ldb) : MindxT(B,i), ldb, 
                                MindxT(C,i*ldc), ldc, pR, indx+pL, COPYC);
      return(np);
   }
/*
 * If we have failed to split N, split M if possible
 */
   if (P > 1 && Mblks > 1)
   {
      eltsh = ptmms[indx].eltsh;
      pR = P>>1;    /* on right, take P/2 threads */
      pL = P - pR;  /* on left, take remaining threads */
      d = (pR == pL) ? 0.5 : ((double)pL)/((double)P);    /* percent on left */
      #ifdef DEBUG
         fprintf(stderr, "Cut M\n");
      #endif
      nblksL = (d * Mblks);
      nblksR = Mblks - nblksL;
      if (nblksR < nblksL)
      {
         rL = 0;
         rR = mr;
      }
      else
      {
         rL = mr;
         rR = 0;
      }
      i = (nblksL*ptmms[indx].mb+rL) << eltsh;
      np = ATL_thrdecompMM_rMN(ptmms, TA, TB, nblksL, rL, Nblks, nr, Kblks, kr, 
                               A, lda, B, ldb, C, ldc, pL, indx, COPYC);
      np += ATL_thrdecompMM_rMN(ptmms, TA, TB, nblksR, rR, Nblks, nr, Kblks, kr,
                                (TA==AtlasNoTrans)?MindxT(A,i):MindxT(A,i*lda), 
                                lda, B, ldb, MindxT(C,i), ldc, pR, indx+pL, 
                                COPYC);
      return(np);
   }
/*
 * If no desirable splitting possible, stop recursion
 */
   ptmms[indx].A = A;
   ptmms[indx].B = B;
   ptmms[indx].C = (void*)C;
   ptmms[indx].lda = lda;
   ptmms[indx].ldb = ldb;
   ptmms[indx].ldc = ldc;
   ptmms[indx].M = ptmms[indx].mb*Mblks + mr;
   ptmms[indx].N = ptmms[indx].nb*Nblks + nr;
   ptmms[indx].K = ptmms[indx].kb*Kblks + kr;
   ptmms[indx].ldcw = ptmms[indx].nCw = 0;
   ptmms[indx].nCp = ptmms[indx].ownC = 1;
   ptmms[indx].Cinfp[ATL_NTHREADS-1] = ptmms+indx;
   ptmms[indx].Cw = NULL;
   #ifdef DEBUG
      fprintf(stderr, "%d: M=%d, N=%d, K=%d, ownC=%d, nCp=%d, nCw=%d\n",
              indx, ptmms[indx].M, ptmms[indx].N, ptmms[indx].K, 
              ptmms[indx].ownC, ptmms[indx].nCp, ptmms[indx].nCw);
   #endif
   return(1);
}

int ATL_thrdecompMM_rMNK
   (ATL_TMMNODE_t *ptmms, const enum ATLAS_TRANS TA, const enum ATLAS_TRANS TB,
    ATL_CINT Mblks, const int mr, ATL_CINT Nblks, const int nr, ATL_CINT Kblks,
    const int kr, const void *A, ATL_INT lda, const void *B, const ATL_INT ldb,
    const void *C, ATL_CINT ldc, const int P, const int indx, const int COPYC)
/*
 * This routine decomposes the GEMM over P processors by splitting any of
 * the dimensions.  We only call this routine when K is very large or
 * M and N are very small (and thus splitting K, with its associated
 * extra workspace and flops, makes sense).
 */
{
   int pR, pL, rL, rR, np, eltsh;
   ATL_INT nblksL, nblksR, j;
   size_t i;
   double d;

   eltsh = ptmms[indx].eltsh;
#ifdef DEBUG
   ATL_assert(P > 0);
#endif
   if (P <= 1 || (Mblks <= 1 && Nblks <= 1 && Kblks <= 1))
      goto STOP_REC;
   pR = P>>1;    /* on right, take P/2 threads */
   pL = P - pR;  /* on left, take remaining threads */
   d = (pR == pL) ? 0.5 : ((double)pL)/((double)P);    /* percent on left */
/*
 * Do not consider cutting K unless we have some K blocks, and we have either
 * already done so, or if we are sure that we are within our workspace limit
 */
   if (Kblks > 1 && (COPYC || 
       ((Mblks*ptmms[indx].mb+mr) * ((Nblks*ptmms[indx].nb+nr)<<eltsh)
        < ATL_PTMAXMALLOC)))
   {
/*
 *    Before splitting K, ask that we are out of M and N blocks, or that
 *    our K is 4 times M and twice N
 */
      if ( (Mblks < 2 && Nblks < 2) ||
           (Kblks > (Mblks<<2) && Kblks > (Nblks+Nblks)) )
      {
         #ifdef DEBUG
            fprintf(stderr, "Cut K\n");
         #endif
         nblksL = (d * Kblks);
         nblksR = Kblks - nblksL;
         if (nblksR < nblksL)
         {
            rL = 0;
            rR = kr;
         }
         else
         {
            rL = kr;
            rR = 0;
         }
         i = (nblksL*ptmms[indx].kb + rL)<<eltsh;
         np = ATL_thrdecompMM_rMNK(ptmms, TA, TB, Mblks, mr, Nblks, nr, 
                                   nblksL, rL, A, lda, B, ldb, C, ldc, pL, 
                                   indx, COPYC);
         np += ATL_thrdecompMM_rMNK(ptmms, TA, TB, Mblks, mr, Nblks, nr, 
                                    nblksR, rR, (TA==AtlasNoTrans)?
                                    MindxT(A,lda*i):MindxT(A,i), lda, 
                                    (TB == AtlasNoTrans)?MindxT(B,i):
                                    MindxT(B,i*ldb), ldb, C, ldc, pR, 
                                    indx+pL, 1);
         return(np);
      }
   }
/*
 * Now choose to split either M or N.  Want M < N always, so require
 * N to be twice as big before splitting
 */
   if (Nblks > 1 && (Mblks < 2 || Nblks >= Mblks+Mblks))
   {
      #ifdef DEBUG
         fprintf(stderr, "Cut N\n");
      #endif
      nblksL = (d * Nblks);
      nblksR = Nblks - nblksL;
      if (nblksR < nblksL)
      {
         rL = 0;
         rR = nr;
      }
      else
      {
         rL = nr;
         rR = 0;
      }
      i = (nblksL*ptmms[indx].nb+rL) << eltsh;
      np = ATL_thrdecompMM_rMNK(ptmms, TA, TB, Mblks, mr, nblksL, rL, Kblks, kr,
                                A, lda, B, ldb, C, ldc, pL, indx, COPYC);
      np += ATL_thrdecompMM_rMNK(ptmms, TA, TB, Mblks, mr, nblksR, rR, 
                                 Kblks, kr, A,  lda, (TB == AtlasNoTrans) ? 
                                 MindxT(B,i*ldb) : MindxT(B,i), ldb, 
                                 MindxT(C,i*ldc), ldc, pR, indx+pL, COPYC);
      return(np);
   }
/*
 * If we have failed to split N or K, split M if possible
 */
   if (Mblks > 1)
   {
      #ifdef DEBUG
         fprintf(stderr, "Cut M\n");
      #endif
      nblksL = (d * Mblks);
      nblksR = Mblks - nblksL;
      if (nblksR < nblksL)
      {
         rL = 0;
         rR = mr;
      }
      else
      {
         rL = mr;
         rR = 0;
      }
      i = (nblksL*ptmms[indx].mb+rL) << eltsh;
      np = ATL_thrdecompMM_rMNK(ptmms, TA, TB, nblksL, rL, Nblks, nr, Kblks, kr,
                                A, lda, B, ldb, C, ldc, pL, indx, COPYC);
      np += ATL_thrdecompMM_rMNK(ptmms, TA, TB, nblksR, rR, Nblks, nr, 
                                 Kblks, kr,
                                 (TA==AtlasNoTrans)?MindxT(A,i):MindxT(A,i*lda),
                                 lda, B, ldb, MindxT(C,i), ldc, pR, indx+pL, 
                                 COPYC);
      return(np);
   }
/*
 * If no desirable splitting possible, stop recursion
 */
STOP_REC:
   ptmms[indx].A = A;
   ptmms[indx].B = B;
   ptmms[indx].C = (void*)C;
   ptmms[indx].lda = lda;
   ptmms[indx].ldb = ldb;
   ptmms[indx].ldc = ldc;
   ptmms[indx].M = ptmms[indx].mb*Mblks + mr;
   ptmms[indx].N = ptmms[indx].nb*Nblks + nr;
   ptmms[indx].K = ptmms[indx].kb*Kblks + kr;
   if (COPYC)
   {
      ptmms[indx].nCw = 1;
      ptmms[indx].nCp = ptmms[indx].ownC = 0;
      ptmms[indx].Cinfp[0] = ptmms+indx;
/*
 *    Make ldcw a multiple of 4 that is not a power of 2
 */
      i = ((ptmms[indx].M + 3)>>2)<<2;
      if (!(i & (i-1)))
         i += 4;
      ptmms[indx].ldcw = i;
   }
   else
   {
      ptmms[indx].ldcw = ptmms[indx].nCw = 0;
      ptmms[indx].nCp = ptmms[indx].ownC = 1;
      ptmms[indx].Cinfp[ATL_NTHREADS-1] = ptmms+indx;
   }
   ptmms[indx].Cw = NULL;
   #ifdef DEBUG
      fprintf(stderr, "%d: M=%d, N=%d, K=%d, ownC=%d, nCp=%d, nCw=%d\n",
              indx, ptmms[indx].M, ptmms[indx].N, ptmms[indx].K, 
              ptmms[indx].ownC, ptmms[indx].nCp, ptmms[indx].nCw);
   #endif
   return(1);

}
@beginskip
int ATL_thrdecompMM_rec
   (ATL_TMMNODE_t *ptmms, const enum ATLAS_TRANS TA, const enum ATLAS_TRANS TB,
    ATL_CINT Mblks, const int mr, ATL_CINT Nblks, const int nr, ATL_CINT Kblks,
    const int kr, const void *A, ATL_INT lda, const void *B, const ATL_INT ldb,
    const void *C, ATL_CINT ldc, const int P, const int indx, const int COPYC)
{
   int pR, pL, rL, rR, np, eltsh;
   ATL_INT nblksL, nblksR, j;
   size_t i;
   double d;

#ifdef DEBUG
   ATL_assert(P > 0);
#endif
/*
 * End recursion if we are down to 1 processor, or if we are out of blocks
 */
   if ( P <= 1 || (Mblks <= 1 && Nblks <= 1 && Kblks <= 2) )
   {
      ptmms[indx].A = A;
      ptmms[indx].B = B;
      ptmms[indx].C = (void*)C;
      ptmms[indx].lda = lda;
      ptmms[indx].ldb = ldb;
      ptmms[indx].ldc = ldc;
      ptmms[indx].M = ptmms[indx].mb*Mblks + mr;
      ptmms[indx].N = ptmms[indx].nb*Nblks + nr;
      ptmms[indx].K = ptmms[indx].kb*Kblks + kr;
      if (COPYC)
      {
         ptmms[indx].nCw = 1;
         ptmms[indx].nCp = ptmms[indx].ownC = 0;
         ptmms[indx].Cinfp[0] = ptmms+indx;
/*
 *       Make ldcw a multiple of 4 that is not a power of 2
 */
         i = ((ptmms[indx].M + 3)>>2)<<2;
         if (!(i & (i-1)))
            i += 4;
@beginskip
         for (j=0; j < sizeof(ATL_INT)*8-1; j++)
         {
            if (!((1<<j)^i))
            {
               i += 4;
               break;
            }
         }
@endskip
         ptmms[indx].ldcw = i;
      }
      else
      {
         ptmms[indx].ldcw = ptmms[indx].nCw = 0;
         ptmms[indx].nCp = ptmms[indx].ownC = 1;
         ptmms[indx].Cinfp[ATL_NTHREADS-1] = ptmms+indx;
      }
      ptmms[indx].Cw = NULL;
#ifdef DEBUG
fprintf(stderr, "%d: M=%d, N=%d, K=%d, ownC=%d, nCp=%d, nCw=%d\n",
        indx, ptmms[indx].M, ptmms[indx].N, ptmms[indx].K, 
        ptmms[indx].ownC, ptmms[indx].nCp, ptmms[indx].nCw);
#endif
      return(1);
   }

   pR = P>>1;    /* on right, take P/2 threads */
   pL = P - pR;  /* on left, take remaining threads */
   d = (pR == pL) ? 0.5 : ((double)pL)/((double)P);    /* percent on left */

/*
 * Only cut K if it dominates M & N (here we say K must be 4 time larger)
 * and M&N are small enough that we can afford to malloc C 
 * (here we say C workspace must be 16MB or less) 
 */
   if ( ( ((Mblks < 2) && Nblks < 2) ||
          (((Kblks>>2) > Mblks) && ((Kblks>>2) > Nblks)) )
       && (Mblks*((Nblks*ptmms[indx].mb*(ptmms[indx].nb<<ptmms[indx].eltsh)
           +1023)>>10) <= 16*1024))
   {
#ifdef DEBUG
   fprintf(stderr, "Cut K\n");
#endif
      nblksL = (d * Kblks);
      nblksR = Kblks - nblksL;
      if (nblksR < nblksL)
      {
         rL = 0;
         rR = kr;
      }
      else
      {
         rL = kr;
         rR = 0;
      }
      i = (nblksL*ptmms[indx].kb + rL)<<eltsh;
      np = ATL_thrdecompMM_rec(ptmms, TA, TB, Mblks, mr, Nblks, nr, nblksL, rL,
                               A, lda, B, ldb, C, ldc, pL, indx, COPYC);
      np += ATL_thrdecompMM_rec(ptmms, TA, TB, Mblks, mr, Nblks, nr, nblksR, rR,
                                (TA==AtlasNoTrans)?MindxT(A,lda*i):MindxT(A,i), 
                                lda, 
                               (TB == AtlasNoTrans)?MindxT(B,i):MindxT(B,i*ldb),
                               ldb, C, ldc, pR, indx+pL, 1);
      return(np);
   }
   else if (Mblks >= Nblks)  /* split M */
   {
#ifdef DEBUG
   fprintf(stderr, "Cut M\n");
#endif
      nblksL = (d * Mblks);
      nblksR = Mblks - nblksL;
      if (nblksR < nblksL)
      {
         rL = 0;
         rR = mr;
      }
      else
      {
         rL = mr;
         rR = 0;
      }
      i = (nblksL*ptmms[indx].mb+rL) << eltsh;
      np = ATL_thrdecompMM_rec(ptmms, TA, TB, nblksL, rL, Nblks, nr, Kblks, kr, 
                               A, lda, B, ldb, C, ldc, pL, indx, COPYC);
      np += ATL_thrdecompMM_rec(ptmms, TA, TB, nblksR, rR, Nblks, nr, Kblks, kr,
                                (TA==AtlasNoTrans)?MindxT(A,i):MindxT(A,i*lda), 
                                lda, B, ldb, MindxT(C,i), ldc, pR, indx+pL, 
                                COPYC);
      return(np);
   }
   else /* split N */
   {
#ifdef DEBUG
   fprintf(stderr, "Cut N\n");
#endif
      nblksL = (d * Nblks);
      nblksR = Nblks - nblksL;
      if (nblksR < nblksL)
      {
         rL = 0;
         rR = nr;
      }
      else
      {
         rL = nr;
         rR = 0;
      }
      i = (nblksL*ptmms[indx].nb+rL) << eltsh;
      np = ATL_thrdecompMM_rec(ptmms, TA, TB, Mblks, mr, nblksL, rL, Kblks, kr, 
                               A, lda, B, ldb, C, ldc, pL, indx, COPYC);
      np += ATL_thrdecompMM_rec(ptmms, TA, TB, Mblks, mr, nblksR, rR, Kblks, kr,
                                A,  lda, (TB == AtlasNoTrans) ? 
                                MindxT(B,i*ldb) : MindxT(B,i), ldb, 
                                MindxT(C,i*ldc), ldc, pR, indx+pL, COPYC);
      return(np);
   }
}
@endskip

int ATL_thrdecompMM_M
   (ATL_TMMNODE_t *ptmms, const enum ATLAS_TRANS TA, const enum ATLAS_TRANS TB,
    ATL_CINT Mblks, const int mr, ATL_CINT Nblks, const int nr, ATL_CINT Kblks,
    const int kr, const void *A, ATL_INT lda, const void *B, const ATL_INT ldb,
    const void *C, ATL_CINT ldc, const int P, const int indx, const int COPYC)
{
   int j, i, m, p;
   const char *a=A, *c=C;
   const int eltsh = ptmms[0].eltsh, mb = ptmms[0].mb, n = ptmms[0].nb*Nblks+nr,
             k = ptmms[0].kb*Kblks+kr, minblks = Mblks / P, 
             extrablks = Mblks - minblks*P;

   for (p=i=0; i < P; i++)
   {
      m = minblks * mb;
      if (i < extrablks)
         m = (minblks + 1)*mb;
      else if (i == extrablks)
         m = minblks*mb + mr;
      else
         m = minblks*mb;
     if (m)
        p++;
         
@skip      j = ATL_launchorder[i];  /* use log2-launch order */
      ptmms[i].A = a;
      ptmms[i].B = B;
      ptmms[i].C = (void*)c;
      ptmms[i].lda = lda;
      ptmms[i].ldb = ldb;
      ptmms[i].ldc = ldc;
      ptmms[i].M = m;
      ptmms[i].N = n;
      ptmms[i].K = (m) ? k : 0;
      ptmms[i].ownC = 1;
      ptmms[i].nCp = ptmms[i].nCw = 0;
      ptmms[i].Cw = NULL;
      ptmms[i].ldcw = 0;
      m <<= eltsh;
      a = (TA == AtlasNoTrans) ? MindxT(a,m) : MindxT(a,m*lda);
      c = MindxT(c,m);
   }
   return(p);
}

int ATL_thrdecompMM_N
   (ATL_TMMNODE_t *ptmms, const enum ATLAS_TRANS TA, const enum ATLAS_TRANS TB,
    ATL_CINT Mblks, const int mr, ATL_CINT Nblks, const int nr, ATL_CINT Kblks,
    const int kr, const void *A, ATL_INT lda, const void *B, const ATL_INT ldb,
    const void *C, ATL_CINT ldc, const int P, const int indx, const int COPYC)
{
   int j, i, n, p;
   const char *b=B, *c=C;
   const int eltsh = ptmms[0].eltsh, nb = ptmms[0].nb, m = ptmms[0].mb*Mblks+mr,
             k = ptmms[0].kb*Kblks+kr, minblks = Nblks / P, 
             extrablks = Nblks - minblks*P;

   for (p=i=0; i < P; i++)
   {
      n = minblks * nb;
      if (i < extrablks)
         n = (minblks + 1)*nb;
      else if (i == extrablks)
         n = minblks*nb + nr;
      else
         n = minblks*nb;
      if (n)
         p++;
         
@skip      j = ATL_launchorder[i];  /* use log2-launch order */
      ptmms[i].A = A;
      ptmms[i].B = b;
      ptmms[i].C = (void*)c;
      ptmms[i].lda = lda;
      ptmms[i].ldb = ldb;
      ptmms[i].ldc = ldc;
      ptmms[i].M = m;
      ptmms[i].N = n;
      ptmms[i].K = (n) ? k : 0;
      ptmms[i].ownC = 1;
      ptmms[i].nCp = ptmms[i].nCw = 0;
      ptmms[i].Cw = NULL;
      ptmms[i].ldcw = 0;
      n <<= eltsh;
      b = (TB == AtlasNoTrans) ? MindxT(b,n*ldb) : MindxT(b,n);
      c = MindxT(c,n*ldc);
   }
   return(p);
}
int ATL_thrdecompMM_K
   (ATL_TMMNODE_t *ptmms, const enum ATLAS_TRANS TA, const enum ATLAS_TRANS TB,
    ATL_CINT Mblks, const int mr, ATL_CINT Nblks, const int nr, ATL_CINT Kblks,
    const int kr, const void *A, ATL_INT lda, const void *B, const ATL_INT ldb,
    const void *C, ATL_CINT ldc, const int P, const int indx, const int COPYC)
{
   int j, i, k, p, ldw;
   const char *a=A, *b=B;
   const int eltsh = ptmms[0].eltsh, kb = ptmms[0].kb, m = ptmms[0].mb*Mblks+mr,
             n = ptmms[0].nb*Nblks+nr, minblks = Kblks / P, 
             extrablks = Kblks - minblks*P;

   for (p=i=0; i < P; i++)
   {
      k = minblks * kb;
      if (i < extrablks)
         k = (minblks + 1)*kb;
      else if (i == extrablks)
         k = minblks*kb + kr;
      else
         k = minblks*kb;
      if (n)
         p++;
         
@skip      j = ATL_launchorder[i];  /* use log2-launch order */
      ptmms[i].A = a;
      ptmms[i].B = b;
      ptmms[i].C = (void*)C;
      ptmms[i].lda = lda;
      ptmms[i].ldb = ldb;
      ptmms[i].ldc = ldc;
      ptmms[i].M = m;
      ptmms[i].N = n;
      ptmms[i].K = k;
      if (i)
      {
         ptmms[i].nCw = 1;
         ptmms[i].nCp = ptmms[i].ownC = 0;
         ldw = ((m + 3)>>2)<<2;  /* make ldw mul of 4 */
         if (!(i & (i-1)))
            ldw += 4;            /* make sure ldw not power of 2 */
         ptmms[i].ldcw = ldw;
         ptmms[i].Cinfp[0] = ptmms+i;
      }
      else
      {
         ptmms[i].ldcw = 0;
         ptmms[i].nCp = ptmms[i].ownC = 1;
         ptmms[i].nCw = 0;
         ptmms[i].Cinfp[ATL_NTHREADS-1] = ptmms+i;
      }
      ptmms[i].Cw = NULL;
      k <<= eltsh;
      a = (TA == AtlasNoTrans) ? MindxT(a,lda*k) : MindxT(a,k);
      b = (TB == AtlasNoTrans) ? MindxT(b,k) : MindxT(b,k*ldb);
   }
   return(p);
}

#include <string.h>
void ATL_linearize_mmnodes(ATL_TMMNODE_t *ptmms, const int P)
/*
 * Takes P intialized entries in ptmms, and makes them contiguous
 * starting from 0 if they aren't already
 */
{
   int i;
   for (i=P-1; i >= 0; i--)
   {
      if (!ptmms[i].K)  /* found empty slot */
      {
         int j;
         for (j=P; !ptmms[j].K; j++);
         memcpy(ptmms+i, ptmms+j, sizeof(ATL_TMMNODE_t));
         if (ptmms[i].nCw || ptmms[i].nCp)
         {
            int k, n;
            n = ptmms[i].nCw;
            for (k=0; k < n; k++)
               if (ptmms[i].Cinfp[k] == ptmms+j)
                  ptmms[i].Cinfp[k] = ptmms+i;
            n = ptmms[i].nCp;
            for (k=0; k < n; k++)
               if (ptmms[i].Cinfp[ATL_NTHREADS-1-k] == ptmms+j)
                  ptmms[i].Cinfp[ATL_NTHREADS-1-k] = ptmms+i;
         }
         ptmms[j].K = 0;
      }
   }
}
@beginskip
void ATL_EnforceNonPwr2LO(ATL_TMMNODE_t *ptmms, const int P)
/*
 * If threads aren't a power of 2, then the recursive decomposition will
 * fill in the ptmms array in a different order than the log2 spawn.
 * As long as P >= NTHREADS, all entries are filled in, so the worst that
 * happens is that thread 3 does the work that we expect to be done by 4.
 * However, if P < NTHREADS, then threads that launch starts won't have
 * work, so we must make sure that the 1st P elts in launchorder have
 * work to do.
 */
{
   int i, j, k, kk, h;

   if (P >= ATL_NTHREADS)
      return;
   for (i=0; i < P; i++)
   {
      j = ATL_launchorder[i];
      if (!ptmms[j].K)  /* we have found an empty required entry! */
      {
/*
 *      Search for a filled in entry that will not be used in launch
 */
        for (kk=ATL_NTHREADS-1; kk >= P; kk--)
        {
           k = ATL_launchorder[kk];
           if (ptmms[k].K)  /* found a non-empty entry */
           {
              ptmms[j].A = ptmms[k].A;
              ptmms[j].B = ptmms[k].B;
              ptmms[j].C = ptmms[k].C;
              ptmms[j].lda = ptmms[k].lda;
              ptmms[j].ldb = ptmms[k].ldb;
              ptmms[j].ldc = ptmms[k].ldc;
              ptmms[j].M = ptmms[k].M;
              ptmms[j].N = ptmms[k].N;
              ptmms[j].K = ptmms[k].K;
              ptmms[j].ownC = ptmms[k].ownC;
              ptmms[j].nCp = ptmms[k].nCp;
              ptmms[j].nCw = ptmms[k].nCw;
              ptmms[j].Cw = ptmms[k].Cw;
              ptmms[j].ldcw = ptmms[k].ldcw;
              for (h=0; h < ptmms[j].nCw; h++)
                 ptmms[j].Cinfp[h] = (ptmms[k].Cinfp[h] != ptmms+k) ?
                                     ptmms[k].Cinfp[h] : ptmms+j;
              for (h=ATL_NTHREADS-1; h >= ATL_NTHREADS-ptmms[j].nCp; h--)
                 ptmms[j].Cinfp[h] = (ptmms[k].Cinfp[h] != ptmms+k) ?
                                     ptmms[k].Cinfp[h] : ptmms+j;
              ptmms[k].K = 0;                /* entry k now empty */
              break;
           }
        }
        ATL_assert(kk >= P);
      }
   }
}
@endskip

int ATL_thrdecompMM
   (ATL_TMMNODE_t *ptmms, const enum ATLAS_TRANS TA, const enum ATLAS_TRANS TB,
    ATL_CINT M, ATL_CINT N, ATL_CINT K, const void *A, ATL_INT lda, 
    const void *B, const ATL_INT ldb, const void *C, ATL_CINT ldc, const int P,
    int *DivideK)
{
   int np, i;
   ATL_CINT Mblks = M/ptmms[0].mb, mr = M-Mblks*ptmms[0].mb;
   ATL_CINT Nblks = N/ptmms[0].nb, nr = N-Nblks*ptmms[0].nb;
   ATL_CINT Kblks = K/ptmms[0].kb, kr = K-Kblks*ptmms[0].kb;
   ATL_CINT mnblks = ((Nblks) ? Nblks : 1) * ((Mblks) ? Mblks : 1);

  *DivideK = 0;
/*
 * First, consider cutting K, which we only do if the number of Kblks
 * dominates the number of blocks we can find in cutting both M & N,
 */
@skip   if (mnblks < P || Kblks > P*mnblks)
   if ((mnblks < P && Kblks > mnblks && Kblks >= 8) || Kblks > P*mnblks)
   {
      np = ATL_thrdecompMM_rMNK(ptmms, TA, TB, Mblks, mr, Nblks, nr, Kblks, kr,
                                A, lda, B, ldb, C, ldc, P, 0, 0);
      for (i=0; i < np; i++)
      {
         if (ptmms[i].K > 0 && ptmms[i].K < K)
         {
            *DivideK = 1;
            break;
         }
      }
      if (np < ATL_NTHREADS)
         ATL_linearize_mmnodes(ptmms, np);
@skip         ATL_EnforceNonPwr2LO(ptmms, np);
      return(np);
   }
/*
 * Divide only the M-dimension to cut down on JIK workspace & improve CE
 * efficiency if we have enough M blocks to make it worthwhile;
 * We ask that we can give each thread at least 4 blocks, and that
 * the N diminsion doesn't dominate
 */
   if ((Mblks >= (P<<2) && Nblks < P*Mblks))
   {
      np = ATL_thrdecompMM_M(ptmms, TA, TB, Mblks, mr, Nblks, nr, Kblks, kr,
                             A, lda, B, ldb, C, ldc, P, 0, 0);
      if (np < ATL_NTHREADS)
         ATL_linearize_mmnodes(ptmms, np);
      return(np);
   }
/*
 * If none of these special cases are triggered, recursively divide up C
 */
   np = ATL_thrdecompMM_rMN(ptmms, TA, TB, Mblks, mr, Nblks, nr, Kblks, kr, 
                            A, lda, B, ldb, C, ldc, P, 0, 0);
   if (np < ATL_NTHREADS)
      ATL_linearize_mmnodes(ptmms, np);
@skip      ATL_EnforceNonPwr2LO(ptmms, np);
   return(np);
}

@ROUT ATL_StructIsInitMM ATL_Xtgemm
int ATL_StructIsInitMM(void *vp)
{
   return(((ATL_TMMNODE_t*)vp)->K);
}

@ROUT ATL_DoWorkMM ATL_Xtgemm
void ATL_DoWorkMM(ATL_LAUNCHSTRUCT_t *lp, void *vp)
/*
 * Current implementation doesn't need lp, but if we had an error queue or
 * something similar we would need it, so keep it around
 */
{
   ATL_thread_t *tp = vp;
   const int myrank = tp->rank;
   ATL_TMMNODE_t *mmp = ((ATL_TMMNODE_t*)lp->opstruct)+myrank;
/*
 * Allocate space if needed, do operation
 */
   if (mmp->nCw)
   {
/*
 *    If malloc fails, we'll do the operation during the combine
 */
      #ifdef ATL_SERIAL_COMBINE
         ATL_assert(mmp->Cw);
      #else
         mmp->Cw = malloc(((mmp->ldcw)<<mmp->eltsh)*mmp->N+ATL_Cachelen);
      #endif
      if (mmp->Cw)
      {
         mmp->gemmK(mmp->M, mmp->N, mmp->K, mmp->alpha, mmp->A, mmp->lda,
                    mmp->B, mmp->ldb, mmp->zero, 
                    ATL_AlignPtr(mmp->Cw), mmp->ldcw);
      }
#ifdef DEBUG
      else
         fprintf(stderr, "%d: unable to allocate C(%dx%d)!!\n", 
                 mmp->rank, mmp->M, mmp->N);
#endif
   }
   else  /* do GEMM directly into original C; no possibility of failure! */
      mmp->gemmK(mmp->M, mmp->N, mmp->K, mmp->alpha, mmp->A, mmp->lda,
                 mmp->B, mmp->ldb, mmp->beta, mmp->C, mmp->ldc);
}
@ROUT ATL_tNumGemmThreads
#include "atlas_misc.h"
#include "atlas_tlvl3.h"
#include Mstr(Mjoin(Mjoin(atlas_,UPR),amm_use.h))

#if ATL_USE_AMM
/*
 * This function provides an estimate on max number of threads to use to
 * perform a access-major GEMM.
 */
size_t Mjoin(PATL,GetAmmmNthr)(ATL_CSZT M, ATL_CSZT N, ATL_CSZT K)
{
   size_t nnblks, nmblks, nkblks, p;

/*
 * On the XeonPHI, threads take a huge time to start up, so don't try
 * threading until we have a big problem.  We can hopefully fix this
 * later by changing to a thread pool model on the PHI.
 */
   #ifdef ATL_ARCH_XeonPHI
      if ((M*1e-6)*N*K < 8.0)
         return(1);
   #endif

   nmblks = (M >= ATL_AMM_66MB) ? M/ATL_AMM_66MB : 1;
   nnblks = (N >= ATL_AMM_66NB) ? N/ATL_AMM_66NB : 1;
   nkblks = (K >= ATL_AMM_66KB) ? K/ATL_AMM_66KB : 1;
/*
 * Any shape with two degenerate dimensions causes a lot of bus traffic,
 * with very little computation to overcome threading overheads,
 * so demand at least 32 blocks before parallelizing
 */
   if ((nmblks==1 && nnblks==1) || (nmblks==1 && nkblks==1) || 
       (nnblks==1 && nkblks==1))
      return((nnblks*nmblks*nkblks)>>5);
/*
 * If it is a rank-K update, ask to have 4 big blocks of C
 */
   if (K <= ATL_MAXM_RKK)
   {
      nnblks=N/ATL_MAXN_RKK, nmblks=M/ATL_MAXM_RKK;
      return((nnblks*nmblks)>>2);
   }
   
/*
 * By default, give everyone 32 blocks to compute; for square problems,
 * the number of blocks is cubic, so this should not meaningfully restrict
 * parallelism.
 */
   return((nmblks*nnblks*nkblks)>>5);
}
#endif

/* 
 * ====================================================================
 * This function will eventually be generated, but for now just written
 * ====================================================================
 */
int Mjoin(PATL,tNumGemmThreads)(ATL_CINT M, ATL_CINT N, ATL_CINT K)
/*
 * RETURNS : estimate of how many threads will be used to run the problem,
 *           assuming we will actually do threading (i.e. THRESH is exceeded)
 *           0 is returned if this number is 1 or less.
 */
{
@beginskip
   int np;
   ATL_TMMNODE_t mms[ATL_NTHREADS];
   #ifdef TCPLX
      TYPE ONE[2] = {1.0, 0.0};
   #else
      TYPE ONE=ATL_rone;
   #endif
   void Mjoin(PATL,InitTMMNodes)
      (const enum ATLAS_TRANS TA, const enum ATLAS_TRANS TB, const TYPE *alpha,
       const TYPE *beta, const TYPE *one, const TYPE *zero, 
       ATL_thread_t *btp, ATL_TMMNODE_t *ptmms);

   np = Mjoin(PATL,threadMM)(AtlasNoTrans, AtlasNoTrans, M, N, K);
   if (np > 1)
   {
      Mjoin(PATL,InitTMMNodes)(AtlasNoTrans, AtlasNoTrans, SADD ONE, SADD ONE, 
                               SADD ONE, SADD ONE, NULL, mms);
      if (np == 1)  /* use recursive distribution */
         np = ATL_thrdecompMM_rec(mms, AtlasNoTrans, AtlasNoTrans, 
                                  M/MB, M%MB, N/NB, N%NB, K/KB, K%KB, 
                                  NULL, M, NULL, K, NULL, M, 
                                  ATL_NTHREADS, 0, 0);
      else
         np = ATL_thrdecompMM_M(mms, AtlasNoTrans, AtlasNoTrans, 
                                M/MB, M%MB, N/NB, N%NB, K/KB, K%KB, 
                                NULL, M, NULL, K, NULL, M, ATL_NTHREADS, 0, 0);
      np = (np < 2) ? 0 : np;
   }
   return(np);
@endskip
   #if ATL_USE_AMM
      size_t np;
      np = Mjoin(PATL,GetAmmmNthr)(M, N, K);
      return(np >= ATL_NTHREADS ? ATL_NTHREADS : np);
   #else
      double flops;
      int np;
      if (M < 4 || N < 4 || K < 4)
         return(0);
      flops = ((2.0*M)*N)*K;
      np = flops / ATL_TGEMM_PERTHR_MF;
      np = (np <= 1) ? 0 : np;
      return(np >= ATL_NTHREADS ? ATL_NTHREADS : np);
   #endif
}

int Mjoin(PATL,GemmWillThread)(ATL_CINT M, ATL_CINT N, ATL_CINT K)
/*
 * Returns: 0 if threshold FLOPS not achieved, rough # of threads used else
 */
{
   #if ATL_USE_AMM
      size_t np;
      np = Mjoin(PATL,GetAmmmNthr)(M, N, K);
      if (np < 2)
         return(0);
      return(Mmin(np, ATL_NTHREADS));
   #else
      if (((2.0*M)*N)*K < ATL_TGEMM_THRESH_MF)
         return(0);
      return(Mjoin(PATL,tNumGemmThreads)(M,N,K));
   #endif
}
@ROUT ATL_tgemm ATL_tgemm_rec ATL_tgemm_M ATL_tgemm_N ATL_tgemm_K ATL_tgemm_p
#include "atlas_misc.h"
#include "atlas_threads.h"
#include "atlas_tlvl3.h"
/*
 * prototype the typeless tGEMM helper routines
 */
void ATL_DoWorkMM(ATL_LAUNCHSTRUCT_t *lp, void *vp);
int ATL_StructIsInitMM(void *vp);
@ROUT ATL_tgemm_rec ATL_tgemm_M ATL_tgemm_N ATL_tgemm_K
void ATL_linearize_mmnodes(ATL_TMMNODE_t *ptmms, const int P);
@whiledef rt  ATL_thrdecompMM_M ATL_thrdecompMM_N ATL_thrdecompMM_K ATL_thrdecompMM_rMNK
int @(rt)
   (ATL_TMMNODE_t *ptmms, const enum ATLAS_TRANS TA, const enum ATLAS_TRANS TB,
    ATL_CINT Mblks, const int mr, ATL_CINT Nblks, const int nr, ATL_CINT Kblks,
    const int kr, const void *A, ATL_INT lda, const void *B, const ATL_INT ldb,
    const void *C, ATL_CINT ldc, const int P, const int indx, const int COPYC);
@endwhile
@ROUT ATL_tgemm ATL_tgemm_p
int ATL_thrdecompMM
   (ATL_TMMNODE_t *ptmms, const enum ATLAS_TRANS TA, const enum ATLAS_TRANS TB,
    ATL_CINT M, ATL_CINT N, ATL_CINT K, const void *A, ATL_INT lda, 
    const void *B, const ATL_INT ldb, const void *C, ATL_CINT ldc, const int P,
    int *DivideK);

@ROUT ATL_tgemm
@multidef tta AtlasTrans AtlasNoTrans AtlasConjTrans
@whiledef TA T N C
   @multidef ttb AtlasTrans AtlasNoTrans AtlasConjTrans
   @whiledef TB T N C
@mif TA = "C
#ifdef TCPLX
@endmif
@mif TB = "C
   @mif TA ! "C
#ifdef TCPLX
   @endmif
@endmif
void Mjoin(PATL,tsvgemm@(TA)@(TB))
   (ATL_CINT M, ATL_CINT N, ATL_CINT K, const void* alpha,
    const void *A, ATL_CINT lda, const void *B, ATL_CINT ldb, 
    const void *beta, void *C, ATL_CINT ldc)
{
#ifdef FindingCE
void Mjoin(PATL,FindCE_mm)(const enum ATLAS_TRANS TA, const enum ATLAS_TRANS TB,
                           const int M, const int N, const int K, 
                           const SCALAR alpha, const TYPE *A, const int lda, 
                           const TYPE *B, const int ldb, const SCALAR beta, 
                           TYPE *C, const int ldc);
   Mjoin(PATL,FindCE_mm)(@(tta), @(ttb), M, N, K, 
                         SVVAL((TYPE*)alpha), A, lda, B, ldb,
                         SVVAL((TYPE*)beta), C, ldc);
#else
   Mjoin(PATL,ammm)(@(tta), @(ttb), M, N, K, SVVAL((TYPE*)alpha), 
                    A, lda, B, ldb, SVVAL((TYPE*)beta), C, ldc);
@skip   Mjoin(PATL,tgemm@(TA)@(TB))(M, N, K, SVVAL((TYPE*)alpha), A, lda, B, ldb,
@skip                       SVVAL((TYPE*)beta), C, ldc);
#endif
}
@mif TA = "C
#endif  /* end ifdef TCPLX */
@endmif
@mif TB = "C
   @mif TA ! "C
#endif  /* end ifdef TCPLX */
   @endmif
@endmif
      @undef ttb
   @endwhile
   @undef tta
@endwhile
@ROUT ATL_tgemm_p
   @define ds @p@
@ROUT ATL_tgemm_M
   @define ds @M@
@ROUT ATL_tgemm_N
   @define ds @rMN@
@ROUT ATL_tgemm_K
   @define ds @K@
@ROUT ATL_tgemm_rec ATL_tgemm_M ATL_tgemm_N ATL_tgemm_K ATL_tgemm_p
void Mjoin(PATL,InitTMMNodes)
   (const enum ATLAS_TRANS TA, const enum ATLAS_TRANS TB, const TYPE *alpha, 
    const TYPE *beta, const TYPE *one, const TYPE *zero, 
    ATL_thread_t *btp, ATL_TMMNODE_t *ptmms);
@ROUT ATL_tgemm

#ifdef ATL_SERIAL_COMBINE
static ATL_combnode_t *ATL_NewCombnode
   (ATL_INT M, ATL_INT N, TYPE *W, ATL_INT ldw, TYPE *D, ATL_INT ldd,
    ATL_combnode_t *next)
{
   ATL_combnode_t *np;
   np = malloc(sizeof(ATL_combnode_t));
   ATL_assert(np);
   np->M = M;
   np->N = N;
   np->W = W;
   np->ldw = ldw;
   np->D = D;
   np->ldd = ldd;
   np->next = next;
   return(np);
}
#endif

void Mjoin(PATL,InitTMMNodes)
   (const enum ATLAS_TRANS TA, const enum ATLAS_TRANS TB, const TYPE *alpha, 
    const TYPE *beta, const TYPE *one, const TYPE *zero, 
    ATL_thread_t *btp, ATL_TMMNODE_t *ptmms)
{
   int i;
   void (*gemmK)(ATL_CINT, ATL_CINT, ATL_CINT, const void*, const void *,
                 ATL_CINT, const void*, ATL_CINT, const void*, void*, ATL_CINT);

   if (TA == AtlasNoTrans)
   {
#ifdef TCPLX
      if (TB == AtlasConjTrans)
         gemmK = Mjoin(PATL,tsvgemmNC);
      else
#endif
      gemmK = (TB == AtlasNoTrans)?Mjoin(PATL,tsvgemmNN):Mjoin(PATL,tsvgemmNT);
   }
#ifdef TCPLX
   else if (TA == AtlasConjTrans)
   {
      if (TB == AtlasNoTrans)
         gemmK = Mjoin(PATL,tsvgemmCN);
      else if (TB == AtlasConjTrans)
         gemmK = Mjoin(PATL,tsvgemmCC);
      else
         gemmK = Mjoin(PATL,tsvgemmCT);
   }
#endif
   else
   {
#ifdef TCPLX
      if (TB == AtlasConjTrans)
         gemmK = Mjoin(PATL,tsvgemmTC);
      else
#endif
      gemmK = (TB == AtlasNoTrans)?Mjoin(PATL,tsvgemmTN):Mjoin(PATL,tsvgemmTT);
   }
   for (i=0; i < ATL_NTHREADS; i++)
   {
      ptmms[i].mb = Mmin(ATL_AMM_LLCMU, ATL_AMM_MAXMB);
      ptmms[i].nb = Mmin(ATL_AMM_LLCMU, ATL_AMM_MAXNB);
      ptmms[i].kb = ATL_AMM_MAXKB;
      ptmms[i].gemmK = gemmK;
      ptmms[i].eltsz = ATL_sizeof;
      ptmms[i].eltsh = Mjoin(PATL,shift);
      ptmms[i].K = 0;
      ptmms[i].nCp = ptmms[i].nCw = 0;
      ptmms[i].ownC = 0;
      ptmms[i].rank = i;
      ptmms[i].alpha = (void*) alpha;
      ptmms[i].beta  = (void*) beta;
      ptmms[i].one = (void*) one;
      ptmms[i].zero  = (void*) zero;
      ptmms[i].Cinfp[0] = ptmms+i;
   }
}

@ROUT ATL_tgemm_rec ATL_tgemm_K ATL_tgemm_p
void Mjoin(PATL,HandleNewCp)(ATL_TMMNODE_t *me, ATL_TMMNODE_t *him);
int Mjoin(PATL,CombineCw)(ATL_TMMNODE_t *me, ATL_TMMNODE_t *him);
@ROUT ATL_tgemm
int Mjoin(PATL,CombineCw)(ATL_TMMNODE_t *me, ATL_TMMNODE_t *him)
/*
 * This routine combines the data in him->Cw into my->Cw (or my->C), if poss.
 * If his workspace is bigger than mine, I combine instead into his workspace,
 * and then set my pointer to his workspace.  The buffer that has been subsumed
 * is freed after the combine.
 * NOTE: This routine assumes him is *not* an owner of C (i.e. he wrote to
 *       workspace, not to the original C)!
 * RETURNS: 0 if we are able to do the combine, non-zero if buffers are
 *          cannot be combined.
 */
{
   ATL_TMMNODE_t *myCp;
   TYPE *w;
   size_t meB, meE, himB, himE, I, J;   /* begin,end of C range */
   #ifdef TREAL
      const TYPE ONE = 1.0;
   #else
      const TYPE ONE[2] = {1.0, 0.0};
   #endif
   const int eltsh = me->eltsh;

   ATL_assert(!him->ownC);
/*
 * Find starting/ending points of our C partitions
 */
   himB = (size_t)him->C; 
   himE = himB + ((him->N*(him->ldc) + him->M)<<eltsh);
   meB  = (size_t) me->C; 
   meE  = meB + ((me->N*(me->ldc) + me->M)<<eltsh);
/*
 * If I own my piece of the original C, then I can combine any C that is
 * a proper subset of mine; 
 */
   if (me->ownC)
   {
      ATL_assert(!him->ownC);  /* should never be true in this routine */
/*
 *    If his wrkspc is not a subset of mine, I can't combine it into the
 *    piece of C originally owned by me
 */
      if (himB < meB || himE > meE)
         return(1);       /* can't combine non subset of my C */
      else if (him->Cw)  /* his malloc succeeded */
      {
         Mjoin(PATL,geadd)(him->M, him->N, ONE, ATL_AlignPtr(him->Cw), 
                           him->ldcw, ONE, (TYPE*)him->C, him->ldc);
         free(him->Cw);
      }
      else if (him->nCw)  /* must do GEMM since he couldn't malloc */
         him->gemmK(him->M, him->N, him->K, him->alpha, him->A, him->lda,
                    him->B, him->ldb, SADD ONE, him->C, him->ldc);
      return(0);        /* successful combine */
   }
/*
 * *************************************************************************
 * Otherwise, I don't own C, so must combine work into my or his buffer when
 * possible, and return failure when not
 * *************************************************************************
 */
/*
 * If my workspace is a superset of his, use my workspace as the target buffer
 * if I was able to allocate it
 */
   if (meB <= himB && meE >= himE && me->Cw)
   {
/*
 *    Determine where our overlap is
 */
      I = (himB - meB)>>eltsh;                    /* gap in elements */
      J = I / him->ldc;                           /* column coord */
      I -= J*him->ldc;                            /* row coord */
      if (I+him->M >= me->M || J+him->N >= me->N) /* no intersec after all! */
         return(1);                               /* so cannot combine */
      w = ATL_AlignPtr(me->Cw);
      w += J*me->ldcw + I;
      if (him->Cw)  /* if he succeeded in malloc, combine his op with mine */
      {
         Mjoin(PATL,geadd)(him->M, him->N, ONE, ATL_AlignPtr(him->Cw), 
                           him->ldcw, ONE, w, me->ldcw);
         free(him->Cw);
      }
      else          /* must do GEMM since he didn't */
         him->gemmK(him->M, him->N, him->K, him->alpha, him->A, him->lda,
                    him->B, him->ldb, SADD ONE, w, me->ldcw);
      return(0);        /* successful combine */
   }
/*
 * else if his workspace is a superset of mine, use his as target buffer if
 * he was able to allocate it
 */
   else if (himB <= meB && himE >= meE && him->Cw)
   {
/*
 *    Determine where our overlap is
 */
      I = (meB - himB)>>eltsh;                    /* gap in elements */
      J = I / me->ldc;                            /* col coordinate */
      I -= J*me->ldc;                             /* row coordinate */
      if (I+me->M >= him->M || J+me->N >= him->N) /* no intersec after all! */
         return(1);                               /* so cannot combine */
      w = ATL_AlignPtr(him->Cw);
      w += J*him->ldcw + I;
      if (me->Cw)  /* if I succeeded in malloc, combine my op with his */
      {
         Mjoin(PATL,geadd)(me->M, me->N, ONE, ATL_AlignPtr(me->Cw), 
                           me->ldcw, ONE, w, him->ldcw);
         free(me->Cw);
      }
      else          /* must do my GEMM into his workspace since I couldn't */
         him->gemmK(me->M, me->N, me->K, me->alpha, me->A, me->lda,
                    me->B, me->ldb, SADD ONE, w, him->ldcw);
      me->C = him->C;
      me->Cw = him->Cw;
      me->ldcw = him->ldcw;
      me->M = him->M;
      me->N = him->N;
      return(0);        /* successful combine */
   }
   return(1);           /* unsuccessful combine */
}

void Mjoin(PATL,HandleNewCp)(ATL_TMMNODE_t *me, ATL_TMMNODE_t *him)
/*
 * Handles joining a Cp to my list of C partitions
 */
{
  size_t himB, himE, meB, meE, B, E, I, J;
  ATL_TMMNODE_t *tp;
  ATL_INT ldc;
  int i, j;
  const int eltsh = me->eltsh;
/*
 * Find the extent of his C partition
 */
   himB = (size_t)him->C; 
   himE = himB + ((him->N*(him->ldc) + him->M)<<eltsh);
   ldc = him->ldc;
/*
 * First, see if this partition can be joined to one I already own
 */
   for (i=0; i < me->nCp; i++)
   {
      tp = me->Cinfp[ATL_NTHREADS-1-i];
      if (tp)
      {
         meB  = (size_t) tp->C; 
         meE  = meB + ((tp->N*(tp->ldc) + tp->M)<<eltsh);
         if (meB <= himB)  /* my partition has the base pointer */
         {
            I = (himB - meB)>>eltsh;    /* gap between our C's in elements */
            J = I / ldc;                /* column coord from meB */
            I -= J*ldc;                 /* row coord from meB */
/*
 *          If we have same row (col) coord and he starts at col (row) that 
 *          I stop at, join column (row) panel
 */
            if (!I && J == tp->N)
               tp->N += him->N;
            else if (!J && I == tp->M)
               tp->M += him->M;
            else
               continue;                /* if unjoinable, go next candidate */
            break;   /* partitions joined, quit */
         }
         else              /* his partition has the base pointer */
         {
            I = (meB - himB)>>eltsh;    /* gap between our C's in elements */
            J = I / ldc;                /* column coord from himB */
            I -= J*ldc;                 /* row coord from himB */
/*
 *          If we have same row (col) coord and I start at col (row) that 
 *          he stops at, join column (row) space
 */
            if (!I && J == him->N)
               tp->N += him->N;
            else if (!J && I == tp->M)
               tp->M += him->M;
            else
               continue;                /* if unjoinable, go next candidate */
            tp->C = him->C;
            break;   /* partitions joined, quit */
         }
      }
   }
/*
 * If I can't join his partition to any of mine, add his to list
 */
   if (i == me->nCp)
   {
      (me->nCp)++;
      me->Cinfp[ATL_NTHREADS-(me->nCp)] = him;
      tp = him;
   }
/*
 * Either new partition is in tp, or an expanded partition is.  In either
 * case, see if any of my workspaces can be combined into this new 
 * (or newly expanded) area I own.
 */
   if (i < me->nCp)
   {
      for (i=0; i < me->nCw; i++)
      {
         if (!Mjoin(PATL,CombineCw)(tp, me->Cinfp[i]))
         {
            for (j=i+1; j < me->nCw; j++)
               me->Cinfp[j-1] = me->Cinfp[j];
            (me->nCw)--;
         }
      }
   }
}

@ROUT ATL_tgemm_rec ATL_tgemm_K ATL_tgemm_p
void Mjoin(PATL,CombineStructsMM)(void *vp, const int myrank, const int herank);
@ROUT ATL_tgemm
void Mjoin(PATL,CombineStructsMM)
(
   void *vp,          /* void ptr to P MMNODE_t structs give tasks to threads */
   const int myrank,  /* my entry in MMNODE_t array */
   const int hisrank  /* array entry to be combined into mine */
)
{
   ATL_TMMNODE_t *mme = ((ATL_TMMNODE_t*)vp)+myrank; 
   ATL_TMMNODE_t *mhim = ((ATL_TMMNODE_t*)vp)+hisrank;
   int i, j;
   #ifdef ATL_SERIAL_COMBINE  /* do nothing if combining serially */
      return;
   #endif

/*
 * First, for all of the partitions of the original C that he owns, either
 * join them with mine, or add them to my list of owned partitions
 */
   for (i=0; i < mhim->nCp; i++)
      Mjoin(PATL,HandleNewCp)(mme, mhim->Cinfp[ATL_NTHREADS-1-i]);
/*
 * For all of his workspaces, find out where to combine them into
 */
   for (i=0; i < mhim->nCw; i++)
   {
/*
 *    Look through my partitions of original C for combine partner
 */
      for (j=0; j < mme->nCp; j++)
         if (!Mjoin(PATL,CombineCw)(mme->Cinfp[ATL_NTHREADS-1-j], 
                                    mhim->Cinfp[i]))
            break;
/*
 *    If I can't combine his data directly into C, see if it can be
 *    combined with any of my workspaces
 */
      if (j == mme->nCp)
      {
         for (j=0; j < mme->nCw; j++)
            if (!Mjoin(PATL,CombineCw)(mme->Cinfp[j], mhim->Cinfp[i]))
               break;
/*
 *       If I can't combine his data into any partition or workspace, add his
 *       node to my list of workspaces to be combined later
 */
         if (j == mme->nCw)
         {
            mme->Cinfp[j] = mhim->Cinfp[i];
            mme->nCw = j + 1;
         }
      }
   }
}

@ROUT ATL_tgemm
void Mjoin(PATL,tgemm)(const enum ATLAS_TRANS TA, const enum ATLAS_TRANS TB,
@ROUT ATL_tgemm_rec
int Mjoin(PATL,tgemm_rec)(const enum ATLAS_TRANS TA, const enum ATLAS_TRANS TB,
@ROUT ATL_tgemm_M ATL_tgemm_N ATL_tgemm_K ATL_tgemm_p
int Mjoin(PATL,tgemm_@(ds))(const enum ATLAS_TRANS TA, const enum ATLAS_TRANS TB,
@ROUT ATL_tgemm ATL_tgemm_rec ATL_tgemm_M ATL_tgemm_N ATL_tgemm_K ATL_tgemm_p
                       ATL_CINT M, ATL_CINT N, ATL_CINT K, const SCALAR alpha, 
                       const TYPE *A, ATL_CINT lda, const TYPE *B, ATL_CINT ldb,
                       const SCALAR beta, TYPE *C, ATL_CINT ldc)
{
   #ifdef ATL_SERIAL_COMBINE
      ATL_combnode_t *combb=NULL, *combp;
   #endif
@skip   ATL_thread_t tp[ATL_NTHREADS];
@skip   ATL_LAUNCHSTRUCT_t ls;
   ATL_TMMNODE_t mms[ATL_NTHREADS];
   int i, np, DividedK=0;
   #ifdef TREAL
      TYPE ONE=ATL_rone, ZERO=ATL_rzero;
   #else
      TYPE ONE[2] = {ATL_rone, ATL_rzero}, ZERO[2] = {ATL_rzero, ATL_rzero};
   #endif
@ROUT ATL_tgemm_p `   extern int ATL_FINDP;`

   if (M < 1 || N < 1)
@ROUT ATL_tgemm
      return;
@ROUT ATL_tgemm_rec ATL_tgemm_M ATL_tgemm_N ATL_tgemm_K ATL_tgemm_p
      return(0);
@ROUT ATL_tgemm ATL_tgemm_rec ATL_tgemm_M ATL_tgemm_N ATL_tgemm_K ATL_tgemm_p
   if (K < 1 || SCALAR_IS_ZERO(alpha))
   {
      if (!SCALAR_IS_ONE(beta))
         Mjoin(PATL,gescal)(M, N, beta, C, ldc);
@ROUT ATL_tgemm
      return;
@ROUT ATL_tgemm_rec ATL_tgemm_M ATL_tgemm_N ATL_tgemm_K ATL_tgemm_p
      return(0);
@ROUT ATL_tgemm ATL_tgemm_rec ATL_tgemm_M ATL_tgemm_N ATL_tgemm_K ATL_tgemm_p
   }
@ROUT ATL_tgemm
#ifdef TREAL
@skip  if (N <= ATL_AMM_MAXMB && K <= ATL_AMM_MAXKB &&  K > 2 &&
@skip      M >= (ATL_AMM_MAXMB<<2)*Mmin(ATL_NTHREADS,8))
   {
      int Mjoin(PATL,tammm)(enum ATLAS_TRANS TA, enum ATLAS_TRANS TB,
         ATL_CSZT M, ATL_CSZT N, ATL_CSZT K, const SCALAR alpha,
         const TYPE *A, ATL_CSZT lda, const TYPE *B, ATL_CSZT ldb,
         const SCALAR beta, TYPE *C, ATL_CSZT ldc);
      if (!Mjoin(PATL,tammm)(TA, TB, M, N, K, alpha, A, lda, B, ldb, 
                             beta, C, ldc))
         return;
   }
#endif
/*
 * See if we are in a case where we've implemented a dynamically scheduled
 * code.  Performance of the dynamically scheduled operations varies:
 * on Intel, its typically faster, and on AMD its slower than
 * statically scheduled *when run on an unloaded machine*.  The difference is
 * that if there is load on one or more processors, dynamically scheduled
 * code is almost twice as fast.  On unloaded machines, the performance 
 * difference is due mostly to the differing partitioning, not the overhead
 * of dynamic scheduling, which always seems to pay for itself.
 * On unloaded AMD machines, the asymptotic loss is roughly 1-2%.
 * Dynamic scheduling seems to always be a performance loss for MAC OSX
 *
 * NOTE: these cases commented out for all systems since they use block-major
 *       code, which is now usually slower than access-major!
 */
@skip   #ifndef ATL_OS_OSX
   #if 0
      #ifdef FindingCE
         ATL_assert(!Mjoin(PATL,tgemm_bigMN_Kp)(TA, TB, M, N, K, alpha, A, lda, 
                                                B, ldb, beta, C, ldc));
         return;
      #endif
/*
 *    Rank-K update has special case
 */
      i = Mmax(ATL_NTHREADS,4)*NB;
      if (K <= 4*NB && M >= 2*MB && N >= 2*NB && Mmax(M,N) >= i)
      {
         if (!Mjoin(PATL,tgemm_rkK)(TA, TB, M, N, K, alpha, A, lda, B, ldb, 
                                    beta, C, ldc))
            return;
      }
/*
 *    Very large matrices loops over rank-Kp updates, where Kp is set by
 *    CacheEdge.  If any dim is very small, use one of the other cases.
 */
      i = Mmin(M,N);
      i = Mmin(i, K);
      if (i > Mmax(8,2*ATL_NTHREADS)*NB)
      {
         if (!Mjoin(PATL,tgemm_bigMN_Kp)(TA, TB, M, N, K, alpha, A, lda, 
                                         B, ldb, beta, C, ldc))
            return;
      }
   #endif
/*
 * See how many processors are optimal for this problem
 */
   np = Mjoin(PATL,threadMM)(TA, TB, M, N, K);
   if (np > 1)
   {
      Mjoin(PATL,InitTMMNodes)(TA, TB, SADD alpha, SADD beta, SADD ONE, 
                               SADD ZERO, NULL, mms);
      np = ATL_thrdecompMM(mms, TA, TB, M, N, K, A, lda, B, ldb, C, ldc, 
                           np, &DividedK);
   }
@ROUT ATL_tgemm_p
   Mjoin(PATL,InitTMMNodes)(TA, TB, SADD alpha, SADD beta, SADD ONE, 
                            SADD ZERO, NULL, mms);
   np = ATL_thrdecompMM(mms, TA, TB, M, N, K, A, lda, B, ldb, C, ldc, 
                        ATL_FINDP, &DividedK);
   if (np < ATL_FINDP)
      return(0);
@ROUT ATL_tgemm_rec
   Mjoin(PATL,InitTMMNodes)(TA, TB, SADD alpha, SADD beta, SADD ONE, 
                            SADD ZERO, NULL, mms);
   np = ATL_thrdecompMM_rMNK(mms, TA, TB, M/MB, M%MB, N/NB, N%NB, K/KB, K%KB,
                             A, lda, B, ldb, C, ldc, ATL_NTHREADS, 0, 0);
   if (np < ATL_NTHREADS)
      ATL_linearize_mmnodes(mms, np);
@beginskip
   #if ATL_NTHREADS != (1<<ATL_NTHRPOW2)
      if (np < ATL_NTHREADS)
         ATL_EnforceNonPwr2LO(mms, np);
   #endif
@endskip
@ROUT ATL_tgemm_M ATL_tgemm_N ATL_tgemm_K
   Mjoin(PATL,InitTMMNodes)(TA, TB, SADD alpha, SADD beta, SADD ONE, 
                            SADD ZERO, NULL, mms);
   np = ATL_thrdecompMM_@(ds)(mms, TA, TB, M/MB, M%MB, N/NB, N%NB, K/KB, K%KB,
                          A, lda, B, ldb, C, ldc, ATL_NTHREADS, 0, 0);
   if (np < ATL_NTHREADS)
      ATL_linearize_mmnodes(mms, np);
@ROUT ATL_tgemm ATL_tgemm_rec ATL_tgemm_M ATL_tgemm_N ATL_tgemm_K ATL_tgemm_p
#ifdef DEBUG
fprintf(stderr, "np=%d\n\n", np);
#endif
   if (np < 2)
   {
      Mjoin(PATL,gemm)(TA, TB, M, N, K, alpha, A, lda, B, ldb, beta, C, ldc);
@ROUT ATL_tgemm_rec ATL_tgemm_M ATL_tgemm_N ATL_tgemm_K ATL_tgemm_p `      return(1);`
@ROUT ATL_tgemm `      return;`
   }
/*
 * If we are debugging, set up serial combine queue
 */
   #ifdef ATL_SERIAL_COMBINE
      for (i=0; i < ATL_NTHREADS; i++)
      {
         if (mms[i].K)   /* if this struct being used */
         {
            if (!mms[i].ownC)   /* I need a workspace for C */
            {
               mms[i].Cw = calloc(mms[i].ldcw * mms[i].N, ATL_sizeof);
               ATL_assert(mms[i].Cw);
               combb = ATL_NewCombnode(mms[i].M, mms[i].N, mms[i].Cw, 
                                       mms[i].ldcw, mms[i].C, mms[i].ldc,
                                       combb);
            }
         }
      }
   #endif

@beginskip
   ls.opstruct = (char*) mms;
   ls.opstructstride = (int) ( ((char*)(mms+1)) - (char*)mms );
   ls.OpStructIsInit = ATL_StructIsInitMM;
@ROUT ATL_tgemm_M ATL_tgemm_N
   ls.CombineOpStructs = NULL;
@ROUT ATL_tgemm ATL_tgemm_rec ATL_tgemm_K ATL_tgemm_p
   ls.CombineOpStructs = DividedK ? Mjoin(PATL,CombineStructsMM) : NULL;
@ROUT ATL_tgemm ATL_tgemm_rec ATL_tgemm_M ATL_tgemm_N ATL_tgemm_K ATL_tgemm_p
   ls.DoWork = ATL_DoWorkMM;
   ls.rank2thr = tp;
   for (i=0; i < ATL_NTHREADS; i++)
   {
      tp[i].vp = &ls;
      tp[i].rank = i;
   }
   ATL_thread_start(tp, 0, 1, ATL_log2tlaunch, tp);
   ATL_thread_join(tp);
@endskip
@ROUT ATL_tgemm ATL_tgemm_K ATL_tgemm_p ATL_tgemm_rec
   ATL_goparallel(np, ATL_DoWorkMM, mms, 
                  DividedK ? Mjoin(PATL,CombineStructsMM) : NULL);
@ROUT ATL_tgemm_M ATL_tgemm_N
   ATL_goparallel(np, ATL_DoWorkMM, mms, NULL);
@ROUT ATL_tgemm ATL_tgemm_rec ATL_tgemm_M ATL_tgemm_N ATL_tgemm_K ATL_tgemm_p
/*
 * If we are debugging, serially combine all workspaces back to original C
 */
   #ifdef ATL_SERIAL_COMBINE
      while(combb)
      {
         Mjoin(PATL,geadd)(combb->M, combb->N, ONE, combb->W, combb->ldw,
                           ONE, combb->D, combb->ldd);
         free(combb->W);
         combp = combb;
         combb = combb->next;
         free(combp);
      }
   #endif
@ROUT ATL_tgemm_rec ATL_tgemm_M ATL_tgemm_N ATL_tgemm_K ATL_tgemm_p `   return(np);`
}
@ROUT ATL_tgemm

void Mjoin(PATL,tvgemm)(const enum ATLAS_TRANS TA, const enum ATLAS_TRANS TB,
                        ATL_CINT M, ATL_CINT N, ATL_CINT K, const void *alpha,
                        const void *A, ATL_CINT lda, const void *B,ATL_CINT ldb,
                        const void *beta, void *C, ATL_CINT ldc)
/* 
 * This void wrapper for tgemm is used in some typeless structures
 */
{
   Mjoin(PATL,tgemm)(TA, TB, M, N, K, SVVAL((const TYPE*)alpha), A, lda,
                     B, ldb, SVVAL((const TYPE*)beta), C, ldc);
}
@ROUT ATL_tsymm
   @define rt @symm@
   @define trans @AtlasTrans@
@ROUT ATL_themm
   @define rt @hemm@
   @define trans @AtlasConjTrans@
@ROUT ATL_themm ATL_tsymm
#include "atlas_misc.h"
@skip #define ATL_LAUNCHORDER         /* we want static ATL_launchorder array */
#include "atlas_threads.h"
#include "atlas_tlvl3.h"
int Mjoin(PATL,StructIsInit@up@(rt))(void *vp)
{
   return(((ATL_TSYMM_t*)vp)->M);
}

void Mjoin(PATL,DoWork@up@(rt))(ATL_LAUNCHSTRUCT_t *lp, void *vp)
{
   ATL_thread_t *tp = vp;
   ATL_TSYMM_t *sp=((ATL_TSYMM_t*)lp->opstruct)+tp->rank;
   Mjoin(PATL,@(rt))(sp->side, sp->uplo, sp->M, sp->N, SVVAL((TYPE*)sp->alpha),
                     sp->A, sp->lda, sp->B, sp->ldb, SVVAL((TYPE*)sp->beta),
                     sp->C, sp->ldc);
}

static void ATL_@(rt)L_rec
   (ATL_TSYMM_t *syp, ATL_CINT Mblks, ATL_CINT mr, ATL_CINT Nblks, ATL_CINT nr,
    const TYPE *A, const TYPE *B, TYPE *C)
{
   const TYPE *A10, *A01, *B10;
   TYPE *C10;
   const int nb = syp->nb;
   ATL_INT nbR, nbL, rR, rL, nL, nR;
   #ifdef TCPLX
      TYPE ONE[2] = {ATL_rone,ATL_rzero};
   #else
      TYPE ONE = ATL_rone;
   #endif

   nbR = Mblks>>1;
   nbL = Mblks - nbR;
/*
 * Stop recursion once we are no longer getting parallelism
 */
@skip   if (nbR*Nblks < ATL_TGEMM_XOVER)
   if (Mjoin(PATL,threadMM)(AtlasNoTrans, AtlasNoTrans, 
                            nbR*nb, Nblks*nb+nr, nbR*nb) < 2)
   {
      Mjoin(PATL,@(rt))(syp->side, syp->uplo, Mblks*nb+mr, syp->N, 
                       SVVAL((TYPE*)syp->alpha), A, syp->lda, B, syp->ldb,
                       SVVAL((TYPE*)syp->beta), C, syp->ldc);
      return;
   }
   rL = (nbR == nbL) ? mr : 0;
   rR = mr - rL;
   nL = nbL*nb + rL;
   nR = nbR*nb + rR;
   B10 = B + (nL SHIFT);
   C10 = C + (nL SHIFT);
   ATL_@(rt)L_rec(syp, nbL, rL, Nblks, nr, A, B, C);
   ATL_@(rt)L_rec(syp, nbR, rR, Nblks, nr, A+(syp->lda+1)*(nL SHIFT), B10, C10);
   if (syp->uplo == AtlasLower)
   {
      A10 = A + (nL SHIFT);
      Mjoin(PATL,tgemm)(@(trans), AtlasNoTrans, nL, syp->N, nR, 
                        SVVAL((TYPE*)syp->alpha), A10, syp->lda, B10, syp->ldb, 
                        ONE, C, syp->ldc);
      Mjoin(PATL,tgemm)(AtlasNoTrans, AtlasNoTrans, nR, syp->N, nL, 
                        SVVAL((TYPE*)syp->alpha), A10, syp->lda, B, syp->ldb, 
                        ONE, C10, syp->ldc);
   }
   else
   {
      A01 = A + nL*(syp->lda SHIFT);
      Mjoin(PATL,tgemm)(AtlasNoTrans, AtlasNoTrans, nL, syp->N, nR, 
                        SVVAL((TYPE*)syp->alpha), A01, syp->lda, B10, syp->ldb, 
                        ONE, C, syp->ldc);
      Mjoin(PATL,tgemm)(@(trans), AtlasNoTrans, nR, syp->N, nL, 
                        SVVAL((TYPE*)syp->alpha), A01, syp->lda, B, syp->ldb, 
                        ONE, C10, syp->ldc);
   }
}
static void ATL_@(rt)R_rec
   (ATL_TSYMM_t *syp, ATL_CINT Mblks, ATL_CINT mr, ATL_CINT Nblks, ATL_CINT nr,
    const TYPE *A, const TYPE *B, TYPE *C)
{
   const TYPE *A10, *A01, *B01;
   TYPE *C01;
   const int nb = syp->nb;
   ATL_INT nbR, nbL, rR, rL, nL, nR;
   #ifdef TCPLX
      TYPE ONE[2] = {ATL_rone,ATL_rzero};
   #else
      TYPE ONE = ATL_rone;
   #endif

   nbR = Nblks>>1;
   nbL = Nblks - nbR;
/*
 * Stop recursion once we are no longer getting parallelism
 */
@skip   if (nbR*Mblks < ATL_TGEMM_XOVER)
   if (Mjoin(PATL,threadMM)(AtlasNoTrans, AtlasNoTrans, 
                            Mblks*nb+mr, nbR*nb, nbR*nb) < 2)
   {
      Mjoin(PATL,@(rt))(syp->side, syp->uplo, syp->M, Nblks*nb+nr,
                       SVVAL((TYPE*)syp->alpha), A, syp->lda, B, syp->ldb,
                       SVVAL((TYPE*)syp->beta), C, syp->ldc);
      return;
   }
   rL = (nbR == nbL) ? nr : 0;
   rR = nr - rL;
   nL = nbL*nb + rL;
   nR = nbR*nb + rR;
   B01 = B + (nL*syp->ldb SHIFT);
   C01 = C + (nL*syp->ldc SHIFT);
   ATL_@(rt)L_rec(syp, Mblks, mr, nbL, rL, A, B, C);
   ATL_@(rt)L_rec(syp, Mblks, mr, nbR, rR, A+(syp->lda+1)*(nL SHIFT), B01, C01);
   if (syp->uplo == AtlasLower)
   {
      A10 = A + (nL SHIFT);
      Mjoin(PATL,tgemm)(AtlasNoTrans, AtlasNoTrans, syp->M, nL, nR, 
                        SVVAL((TYPE*)syp->alpha), B01, syp->ldb, A10, syp->lda, 
                        ONE, C, syp->ldc);
      Mjoin(PATL,tgemm)(AtlasNoTrans, @(trans), syp->M, nR, nL, 
                        SVVAL((TYPE*)syp->alpha), B, syp->ldb, A10, syp->lda, 
                        ONE, C01, syp->ldc);
   }
   else
   {
      A01 = A + (syp->lda SHIFT);
      Mjoin(PATL,tgemm)(AtlasNoTrans, @(trans), syp->M, nL, nR, 
                        SVVAL((TYPE*)syp->alpha), B01, syp->ldb, A01, syp->lda, 
                        ONE, C, syp->ldc);
      Mjoin(PATL,tgemm)(AtlasNoTrans, AtlasNoTrans, syp->M, nR, nL, 
                        SVVAL((TYPE*)syp->alpha), B, syp->ldb, A01, syp->lda, 
                        ONE, C01, syp->ldc);
   }
}

static void ATL_t@(rt)_SYsplit
   (const enum ATLAS_SIDE Side, const enum ATLAS_UPLO Uplo, 
    ATL_CINT M, ATL_CINT N, const SCALAR alpha, const TYPE *A, ATL_CINT lda, 
    const TYPE *B, ATL_CINT ldb, const SCALAR beta, TYPE *C, ATL_CINT ldc,
    ATL_CINT nb)
/*
 * This routine is specialized for the case where we cannnot split the
 * B matrix, and must instead split the symmetric matrix (A).  It calls
 * a recursive GEMM-based BLAS, that gets its parallel performance from
 * calling threaded GEMM.
 */
{
   ATL_TSYMM_t ss;
   ss.side = Side;
   ss.uplo = Uplo;
   ss.M = M;
   ss.N = N;
   ss.nb = nb;
   ss.alpha = SADD alpha;
   ss.beta  = SADD beta;
   ss.lda = lda;
   ss.ldb = ldb;
   ss.ldc = ldc;
   if (Side == AtlasLeft)
      ATL_@(rt)L_rec(&ss, M/nb, M%nb, N/nb, N%nb, A, B, C);
   else
      ATL_@(rt)R_rec(&ss, M/nb, M%nb, N/nb, N%nb, A, B, C);

}

/*
 * The XOVER is the min # of blocks required to do parallel operation
 */
#ifndef ATL_TSYMM_XOVER
   #ifdef ATL_TGEMM_XOVER
      #define ATL_TSYMM_XOVER ATL_TGEMM_XOVER
   #else
      #define ATL_TSYMM_XOVER 4  /* want 4 blocks for parallel execution */
   #endif
#endif
/*
 * Once you have achieved enough blocks to do computation, minimum number
 * of blocks to give each processor
 */
#ifndef ATL_TSYMM_ADDP
   #ifdef ATL_TGEMM_ADDP 
      #define ATL_TSYMM_ADDP  ATL_TGEMM_ADDP 
   #else
      #define ATL_TSYMM_ADDP  1  /* want 1 blocks to add proc to workers */
   #endif
#endif
void Mjoin(PATL,t@(rt))
   (const enum ATLAS_SIDE Side, const enum ATLAS_UPLO Uplo, 
    ATL_CINT M, ATL_CINT N, const SCALAR alpha, const TYPE *A, ATL_CINT lda, 
    const TYPE *B, ATL_CINT ldb, const SCALAR beta, TYPE *C, ATL_CINT ldc)
{
   ATL_INT n, nblks, tblks, nr, minblks, extrablks, p, i, j;
   ATL_thread_t tp[ATL_NTHREADS];
   ATL_TSYMM_t symms[ATL_NTHREADS];
   ATL_LAUNCHSTRUCT_t ls;
   const TYPE *b;
   TYPE *c;
   static int nb=0;

   if (M < 1 || N < 1)
      return;
   if (SCALAR_IS_ZERO(alpha))
   {
      if (!SCALAR_IS_ONE(beta))
         Mjoin(PATL,gescal)(M, N, beta, C, ldc);
      return;
   }
   if (!nb) nb = Mjoin(PATL,GetNB());
   if (Side == AtlasLeft)
   {
      nblks = N / nb;
      nr = N - nblks*nb;
      tblks = ((double)(M*N)) / ( (double)nb * nb );
      p = (nblks+ATL_TSYMM_ADDP-1)/ATL_TSYMM_ADDP;
      if (p < ATL_NTHREADS)  /* N not big enough to give blk to each proc */
      {
/*
 *       If I can't split N, and M is the dominant cost, use recursion to
 *       decompose symmetric matrix; parallelism will come from TGEMM calls
 */
         if (M > (N<<(ATL_NTHRPOW2+2)))
         {
            ATL_t@(rt)_SYsplit(Side, Uplo, M, N, alpha, A, lda, B, ldb, 
                              beta, C, ldc, nb);
            return;
         }
      }
      else
         p = ATL_NTHREADS;
      if (p < 2)
         goto SERIAL;
/*
 *    Distribute N over the processors
 */
      b = B;
      c = C;
      minblks = nblks / p;
      extrablks = nblks - minblks*p;
      for (i=0; i < p; i++)
      {
         if (i < extrablks)
            n = (minblks+1)*nb;
         else if (i == extrablks)
            n = minblks*nb + nr;
         else
            n = minblks*nb;
@skip         j = ATL_launchorder[i];
         symms[i].A = A;
         symms[i].B = b;
         symms[i].alpha = SADD alpha;
         symms[i].beta = SADD beta;
         symms[i].C = c;
         symms[i].M = M;
         symms[i].N = n;
         symms[i].lda = lda;
         symms[i].ldb = ldb;
         symms[i].ldc = ldc;
         symms[i].side = Side;
         symms[i].uplo = Uplo;
         b = MindxT(b, ATL_MulBySize((size_t)ldb)*n);
         c = MindxT(c, ATL_MulBySize((size_t)ldc)*n);
      }
      for (; i < ATL_NTHREADS; i++)  /* flag rest of struct as uninitialized */
         symms[i].M = 0;
   }
   else  /* Side == AtlasRight */
   {
      nblks = M / nb;
      nr = M - nblks*nb;
      tblks = ((double)(M*N)) / ( (double)nb * nb );
      p = (nblks+ATL_TSYMM_ADDP-1)/ATL_TSYMM_ADDP;
      if (p < ATL_NTHREADS)  /* N not big enough to give blk to each proc */
      {
/*
 *       If I can't split M, and N is the dominant cost, use recursion to
 *       decompose symmetric matrix; parallelism will come from TGEMM calls
 */
         if (N > (M<<(ATL_NTHRPOW2+2)))
         {
            ATL_t@(rt)_SYsplit(Side, Uplo, M, N, alpha, A, lda, B, ldb, 
                              beta, C, ldc, nb);
            return;
         }
      }
      else
         p = ATL_NTHREADS;
      if (p < 2)
         goto SERIAL;
/*
 *    Distribute M over the processors
 */
      b = B;
      c = C;
      minblks = nblks / p;
      extrablks = nblks - minblks*p;
      for (i=0; i < p; i++)
      {
         if (i < extrablks)
            n = (minblks+1)*nb;
         else if (i == extrablks)
            n = minblks*nb + nr;
         else
            n = minblks*nb;
@skip         j = ATL_launchorder[i];
         symms[i].A = A;
         symms[i].B = b;
         symms[i].alpha = SADD alpha;
         symms[i].beta = SADD beta;
         symms[i].C = c;
         symms[i].M = n;
         symms[i].N = N;
         symms[i].lda = lda;
         symms[i].ldb = ldb;
         symms[i].ldc = ldc;
         symms[i].side = Side;
         symms[i].uplo = Uplo;
         b = MindxT(b, ATL_MulBySize((size_t)n));
         c = MindxT(c, ATL_MulBySize((size_t)n));
      }
      for (; i < ATL_NTHREADS; i++)  /* flag rest of struct as uninitialized */
         symms[i].M = 0;
   }
   if (p < 2)
   {
SERIAL:
      Mjoin(PATL,@(rt))(Side, Uplo, M, N, alpha, A, lda, B, ldb, beta, C, ldc);
      return;
   }
   ATL_goparallel(p, Mjoin(PATL,DoWork@up@(rt)), symms, NULL);
@beginskip
   ls.opstruct = (char*) symms;
   ls.opstructstride = (int) ( ((char*)(symms+1)) - (char*)(symms) );
   ls.CombineOpStructs = NULL;
   ls.OpStructIsInit = Mjoin(PATL,StructIsInit@up@(rt));
   ls.DoWork = Mjoin(PATL,DoWork@up@(rt));
   ls.rank2thr = tp;
   for (i=0; i < ATL_NTHREADS; i++)
   {
      tp[i].vp = &ls;
      tp[i].rank = i;
   }
   ATL_thread_start(tp, 0, 1, ATL_tlaunch, tp);
   ATL_thread_join(tp);
@endskip
}

@ROUT ATL_ttrmm
   @define rt @trmm@
@ROUT ATL_ttrsm
   @define rt @trsm@
@ROUT ATL_ttrsm ATL_ttrmm
#include "atlas_misc.h"
@skip #define ATL_LAUNCHORDER         /* we want static ATL_launchorder array */
#include "atlas_threads.h"
#define ATL_ESTNCTR 1
#include "atlas_tlvl3.h"
#include "atlas_ttypes.h"

/*
 * For very small problems, use static scheduling to avoid dynamic overheads
 * Since we currently wait for all the threads to go to sleep before returning,
 * static scheduling essentially cannot be slower for tiny problems.
 */
void Mjoin(PATL,tDo@up@(rt)_tTRs)(void *vpp, int rank, int vrank)
{
   ATL_tpool_t *pp=vpp;
   ATL_ttrsm_tTR_t *pd = pp->PD;
   const enum ATLAS_SIDE side = pd->side;
   const size_t ldb=pd->ldb;
   const int rb=pd->rb, rbF = pd->rbF;
   const int nrhs = (vrank) ? rb : rbF;
   TYPE *X = pd->B;

//printf("%d: P=%d, nrhsL=%d, rbF=%d\n", vrank, pp->nworkers, nrhs, pd->rbF);
   if (side == AtlasRight)
   {
      X += (vrank) ? ((rbF + (vrank-1)*rb)SHIFT) : 0;
      Mjoin(PATL,@(rt))(side, pd->uplo, pd->TA, pd->diag, nrhs, pd->N, 
                       pd->alpha, pd->A, pd->lda, X, ldb);
   }
   else
   {
      X += (vrank) ? (((rbF + (vrank-1)*rb)*ldb)SHIFT) : 0;
      Mjoin(PATL,@(rt))(side, pd->uplo, pd->TA, pd->diag, pd->M, nrhs, 
                       pd->alpha, pd->A, pd->lda, X, ldb);
   }
}

void Mjoin(PATL,tDo@up@(rt)_tTR)(void *vpp, int rank, int vrank)
{
   ATL_tpool_t *pp=vpp;
   ATL_ttrsm_tTR_t *pd = pp->PD;
   const unsigned int rb=pd->rb, rbF=pd->rbF, nrblks=pd->nrblks;
   const enum ATLAS_SIDE side = pd->side;
   enum ATLAS_TRANS TA=pd->TA;
   enum ATLAS_DIAG uplo=pd->uplo;
   enum ATLAS_DIAG diag=pd->diag;
   ATL_CINT M=pd->M, N=pd->N;
   const size_t lda=pd->lda, ldb=pd->ldb;
   int rcnt, rblk;
   TYPE *X = pd->B;
   const TYPE *A = pd->A;
   void *rhsBlkCtr = pd->rhsBlkCtr;
   const SCALAR alpha = pd->alpha;

   while ((rcnt = ATL_DecGlobalAtomicCount(rhsBlkCtr, vrank)))
   {
      const int rblk = nrblks - rcnt;
      const int nrhs = (rcnt != 1) ? rb : rbF;
      if (side == AtlasRight)
         Mjoin(PATL,@(rt))(side, uplo, TA, diag, nrhs, N, alpha, 
                          A, lda, X+rblk*(rb SHIFT), ldb);
      else
         Mjoin(PATL,@(rt))(side, uplo, TA, diag, M, nrhs, alpha,
                          A, lda, X+rblk*rb*(ldb SHIFT), ldb);
   }
}

@ROUT ATL_ttrsm
/*
 * This routine specialized for small triangular matrix
 */
void Mjoin(PATL,ttrsm_tTR)
   (const enum ATLAS_SIDE side, const enum ATLAS_UPLO uplo,
    const enum ATLAS_TRANS TA, const enum ATLAS_DIAG diag,
    ATL_CINT M, ATL_CINT N, const SCALAR alpha,
    const TYPE *A, ATL_CINT lda, TYPE *B, ATL_CINT ldb)
{
   ATL_ttrsm_tTR_t pd;
   ATL_INT nrhs = (side == AtlasLeft) ? N : M;
   ATL_INT nrblks;
   int rb, rbF, P;
   ATL_tpjfunc_t DoWork = Mjoin(PATL,tDoTRSM_tTR);
   size_t tblks;

   tblks = (((size_t)M)*N)>>12;
   tblks = (tblks+3)>>1;
   P = Mmin(ATL_NTHREADS,tblks);
   rb = nrhs>>1;
   P = Mmin(P, rb);
   if (P < 2)
   {
      Mjoin(PATL,trsm)(side, uplo, TA, diag, M, N, alpha, A, lda, B, ldb);
      return;
   }
   if (nrhs < (P<<6))
   {
      nrblks = P;
      if (nrhs >= (P<<2))
         rb = (nrhs / (P<<2))<<2;
      else
         rb = nrhs / P;
      rbF = rb + nrhs - nrblks*rb;
      DoWork = Mjoin(PATL,tDoTRSM_tTRs);
   }
   else
   {

/*
 *    Try block being a multple of 60 (LCM(1,2,3,4,5), so good for NU of amm)
 */
      rb = nrhs/60;

      if (rb >= (P<<4))                  /* we've got enough to give everyone */
         rb = ((rb+P+P-1)/(P<<1))*60;    /* 2 full jobs for load balancing */
      else                               /* we've got enough to give everyone */
         rb = ((rb+P-1)/P)*60;           /* 1 job: screw load balancing */
      nrblks = nrhs / rb;
      rbF = nrhs - nrblks*rb;
      if (!rbF)
         rbF = rb;
      else
         nrblks++;
   }
//printf("M=%d, N=%d, P=%d, nrblks=%d, rb=%d, rbF=%d, R=%d \n", 
//       M, N, P, nrblks, rb, rbF, rb*(nrblks-1)+rbF);
   pd.side = side;
   pd.uplo = uplo;
   pd.TA = TA;
   pd.diag = diag;
   pd.M = M;
   pd.N = N;
   pd.alpha = alpha;
   pd.A = A;
   pd.lda = lda;
   pd.B = B;
   pd.ldb = ldb;
   pd.nrblks = nrblks;
   pd.rb = rb;
   pd.rbF = rbF;
   pd.rhsBlkCtr = ATL_SetGlobalAtomicCount(ATL_EstNctr(nrblks, P), nrblks, 0);
   ATL_goParallel(P, DoWork, NULL, &pd, NULL);
   ATL_FreeGlobalAtomicCount(pd.rhsBlkCtr);
}
@ROUT ATL_ttrsm ATL_ttrmm

@ROUT ATL_ttrsm ATL_ttrmm
@beginskip
int Mjoin(PATL,StructIsInit@up@(rt))(void *vp)
{
   return(((ATL_TTRSM_t*)vp)->B != NULL);
}

void Mjoin(PATL,tvtrsm)(ATL_TTRSM_t *tp)
{
   Mjoin(PATL,trsm)(tp->side, tp->uplo, tp->TA, tp->diag, tp->M, tp->N,
                    SVVAL((TYPE*)tp->alpha), tp->A, tp->lda, tp->B, tp->ldb);
}

void Mjoin(PATL,DoWork@up@(rt))(ATL_LAUNCHSTRUCT_t *lp, void *vp)
{
   ATL_thread_t *thp=vp;
   ATL_TTRSM_t *tp=((ATL_TTRSM_t*)lp->opstruct) + thp->rank;
   Mjoin(PATL,@(rt))(tp->side, tp->uplo, tp->TA, tp->diag, tp->M, tp->N,
                    SVVAL((TYPE*)tp->alpha), tp->A, tp->lda, tp->B, tp->ldb);
}

@endskip
@ROUT ATL_ttrsm ATL_ttrmm
#ifndef ATL_TTRSM_XOVER
   #define ATL_TTRSM_XOVER 4   /* want 4 total blocks before adding proc */
#endif
void Mjoin(PATL,t@(rt))(const enum ATLAS_SIDE side, const enum ATLAS_UPLO uplo, 
                       const enum ATLAS_TRANS TA, const enum ATLAS_DIAG diag,
                       ATL_CINT M, ATL_CINT N, const SCALAR alpha,
                       const TYPE *A, ATL_CINT lda, TYPE *B, ATL_CINT ldb)
{
   ATL_ttrsm_tTR_t pd;
   ATL_INT nrhs = (side == AtlasLeft) ? N : M;
   ATL_INT nrblks;
   int rb, rbF, P;
   size_t tblks;

   if (M < 1 || N < 1)
      return;
   if (SCALAR_IS_ZERO(alpha))
   {
      Mjoin(PATL,gezero)(M, N, B, ldb);
      return;
   }
@ROUT ATL_ttrsm
   #if 0
   if (0)
   {
      Mjoin(PATL,ttrsm_tTR)(side, uplo, TA, diag, M, N, alpha, A, lda, B, ldb);
      return;
   }
   #endif
   #if defined(ATL_ARCH_XeonPHI) && defined(TREAL)
   {
      int Mjoin(PATL,ttrsm_amm)
         (const enum ATLAS_SIDE side, const enum ATLAS_UPLO uplo,
          const enum ATLAS_TRANS TA, const enum ATLAS_DIAG diag,
          ATL_CINT M, ATL_CINT N, const SCALAR alpha,
          const TYPE *A, ATL_CINT lda, TYPE *B, ATL_CINT ldb);
     if (!Mjoin(PATL,ttrsm_amm)(side, uplo, TA, diag, M, N, alpha, 
                                A, lda, B, ldb))
        return;
   }
   #endif
@ROUT ATL_ttrsm ATL_ttrmm
   tblks = (((size_t)M)*N)>>12;
   tblks = (tblks+3)>>2;
   P = Mmin(ATL_NTHREADS,tblks);
   rb = nrhs>>1;
   P = Mmin(P, rb);
   if (P < 2)
   {
      Mjoin(PATL,@(rt))(side, uplo, TA, diag, M, N, alpha, A, lda, B, ldb);
      return;
   }
   nrblks = P;
   if (nrhs >= (P<<2))
      rb = (nrhs / (P<<2))<<2;
   else
      rb = nrhs / P;
   rbF = rb + nrhs - nrblks*rb;
   pd.side = side;
   pd.uplo = uplo;
   pd.TA = TA;
   pd.diag = diag;
   pd.M = M;
   pd.N = N;
   pd.alpha = alpha;
   pd.A = A;
   pd.lda = lda;
   pd.B = B;
   pd.ldb = ldb;
   pd.nrblks = nrblks;
   pd.rb = rb;
   pd.rbF = rbF;
   pd.rhsBlkCtr = NULL;
   ATL_goParallel(P, Mjoin(PATL,tDo@up@(rt)_tTRs), NULL, &pd, NULL);
}
@ROUT ATL_Xtsyr2k
#include "atlas_misc.h"
#include "atlas_threads.h"
#include "atlas_tlvl3.h"
                  
static int ATL_tsyr2kK(ATL_SYR2K_t *syp, ATL_CINT N, ATL_CINT K, 
                       const void *A, const void *B, void *C)
/*
 * Attempts to allocate workspace W, then do:
 *   (1) W = alpha*A*B' or alpha*A'B (GEMM)
 *   (2) C <- beta*C + W + W'
 * RETURNS: 0 on success, nonzero if unable to allocate memory
 */
{
   void *v, *W;
   ATL_INT ldw;
   int i;
   size_t sz;
   const int eltsh = syp->eltsh;

/*
 * Make ldw a multiple of 4 that is not a power of 2
 */
   ldw = ((N+3)>>2)<<2;
   if (!(ldw&(ldw-1)))
      ldw += 4;
@beginskip
   for (i=0; i <= sizeof(ldw)*8; i++)
   {
      if (!(ldw^(1<<i)))
      {
         ldw += 4;
         break;
      }
   }
@endskip
   sz = (ldw*N)<<eltsh;
   if (sz <= ATL_NTHREADS*ATL_PTMAXMALLOC)
      v = malloc(sz + ATL_Cachelen);
   if (!v)
      return(1);  /* signal we can't get memory */

   W = ATL_AlignPtr(v);
   syp->tvgemm(syp->TA, syp->TB, N, N, K, syp->alpha, A, syp->lda, B, syp->ldb,
               syp->zero, W, ldw);
   syp->tvApAt(syp->Uplo, N, W, ldw, syp->beta, C, syp->ldc);
   free(v);
   return(0);
}

void ATL_tvsyr2k_rec
   (ATL_SYR2K_t *syp, ATL_CINT Nblks, ATL_CINT nr, const void *A, 
    const void *B, void *C)
/*
 * Do SYR2K/HER2K, either by mallocing space and calling GEMM, or recurring
 * until C is small enough that space can be allocated.  Gets its parallelism
 * from the calls to parallel GEMM
 */
{
   const int nb = syp->nb, eltsh = syp->eltsh;
   ATL_INT nL, nR, nbL, nbR, rL, rR;
   void *gc, *sc;   /* ptr to C to update with gemm & 2nd syr2k call, resp */
   void *A1, *B1;   /* ptr to 2nd block of a/b resp */
/*
 * Attempt to halt recursion by allocating workspace, and calling GEMM
 */
   if (!ATL_tsyr2kK(syp, Nblks*nb+nr, syp->K, A, B, C))
      return;
   ATL_assert(Nblks>1 || (Nblks==1 && nr));  /* must have something to split */
/*
 * Must recur in order to make problem small enough to allocate C workspace
 */
   nbR = Nblks>>1;
   nbL = Nblks - nbR;
   rL = (nL == nR) ? nr : 0;
   rR = nr - rL;
   nL = nbL*nb + rL;
   nR = nbR*nb + rR;
   sc = MindxT(C, (((size_t)nL*(syp->ldc+1))<<eltsh));
   if (syp->trans == AtlasNoTrans)
   {
      A1 = MindxT(A, ((size_t)nL<<eltsh));
      B1 = MindxT(B, ((size_t)nL<<eltsh));
   }
   else  /* index like transpose */
   {
      A1 = MindxT(A, (((size_t)nL*syp->lda)<<eltsh));
      B1 = MindxT(B, (((size_t)nL*syp->ldb)<<eltsh));
   }

   ATL_tvsyr2k_rec(syp, nbL, rL, A, B, C);
   if (syp->Uplo == AtlasUpper)
   {
      gc = MindxT(C, (((size_t)nL*syp->ldc)<<eltsh));
      syp->tvgemm(syp->TA, syp->TB, nL, nR, syp->K, syp->alpha, A, syp->lda, 
                  B1, syp->ldb, syp->beta, gc, syp->ldc);
      syp->tvgemm(syp->TA, syp->TB, nL, nR, syp->K, syp->alpha2, B, syp->ldb, 
                  A1, syp->lda, syp->one, gc, syp->ldc);
   }
   else
   {
      gc = MindxT(C, ((size_t)nL<<eltsh));
      syp->tvgemm(syp->TA, syp->TB, nR, nL, syp->K, syp->alpha, A1, syp->lda, 
                  B, syp->ldb, syp->beta, gc, syp->ldc);
      syp->tvgemm(syp->TA, syp->TB, nR, nL, syp->K, syp->alpha2, B1, syp->ldb, 
                  A, syp->lda, syp->one, gc, syp->ldc);
   }
   ATL_tvsyr2k_rec(syp, nbR, rR, A1, B1, sc);

}

@ROUT ATL_tsyr2k
   @define rt @syr2k@
   @define ApA @syApAt@
   @define trans @AtlasTrans@
@ROUT ATL_ther2k
   @define rt @her2k@
   @define ApA @heApAc@
   @define trans @AtlasConjTrans@
@ROUT ATL_tsyr2k ATL_ther2k
#include "atlas_misc.h"
#include "atlas_threads.h"
#include "atlas_tlvl3.h"
#include "atlas_lvl3.h"

void Mjoin(PATL,tv@(ApA))(const enum ATLAS_UPLO Uplo, ATL_CINT N, const void *A,
                          ATL_CINT lda, const void *beta, void *C, ATL_CINT ldc)
{
   Mjoin(PATL,@(ApA))(Uplo, N, A, lda, SVVAL((const TYPE*)beta), C, ldc);
}

void Mjoin(PATL,t@(rt))
   (const enum ATLAS_UPLO Uplo, const enum ATLAS_TRANS Trans,
    ATL_CINT N, ATL_CINT K, const SCALAR alpha, const TYPE *A, ATL_CINT lda,
@ROUT ATL_tsyr2k
    const TYPE *B, ATL_CINT ldb, const SCALAR beta, TYPE *C, ATL_CINT ldc)
@ROUT ATL_ther2k
    const TYPE *B, ATL_CINT ldb, const TYPE beta0, TYPE *C, ATL_CINT ldc)
@ROUT ATL_tsyr2k ATL_ther2k
{
   ATL_SYR2K_t sy;
   #ifdef TREAL
      const TYPE ONE = ATL_rone, ZERO = ATL_rzero;
   #else
      const TYPE ONE[2]={ATL_rone, ATL_rzero}, ZERO[2]={ATL_rzero,ATL_rzero};
@ROUT ATL_ther2k 
      const TYPE alpha2[2]={*alpha,(alpha[1]!=ATL_rzero)?-alpha[1]:ATL_rzero};
      const TYPE beta[2] = {beta0, ATL_rzero};
@ROUT ATL_tsyr2k ATL_ther2k
   #endif

   if (N < 1)
      return;
   if (SCALAR_IS_ZERO(alpha) || K < 1)
   {
@ROUT ATL_ther2k 
      if (beta0 != ATL_rone)
         Mjoin(PATL,hescal)(Uplo, N, N, beta0, C, ldc);
@ROUT ATL_tsyr2k 
      if (!SCALAR_IS_ONE(beta))
         Mjoin(PATL,trscal)(Uplo, N, N, beta, C, ldc);
@ROUT ATL_tsyr2k ATL_ther2k
      return;
   }
/*
 * This call to serial is usually a performance optimization, but was actually
 * put in to avoid a failure in the LAPACK eigenvalue tests that is caused
 * by subtractive cancellation.  There is no bug, but our parallel algorithm
 * does the operations in a different order than small-case serial, and in
 * one case used by the lapack testers, this causes subtractive cancellation
 * to occur (the serial code can use different orders for differing problem
 * sizes).  Other problems will experience cancelation under the serial order,
 * so AFAIK, neither is more correct numerically.
 */
   if (N < 3*ATL_AMM_LLCMU && K < 3*ATL_AMM_LLCMU)
   {
@ROUT ATL_tsyr2k
      Mjoin(PATL,syr2k)(Uplo, Trans, N, K, alpha, A, lda, B, ldb, beta, C, ldc);
@ROUT ATL_ther2k 
      Mjoin(PATL,her2k)(Uplo, Trans, N, K, alpha, A, lda, B, ldb, 
                        beta0, C, ldc);
@ROUT ATL_tsyr2k ATL_ther2k
      return;
   }
@ROUT ATL_tsyr2k
   sy.alpha2 = sy.alpha = SADD alpha;
   sy.beta  = SADD beta;
@ROUT ATL_ther2k
   sy.alpha = SADD alpha;
   sy.alpha2 = alpha2;
   sy.beta  = beta;
@ROUT ATL_tsyr2k ATL_ther2k
   sy.one = SADD ONE;
   sy.zero = SADD ZERO;
   sy.tvgemm = Mjoin(PATL,tvgemm);
   sy.tvApAt = Mjoin(PATL,tv@(ApA));
   sy.K = K;
   sy.lda = lda;
   sy.ldb = ldb;
   sy.ldc = ldc;
   sy.eltsh = Mjoin(PATL,shift);
   sy.Uplo = Uplo;
   sy.trans = Trans;
   if (Trans == AtlasNoTrans)
   {
      sy.TA = AtlasNoTrans;
      sy.TB = @(trans);
      sy.TA2 = @(trans);
      sy.TB2 = AtlasNoTrans;
   }
   else
   {
      sy.TA = @(trans);
      sy.TB = AtlasNoTrans;
      sy.TA2 = AtlasNoTrans;
      sy.TB2 = @(trans);
   }
   sy.nb = Mjoin(PATL,GetNB)();
   ATL_tvsyr2k_rec(&sy, N/sy.nb, N%sy.nb, A, B, C);
}

@beginskip
/*
 * NOTE: want to put these primitives in src/auxil, then I write higher
 *       level drivers that call them on blocks
 */
#ifdef TREAL
void Mjoin(PATL,putAAt_L)
   (ATL_CINT N, const TYPE beta, const TYPE *A, ATL_CINT lda, 
    TYPE *L, ATL_CINT ldl)
/*
 * L <- A+At, L lower triangular
 */
{
   TYPE *Ar, *Ac=A;
   ATL_INT i, j;

   for (j=0; j < N; j++)
   {
      for (Ar=A+j,i=j; i < M; i++, Ar += lda)
      #ifdef BETA0
         Cc[i] = Ac[i] + *Ar;
      #elif defined(BETA1)
         Cc[i] += Ac[i] + *Ar;
      #else
         Cc[i] = beta*C[i] + Ac[i] + *Ar;
      #endif
      Ac += lda;
   }
}
void Mjoin(PATL,putABt_L)
   (ATL_CINT M, ATL_CINT N, const TYPE beta, const TYPE *A, ATL_CINT lda, 
    const TYPE *B, ATL_CINT ldb, TYPE *C, ATL_CINT ldc)
/*
 * C <- beta*C + A + B'
 */
{
   TYPE *Br;
   for (j=0; j < N; j++)
   {
      for (Br=B,i=0; i < M; i++, Br += ldb)
      #ifdef BETA0
         Cc[i] = A[i] + *Br;
      #elif defined(BETA1)
         Cc[i] += A[i] + *Br;
      #else
         Cc[i] = beta*C[i] + A[i] + *Br;
      #endif
      C += ldc;
      A += lda;
      B++;
   }
}
#else
#endif

void Mjoin(PATL,vsyr2k_putL)(ATL_CINT N, const void *beta0, const void *A, 
                             ATL_CINT lda, void *C, ATL_CINT ldc)
/* 
 * Takes A with property (A + A') = (A + A')', 
 *    C <- beta*C + A + A'
 * NOTE: This kernel is unblocked for initial try, which could cause TLB 
 *        disaster.  Need to write blocked version, and perhaps thread.
 */
{
   const TYPE *Ac = A, *Ar;
   const TYPE beta = *((const TYPE*)beta0);
   TYPE *Cc = C;
   ATL_CINT i, j;

   if (beta == ATL_rzero)
   {
      for (j=0; j != N; j++, Ac += lda, Cc += ldc)
      {
         for (Ar=Ac+j, i=j; i != N; i++, Ar += lda)
            Cc[i] = Ac[i] + *Ar;
      }
   }
   else if (beta == ATL_rone)
   {
      for (j=0; j != N; j++, Ac += lda, Cc += ldc)
      {
         for (Ar=Ac+j, i=j; i != N; i++, Ar += lda)
            Cc[i] += Ac[i] + *Ar;
      }
   }
   else
   {
      for (j=0; j != N; j++, Ac += lda, Cc += ldc)
      {
         for (Ar=Ac+j, i=j; i != N; i++, Ar += lda)
            Cc[i] = beta*Cc[i] + Ac[i] + *Ar;
      }
   }
}
@endskip
@ROUT ATL_Xtsyrk
#include "atlas_misc.h"
@skip #define ATL_LAUNCHORDER
#include "atlas_threads.h"
#include "atlas_tlvl3.h"
#include "math.h"
/*
 * Recursive decompositon on trapazoidal-shaped matrix ($C$ after splitting)
 */
#ifndef ATL_MINL3THRFLOPS
   #ifdef ATL_TGEMM_ADDP
      #define ATL_MINL3THRFLOPS \
         (((2.0*ATL_TGEMM_ADDP)*ATL_TGEMM_ADDP)*ATL_TGEMM_ADDP)
   #else
      #define ATL_MINL3THRFLOPS (((2.0*MB)*NB)*KB)
   #endif
#endif
@beginskip
int ATL_tsyrkdecomp_tr
   (ATL_TSYRK_t *psyrk, const int P, ATL_CINT Tblks, ATL_CINT tr, 
    ATL_CINT Mblks, ATL_CINT mr, ATL_CINT Nblks, ATL_CINT nr, ATL_CINT K,
    ATL_CINT ia, ATL_CINT ja, ATL_CINT ib, ATL_CINT jb, 
    ATL_CINT ic, ATL_CINT jc)
{
   double flops;
   double percL;  /* % of calculation to do on left size */
   int pL, pR; 
   ATL_INT i, nL, nR, rL, rR;
   const int nb = psyrk->nb;
  
   pR = (P>>1);
   pL = P - pR;
@skip pL=1; pR=P-1;   /* HERE HERE HERE: debug */
   percL = (pL == pR) ? 0.5 : ((double)pL)/((double)P);
   
/*
 * If problem is triangular, divide up problem so LEFT does SYRK+GEMM, and
 * RIGHT does SYRK only; this means nL = T(1-sqrt(percentL))
 */
   if (!Mblks && !Nblks)
   {
@skip      nL = 0.5 + ((pL == pR) ? 0.29289322*Tblks : 2.0*percL*Tblks*(1.0-0.5*sqrt(2)));
      nL = (0.58578664*Tblks)*percL;
      nR = Tblks - nL;
      flops = nR*nb;
      flops *= flops;
      flops *= K;
      if (P < 2 || !nL || !nR || flops < ATL_MINL3THRFLOPS)
      {
         psyrk->T = Tblks*nb + tr;
fprintf(stderr, "FLOPS=%.2f\n", (1.0*psyrk->T)*psyrk->T);
         psyrk->M = psyrk->N = 0;
         psyrk->ia = ia;
         psyrk->ja = ja;
         psyrk->ib = ib;
         psyrk->jb = jb;
         psyrk->ic = ic;
         psyrk->jc = jc;
         return(1);
      }
      rL = (nL > nR) ? 0 : tr;
      rR = tr - rL;
fprintf(stderr, "%d of %s\n", __LINE__, __FILE__);
      i = ATL_tsyrkdecomp_tr(psyrk, pL, nL, rL, nR, rR, nL, rL, K, 
                             ia, ja, ib, jb, ic, jc);
      i += ATL_tsyrkdecomp_tr(psyrk+pL, pR, nR, rR, 0, 0, 0, 0, K,
                              ia+nL*nb+rL, ja, ib, jb+nL*nb+rL, 
                              ic+nL*nb+rL, jc+nL*nb+rL);
      return(i);
   }
/*
 * Only divide M if all the SYRK flops can be done in LEFT's work.
 * Divides gemm's M asymmetrically to match the SYRK flops
 */
   if (Mblks>=Nblks)
   {
      if (Tblks)
         ATL_assert(Nblks == Tblks && nr == tr);
      nL = 0.5*percL*(Mblks+Mblks-Tblks);
      if (nL < 1) nL = 1;
      nR = Mblks - nL;
      flops = 2.0*nR*nb*Nblks*nb*K;
      if (P < 2 || !nL || flops < ATL_MINL3THRFLOPS)
      {
         psyrk->T = Tblks*nb + tr;
         psyrk->M = Mblks*nb + mr;
         psyrk->N = Nblks*nb + nr;
fprintf(stderr, "T=%d, M-%d, N=%d, FLOPS=%.2f\n", psyrk->T, psyrk->M, psyrk->N,
        (1.0*psyrk->T)*psyrk->T + (2.0*psyrk->M)*psyrk->N);
         psyrk->ia = ia;
         psyrk->ja = ja;
         psyrk->ib = ib;
         psyrk->jb = jb;
         psyrk->ic = ic;
         psyrk->jc = jc;
         return(1);
      }
fprintf(stderr, "%d of %s\n", __LINE__, __FILE__);
      rL = (!Tblks && nL <= nR) ? mr : 0;
      rR = mr - rL;
      i = ATL_tsyrkdecomp_tr(psyrk, pL, Tblks, tr, nL, rL, Nblks, nr, K,
                             ia, ja, ib, jb, ic, jc);
      i += ATL_tsyrkdecomp_tr(psyrk+pL, pR, 0, 0, nR, rR, Nblks, nr, K,
                              ia+(nL+Tblks)*nb+rL+tr, ja, ib, jb, 
                              ic+(nL+Tblks)*nb+rL+tr, jc);
      return(i);
   }
/*
 * As a last choice, cut both GEMM and SYRK (N & T) together
 * Must be done asymmetrically to balance differently sized gemms
 */
   if (Nblks && Tblks && nr == tr) /* divide N & T*/
   {
      nL = 2.0*percL*(Nblks+Mblks-
         sqrt((0.5*Nblks*Nblks)+((double)Nblks)*Mblks+((double) Mblks)*Mblks));
      if (nL < 1) nL = 1;
      nR = Nblks - nL;
      flops = nR * nb;
      flops = flops*flops + (2.0*Mblks)*(nb*flops);
      flops *= K;
      if (P < 2 || !nL || !nR || flops < ATL_MINL3THRFLOPS)
      {
         psyrk->T = Tblks*nb + tr;
         psyrk->M = Mblks*nb + mr;
         psyrk->N = Nblks*nb + nr;
fprintf(stderr, "bT=%d, M-%d, N=%d, FLOPS=%.2f\n", psyrk->T, psyrk->M, psyrk->N,
        (1.0*psyrk->T)*psyrk->T + (2.0*psyrk->M)*psyrk->N);
         psyrk->ia = ia;
         psyrk->ja = ja;
         psyrk->ib = ib;
         psyrk->jb = jb;
         psyrk->ic = ic;
         psyrk->jc = jc;
         return(1);
      }
fprintf(stderr, "%d of %s\n", __LINE__, __FILE__);
      i = ATL_tsyrkdecomp_tr(psyrk, pL, nL, 0, Mblks+nR, tr, nL, 0, K,
                             ia, ja, ib, jb, ic, jc);
      i += ATL_tsyrkdecomp_tr(psyrk+pL, pR, nR, tr, Mblks, mr, nR, tr, K,
                              ia+nL*nb, ja, ib, jb+nL*nb, ic+nL*nb, jc+nL*nb);
      return(i);
   }
   else  /* dividing a GEMM on N-dimension only */
   {
      ATL_assert(!Tblks && !tr && Nblks >= Mblks);
      nL = percL*Nblks + 0.5;
      nR = Nblks - nL;
      flops = ((2.0*nR*nb)*Mblks)*nb*K;
      if (P < 2 || !nL || !nR || flops < ATL_MINL3THRFLOPS)
      {
         psyrk->T = Tblks*nb + tr;
         psyrk->M = Mblks*nb + mr;
         psyrk->N = Nblks*nb + nr;
fprintf(stderr, "bT=%d, M-%d, N=%d, FLOPS=%.2f\n", psyrk->T, psyrk->M, psyrk->N,
        (1.0*psyrk->T)*psyrk->T + (2.0*psyrk->M)*psyrk->N);
         psyrk->ia = ia;
         psyrk->ja = ja;
         psyrk->ib = ib;
         psyrk->jb = jb;
         psyrk->ic = ic;
         psyrk->jc = jc;
         return(1);
      }
fprintf(stderr, "%d of %s\n", __LINE__, __FILE__);
      rL = (nL > nR) ? 0 : nr;
      rR = nr - rL;
      i = ATL_tsyrkdecomp_tr(psyrk, pL, 0, 0, Mblks, mr, nL, rL, K, 
                             ia, ja, ib, jb, ic, jc);
      i += ATL_tsyrkdecomp_tr(psyrk+pL, pR, 0, 0, Mblks, mr, nR, rR, K,
                              ia, ja, ib, jb+nL*nb+rL, ic, jc+nL*nb+rL);
      return(i);
   }
}

#include <string.h>
void SortSYRKByFlopCount(int P, ATL_TSYRK_t *syp)
{
   ATL_TSYRK_t stmp;
   int i, j, ib, jj;
   double mf0, mf;

   for (i=0; i < P-1; i++)
   {
      ib = ATL_launchorder[i];
      if (syp[ib].ia < 0)
         continue;
      mf0 = ((double)syp[ib].T)*syp[ib].T + (2.0*syp[ib].M)*syp[ib].N;
      for (j=i+1; j < P; j++)
      {
         jj = ATL_launchorder[j];
         if (syp[jj].ia < 0)
            continue;
         mf = ((double)syp[jj].T)*syp[jj].T + (2.0*syp[jj].M)*syp[jj].N;
         if (mf > mf0)
         {
            mf0 = mf;
            ib = jj;
         }
      }
      jj = ATL_launchorder[i];
      if (ib != jj)
      {
         memcpy(&stmp, syp+ib, sizeof(ATL_TSYRK_t));
         memcpy(syp+ib, syp+jj, sizeof(ATL_TSYRK_t));
         memcpy(syp+jj, &stmp, sizeof(ATL_TSYRK_t));
      }
   }
}
int ATL_StructIsInitSYRK(void *vp)
{
   return( ((ATL_TSYRK_t*)vp)->ia >= 0 );
}

void ATL_DoWorkSYRK(ATL_LAUNCHSTRUCT_t *lp, void *vp)
{
   ATL_thread_t *tp=vp;
   ATL_TSYRK_t *syp = ((ATL_TSYRK_t*)lp->opstruct)+tp->rank;
   const int lda = syp->lda, ldc = syp->ldc, eltsh = syp->eltsh;

   if (syp->T)
      syp->tvsyrk(syp->Uplo, syp->Trans, syp->T, syp->K, syp->alpha, 
                  syp->A+((syp->ia+syp->ja*lda)<<eltsh), lda, syp->beta,
                  syp->C+((syp->ic+syp->jc*ldc)<<eltsh), ldc);
   if (syp->M && syp->N)
      syp->tvgemm(AtlasNoTrans, AtlasTrans, syp->M, syp->N, syp->K, syp->alpha,
                  syp->A+((syp->ia+syp->T+syp->ja*lda)<<eltsh), lda,
                  syp->A+((syp->jb+syp->ib*lda)<<eltsh), lda, syp->beta,
                  syp->C+((syp->ic+syp->T+syp->jc*ldc)<<eltsh), ldc);
}
@endskip
@beginskip
#ifndef ATL_DoMMParallel
   #ifndef ATL_TGEMM_MINFLOPS
      #define ATL_TGEMM_MINFLOPS 512000.0
   #endif
   #define ATL_DoMMParallel(M_, N_, K_) \
      (((((double)(M_))*(N_))*(K_)) >= ATL_TGEMM_MINFLOPS)
#endif

int ATL_tsyrkdecomp_MM(ATL_TSYRK_N_t *psy, ATL_CINT nthr)
/*
 * This routine splits a gemm coming from SYRK until nthr is exhausted
 */
{
   int lo[ATL_NTHREADS];
   ATL_CINT M=psy->M, N=psy->N, K=psy->K, nb=psy->nb;
   ATL_INT m, nblks, nr, minblks, extrablks;
   const int eltsh=psy->eltsh, amul=(psy->TA == AtlasNoTrans) ? 1 : psy->lda;
   const int bmul = (psy->TB == AtlasNoTrans) ? 1 : psy->lda;
   int i, j, k, nt, np;
   const void *a;
   void *c;

   if (nthr == 1 || !psy->numthr(M, N, K))
      return(1);
   nt = nthr >> 1;
/*
 * If M is large enough, cut it for entire GEMM distribution in order to
 * optimize ATLAS's common JIK pattern
 */
   if (M >= nb*nthr*ATL_TMMMINMBLKS)
   {
/*
 *    Determine launchorder on this subset of processors
 */
      lo[0] = 0;
      for (i=0; (1<<i)^nthr; i++);
      lo[0] = 0;
      k=1;
      for (i--; i >= 0; i--)
      {
         for (j=0; j < k; j++)
            lo[k+j] = lo[j] + (1<<i);
         k += k;
      }
/*
 *    Find how many blocks we've got
 */
      nblks = M / nb;
      nr = M - nblks*nb;
      minblks = nblks / nthr;
      extrablks = nblks - minblks*nthr;
/*
 *    Get everyone a copy of entire data structure & assign subpieces
 */
      c = psy->C;
      a = psy->A0;
      for (i=0; i < nthr; i++)
      {
         j = lo[i];
         if (i) { McpSYN(psy, j, 0); }
         if (i < extrablks)
            m = (minblks+1)*nb;
         else if (i == extrablks)
            m = minblks*nb + nr;
         else
            m = minblks*nb;
         psy[j].M = m;
         psy[j].C = c;
         psy[j].A0 = a;
         m <<= eltsh;
         a = MindxT(a,m*amul);
         c = MindxT(c,m);
      }
      return(nthr);
   }
   else if (M >= N) /* split M */
   {
      McpSYN(psy, nt, 0);
      m = M>>1;
      psy[nt].M = m;
      psy->M = m = M - m;
      m <<= eltsh;
      psy[nt].A0 = MindxT(psy->A0,m*amul);
      psy[nt].C = MindxT(psy->C,m);
      np = ATL_tsyrkdecomp_MM(psy, nt);
      np += ATL_tsyrkdecomp_MM(psy+nt, nt);
      return(np);
   }
   else        /* split N */
   {
      McpSYN(psy, nt, 0);
      m = N>>1;
      psy[nt].N = m;
      psy->N = m = N-m;
      m <<= eltsh;
      psy[nt].C = MindxT(psy->C,m*psy->ldc);
      psy[nt].A1 = MindxT(psy->A1,m*amul);
      np = ATL_tsyrkdecomp_MM(psy, nt);
      np += ATL_tsyrkdecomp_MM(psy+nt, nt);
      return(np);
   }
}

int ATL_IsInitSYRK_N(void *vp)
{
   return( ((ATL_TSYRK_N_t*)vp)->K );
}

int ATL_tsyrkdecomp_N(ATL_TSYRK_N_t *psy, ATL_CINT nthr)
{
   void *vp;
   ATL_INT nL, nR, iL, iR;
   int nt, p;
   const int ISUPPER = (psy->Uplo == AtlasUpper), 
             ISNOTRANS = (psy->TA == AtlasNoTrans);

   if (nthr == 1)
      return(1);
   nt = nthr >> 1;  /* nthr is power of two, so both sides get nt threads */
/*
 * Copy present psy struct into new one, and then modify both as required
 */
   if (psy->C)   /* we are splitting two SYRKs */
   {
      if (!psy->numthr(psy->N, psy->N, psy->K))
         return(1);
      McpSYN(psy, nt, 0);   /* psy[nt] = psy[0] */
      psy[nt].T = psy->C;
      psy[nt].A0 = psy[nt].A1;
      psy[nt].C = psy->C = NULL;
      psy[nt].M = psy->N;
      psy->N = psy[nt].N = 0;
      p = ATL_tsyrkdecomp_N(psy, nt);
      p += ATL_tsyrkdecomp_N(psy+nt, nt);
      return(p);
   }
   else         /* we are splitting one SYRK into 2 SYRKS and one GEMM */
   {
      if (!psy->numthr(psy->M, (psy->M)>>1, psy->K))
         return(1);
      McpSYN(psy, nt, 0);   /* psy[nt] = psy[0] */
      nL = (psy->M)>>1;
      nR = psy->M - nL;
      iL = nL << psy->eltsh;
      iR = nR << psy->eltsh;
/*
 *    Give psy[0] both SYRKs to do, and continue splitting
 */
      psy->C = MindxT(psy->T, iL*(psy->ldc+1));
      psy->A1 = (ISNOTRANS) ? MindxT(psy->A0, iL) : MindxT(psy->A0,iL*psy->lda);
      psy->M = nL;
      psy->N = nR;
      p = ATL_tsyrkdecomp_N(psy, nt);
/*
 *    Give psy[nt] a GEMM to do, and call routine to split GEMMs
 */
      psy[nt].C = (ISUPPER) ? MindxT(psy[nt].T,iL*psy[nt].ldc) : 
                              MindxT(psy[nt].T,iL);
      psy[nt].T = NULL;
      if (ISUPPER)
      {
         psy[nt].M = nL;
         psy[nt].N = nR;
         psy[nt].A1 = (ISNOTRANS) ? 
            MindxT(psy[nt].A0,iL) : MindxT(psy[nt].A0,iL*psy[nt].lda);
      }
      else
      {
         psy[nt].M = nR;
         psy[nt].N = nL;
         if (ISNOTRANS)
         {
            psy[nt].A1 = psy[nt].A0;
            psy[nt].A0 = MindxT(psy[nt].A0,iL);
         }
         else  /* A is Transposed */
         {
            psy[nt].A1 = psy[nt].A0;
            psy[nt].A0 = MindxT(psy[nt].A0,iL*psy[nt].lda);
         }
      }
      p += ATL_tsyrkdecomp_MM(psy+nt, nt);
      return(p);
   }
}

void ATL_DoWorkSYRK_N(ATL_LAUNCHSTRUCT_t *lp, void *vp)
{
   ATL_thread_t *tp=vp;
   ATL_TSYRK_N_t *syp=((ATL_TSYRK_N_t*)lp->opstruct)+tp->rank;
   if (syp->T)  /* doing SYRKs */
   {
      syp->tvsyrk(syp->Uplo, syp->TA, syp->M, syp->K, syp->alpha, 
                  syp->A0, syp->lda, syp->beta, syp->T, syp->ldc);
      if (syp->C)
         syp->tvsyrk(syp->Uplo, syp->TA, syp->N, syp->K, syp->alpha, 
                     syp->A1, syp->lda, syp->beta, syp->C, syp->ldc);
   }
   else /* doing GEMM */
      syp->gemmK(syp->M, syp->N, syp->K, syp->alpha, syp->A0, syp->lda, 
                 syp->A1, syp->lda, syp->beta, syp->C, syp->ldc);
}
@endskip

int ATL_tsyrkdecomp_K
   (ATL_TSYRK_K_t *psyrk, 
    void (*syrkK)(const enum ATLAS_UPLO, const enum ATLAS_TRANS, ATL_CINT,
                  ATL_CINT, const void*, const void*, ATL_CINT, const void*,
                  void*, ATL_CINT),
    int np, const int eltsh, const int nb, const void *zero, const void *one,
    const enum ATLAS_UPLO Uplo, const enum ATLAS_TRANS Trans, 
    ATL_CINT N, ATL_CINT Kblks, const int kr, 
    const void *alpha, const void *A, ATL_CINT lda,
    const void *beta, void *C, ATL_CINT ldc)
{
@skip   ATL_CINT minblks = Kblks / ATL_NTHREADS, 
@skip            extrablks = Kblks - minblks*ATL_NTHREADS;
   ATL_INT minblks, extrablks, j, k, ldcw;
   int i;

/*
 * Note that this routine is essentially for large K, so we don't consider
 * any K smaller than NB for a processor
 */
   minblks = Kblks / np;
   if (minblks)
      extrablks = Kblks - minblks*np;
   else
   {
      np = Kblks;
      minblks = 1;
      extrablks = 0;
   }
/*
 * Find a good ldcw: multiple of 4 that is not a power of two
 */
   ldcw = ((N+3)>>2)<<2;   /* multiple of 4 */
   if (!(ldcw&(ldcw-1)))
      ldcw += 4;
@beginskip
   for (i=0; i < sizeof(ldcw)*8; i++)
   {
      if (!((1<<i)^ldcw))  /* if it is a power of two this is eventually 0 */
      {
         ldcw += 4;
         break;
      }
   }
@endskip
   if ((ldcw<<eltsh)*N > ATL_PTMAXMALLOC)
      return(0);
   for (i=0; i < np; i++)
   {
      if (i < extrablks)
         k = (minblks + 1)*nb;
      else if (i == extrablks)
         k = minblks*nb + kr;
      else
         k = minblks * nb;
      j = N;
@skip      lo = ATL_launchorder[i];   /* use log2-launch order */
      psyrk[i].alpha = alpha;
      psyrk[i].beta  = beta ;
      psyrk[i].one   = one  ;
      psyrk[i].zero  = zero ;
      psyrk[i].Uplo = Uplo;
      psyrk[i].Trans = Trans;
      psyrk[i].N = N;
      psyrk[i].K = k;
      psyrk[i].A = A;
      psyrk[i].C = C;
      psyrk[i].lda = lda;
      psyrk[i].ldc = ldc;
      psyrk[i].eltsh = eltsh;
      if (!i)
         psyrk[0].nCw = psyrk[0].ldcw = 0;
      else
      {
         psyrk[i].nCw = 1;
         psyrk[i].ldcw = ldcw;
      }
      psyrk[i].Cw = NULL;
      psyrk[i].Cinfp[0] = psyrk + i;
      psyrk[i].tvsyrk = syrkK;
      k = (Trans == AtlasNoTrans) ? lda * k : k;
      k <<= eltsh;
      A = MindxT(A,k);
   }
   for (; i < ATL_NTHREADS; i++)
      psyrk[i].N = 0;
   return(np);
}

void ATL_tsyrk_K(ATL_TSYRK_K_t *syp, int np, ATL_CINT N, ATL_CINT K, 
                 const void *A, void *C)
{
   const int nb = syp->nb;
   void ATL_DoWorkSYRK_K(ATL_LAUNCHSTRUCT_t *lp, void *vp);

   if (np < 1 || Mmin(N,K) < 8)
      np = 1;
   else
      np = ATL_tsyrkdecomp_K(syp, syp->tvsyrk, np, syp->eltsh, nb, syp->zero,
                             syp->one, syp->Uplo, syp->Trans, N, K/nb, K%nb, 
                             syp->alpha, A, syp->lda, syp->beta, C, syp->ldc);
   if (np < 2)
   {
      syp->tvsyrk(syp->Uplo, syp->Trans, N, K, syp->alpha, A, syp->lda, 
                  syp->beta, C, syp->ldc);
      return;
   }
   ATL_goparallel(np, ATL_DoWorkSYRK_K, syp, syp->DoComb);
@beginskip
   ATL_thread_start(syp->lp->rank2thr, 0, 1, ATL_tlaunch, syp->lp->rank2thr);
   ATL_thread_join(syp->lp->rank2thr);
@endskip
}

void ATL_tsyrk_K_rec(ATL_TSYRK_K_t *syp, int np, ATL_CINT Nblks, ATL_CINT nr, 
                     ATL_CINT K, const void *A0, void *C00)
/*
 * This routine recurs on N until we can allocate the full NxN workspace,
 * at which point it stops the recursion and distributes K for parallel
 * operation
 */
{
   const enum ATLAS_TRANS TA = syp->Trans;
   ATL_CINT lda = syp->lda, ldc = syp->ldc, eltsh = syp->eltsh;
   ATL_CINT nb = syp->nb, N = Nblks*nb+nr;
   ATL_INT sz, nblksL, nblksR, nrL, nrR, nL, nR;
   const void *A1;
   void *C10, *C01, *C11;
/*
 * Stop recursion & call threaded SYRK if we can allocate workspace for all of C
 */
   sz = (N * N) << eltsh;
/*
 * Quit recurring if we can allocate space for C workspace and we can
 * no longer usefully split Nblks, or we can usefully split K
 */
   if (sz <= ATL_PTMAXMALLOC && (nb*ATL_NTHREADS < K || Nblks < ATL_NTHREADS))
   {
      ATL_tsyrk_K(syp, np, Nblks*nb+nr, K, A0, C00);
      return;
   }
   nblksL = (Nblks+1)>>1;
   nblksR = Nblks - nblksL;
   if (nblksL >= nblksR)
   {
      nrL = nr;
      nrR = 0;
   }
   else
   {
      nrL = 0;
      nrR = nr;
   }

   nL = nblksL * nb + nrL;
   nR = nblksR * nb + nrR;
   if (syp->Uplo == AtlasUpper) 
   {
      sz = nL<<eltsh;
      C01 = MindxT(C00,sz*ldc);
      A1 = (TA == AtlasNoTrans) ? MindxT(A0,sz) : MindxT(A0,sz*lda);
      C11 = MindxT(C01,sz);
      ATL_tsyrk_K_rec(syp, np, nblksL, nrL, K, A0, C00);
      syp->gemmT(syp->Trans, syp->TB, nL, nR, K, syp->alpha, A0, lda, A1, lda, 
                 syp->beta, C01, ldc);
      ATL_tsyrk_K_rec(syp, np, nblksR, nrR, K, A1, C11);
   }
   else /* Lower triangular matrix */
   {
      sz = nL<<eltsh;
      C10 = MindxT(C00,sz);
      A1 = (TA == AtlasNoTrans) ? MindxT(A0,sz) : MindxT(A0,sz*lda);
      sz += (ldc*nL)<<eltsh;
      C11 = MindxT(C00,sz);
      ATL_tsyrk_K_rec(syp, np, nblksL, nrL, K, A0, C00);
      syp->gemmT(syp->Trans, syp->TB, nR, nL, K, syp->alpha, A1, lda, A0, lda, 
                 syp->beta, C10, ldc);
      ATL_tsyrk_K_rec(syp, np, nblksR, nrR, K, A1, C11);
   }
}

void ATL_DoWorkSYRK_K(ATL_LAUNCHSTRUCT_t *lp, void *vp)
{
   ATL_thread_t *tp=vp;
   ATL_TSYRK_K_t *syp = ((ATL_TSYRK_K_t *)lp->opstruct)+tp->rank;
/*
 * Allocate space if needed, and then do SYRK into it
 */
   if (syp->nCw)
   {
      syp->Cw = malloc((syp->ldcw << syp->eltsh)*syp->N+ATL_Cachelen);
      if (syp->Cw)
         syp->tvsyrk(syp->Uplo, syp->Trans, syp->N, syp->K, syp->alpha, syp->A,
                     syp->lda, syp->zero, ATL_AlignPtr(syp->Cw), syp->ldcw);
   }
   else /* do SYRK directly into original C: no poss of failure */
      syp->tvsyrk(syp->Uplo, syp->Trans, syp->N, syp->K, syp->alpha, 
                  syp->A, syp->lda, syp->beta, syp->C, syp->ldc);
}

int ATL_IsInitSYRK_K(void *vp)
{
   return( ((ATL_TSYRK_K_t*)vp)->N );
}

@ROUT tsttr
#include <stdio.h>
#include <stdlib.h>
#ifndef ATL_MU
   #define ATL_MU 4
#endif
#ifndef ATL_NTHREADS
   #define ATL_NTHREADS 8
#endif
#ifndef NB
   #define NB 56
#endif
#define ATL_INT int
#define ATL_CINT int
#define ATL_MINL3THRFLOPS (2.0*NB*NB*NB)
#define ATL_TGEMM_PERTHR_MF (2.0*NB*NB*NB)
@ROUT ATL_Xtsyrk tsttr
int ATL_tsyrkdecomp_tr1D(int P, ATL_CINT N, ATL_CINT K, 
                         ATL_CINT nb, ATL_CINT mu, double minmf, ATL_INT *Ms)
/*
 * Partitions triangular matrix from SYRK into roughly equal flop count
 * regions, with the first such region being strictly triangular, and the
 * rest trapazoidal row-panels.
 * Ms : must be of length P at least, on output contains the correct size
 *      matrix to give to each processor.
 * RETURNS: number of processors used
 */
{
   double Pflops, myflops, tflops, pinv;
   const int incM = (nb >= 60) ? ((24+mu-1)/mu)*mu : 
                    ((nb < 16) ? nb : ((16+mu-1)/mu)*mu);
   ATL_INT n, m, j;
   int k, p;

   for (k=0; k < P; k++)
      Ms[k] = 0;
   tflops = (((double)N)*N)*K;
   while (P && (Pflops = tflops/((double)P)) < minmf) P--;
   if (P < 2)
      return(0);

   tflops /= K;
/*
 * For each processor, find m that balances the flop count
 */
   for (n=p=0; p < P; p++)
   {
      Pflops = tflops / (P-p);
      if (tflops*K < minmf)
      {
         if (p < 2)
            return(0);
         Ms[p-1] += N - n;
         return(p);
      }
/*
 *    Finds the largest m that is a multiple of nb that generates <= Pflops
 */
      m = nb;  /* number of rows in row-panel */
      k = 1;   /* number of blocks in m */
      do
      {
         myflops = m;
         myflops *= myflops + n + n;
         if (myflops == Pflops)
            break;
         else if (myflops > Pflops)
         {
            m -= nb;
            k--;
            myflops = m;
            myflops *= myflops + n + n;
            break;
         }
         m += nb;
         k++;
      }
      while (1);
/*
 *    If we are below target flop count, see how to adjust
 */
      if (myflops < Pflops)
      {
         j = (k < 4) ? incM : mu;  /* for small M, don't tolerate cleanup */
         while ((((double)m)*((((double)m)+n)+n)) < Pflops) m += j;
         myflops = (((double)m)*((((double)m)+n)+n));
      }
      j = N - n;
      if (m >= j)
      {
         if (j < incM)
         {
            if (p < 1)
               return(0);
            Ms[p-1] += j;
            return(p);
         }
         Ms[p] = j;
         return(p+1);
      }
      else if (p == P-1)
         m = N - n;
      n += m;
      Ms[p] = m;
      tflops -= myflops;
   }
   return(p);
}
@ROUT tsttr
int main(int nargs, char **args)
{
   int N=1000, K=1000;
   int i, p, n;
   ATL_INT Ms[ATL_NTHREADS];
   double myflops, tflops;
   if (nargs > 1)
      N = atoi(args[1]);
   if (nargs > 2)
      K = atoi(args[2]);
   p = ATL_tsyrkdecomp_tr1D(ATL_NTHREADS, N, K, NB, ATL_MU, 
                            ATL_TGEMM_PERTHR_MF, Ms);
   printf("\n\nN=%d, K=%d:\n", N, K);
   if (p < 1)
      printf("   Unable to distribute!\n");
   else
   {
      tflops = (((double)N)*N)*K;
      printf("   P       M       N       FLOPS\n");
      printf("====  ======  ======  ==========\n\n");
      n = 0;
      for (i=0; i < p; i++)
      {
         myflops = (((double)Ms[i])*(Ms[i]+n+n))*K;
         printf("%4d %7d %7d %.0f (%.2f)\n", i, Ms[i], N, 
                myflops, myflops/tflops);
                
         n += Ms[i];
      }
      printf("Total M = %d\n\n", n);
   }
   return(0);
}
@ROUT ATL_Xtsyrk
int ATL_IsInitSYRK_M(void *vp)
{
   return( ((ATL_TSYRK_M_t*)vp)->K );
}

int ATL_tsyrkdecomp_M
(
   ATL_TSYRK_M_t *syp,          /* output: parallel decomposition structs */
   const enum ATLAS_UPLO Uplo,
   const enum ATLAS_TRANS TA,
   ATL_CINT N, ATL_CINT K,      /* original problem size */
   const void *alpha,
   const void *A,
   ATL_CINT lda,
   const void *beta,
   void *C,
   ATL_CINT ldc,
   ATL_CINT nb,                 /* MB of GEMM kernel */
   const int mu,                /* reg blking factor along M of MM kernel */
   const int eltsh,
   const enum ATLAS_TRANS TB,   /* Dual of TA (Conj for herk, trans for syrk) */
   double minmf,
   void (*gemmK)(ATL_CINT, ATL_CINT, ATL_CINT, const void*, const void *,
                 ATL_CINT, const void*, ATL_CINT, const void*, void*, ATL_CINT),
   void (*tvsyrk)(const enum ATLAS_UPLO, const enum ATLAS_TRANS, ATL_CINT,
                  ATL_CINT, const void*, const void*, ATL_CINT, const void*,
                  void*, ATL_CINT)
)
{
   ATL_INT Ms[ATL_NTHREADS];
   int k, j, p;
   ATL_CINT incA = lda << eltsh, incC = (ldc+1) << eltsh;
   ATL_INT n, m, JJ;
   const int ISNOTRANS = (TA == AtlasNoTrans);

   p = ATL_tsyrkdecomp_tr1D(ATL_NTHREADS, N, K, nb, mu, minmf, Ms);
   if (p < 2)
      return(0);
   if (Uplo == AtlasLower)
   { 
      n = 0;
      for (k=0; k < p; k++)
      {
@skip         j = ATL_launchorder[k];
         m = Ms[k];
         syp[k].gemmK = gemmK;
         syp[k].tvsyrk = tvsyrk;
         syp[k].alpha = alpha;
         syp[k].beta  = beta ;
         syp[k].K = K;
         syp[k].lda = lda;
         syp[k].ldc = ldc;
         syp[k].nb = nb;
         syp[k].eltsh = eltsh;
         syp[k].Uplo = Uplo;
         syp[k].TA = TA;
         syp[k].TB = TB;
         syp[k].M = m;
         syp[k].N = n;
         syp[k].T = MindxT(C,((size_t)n*incC));
         syp[k].C = (n > 0) ? MindxT(C,((size_t)n<<eltsh)) : NULL;
         syp[k].A0 = (ISNOTRANS) ? MindxT(A,((size_t)n<<eltsh)) 
                                 : MindxT((size_t)A,n*incA);
         syp[k].A = syp[k].A0;
         syp[k].B = A;
         n += m;
      }
   }
   else  /* Uplo == AtlasUpper */
   {
      n = 0;
      for (k=0; k < p; k++)
      {
@skip         j = ATL_launchorder[k];
         m = Ms[k];
         syp[k].gemmK = gemmK;
         syp[k].tvsyrk = tvsyrk;
         syp[k].alpha = alpha;
         syp[k].beta  = beta ;
         syp[k].K = K;
         syp[k].lda = lda;
         syp[k].ldc = ldc;
         syp[k].nb = nb;
         syp[k].eltsh = eltsh;
         syp[k].Uplo = Uplo;
         syp[k].TA = TA;
         syp[k].TB = TB;
         syp[k].M = m;
         syp[k].N = n;
         JJ = N - n - m;
         syp[k].T = MindxT(C,((size_t)JJ*incC));
         syp[k].C = (n > 0) ? MindxT(C,((size_t)JJ*incC+m*(ldc<<eltsh))) : NULL;
         if (ISNOTRANS)
         {
            syp[k].A = syp[k].A0 = MindxT(A,((size_t)JJ<<eltsh));
            syp[k].B = MindxT(syp[k].A0, ((size_t)m<<eltsh)); 
         }
         else
         {
            syp[k].A = syp[k].A0 = MindxT(A,((size_t)JJ*incA));
            syp[k].B = MindxT(syp[k].A0, ((size_t)m*incA)); 
         }
         n += m;
      }
   }
   for (k=p; k < ATL_NTHREADS; k++)
      syp[k].K = 0;
   return(p);
}

void ATL_DoWorkSYRK_M(ATL_LAUNCHSTRUCT_t *lp, void *vp)
{
   ATL_thread_t *tp=vp;
   ATL_TSYRK_M_t *syp = ((ATL_TSYRK_M_t*)lp->opstruct) + tp->rank;

   syp->tvsyrk(syp->Uplo, syp->TA, syp->M, syp->K, syp->alpha, 
               syp->A0, syp->lda, syp->beta, syp->T, syp->ldc);
   if (syp->C)
      syp->gemmK(syp->M, syp->N, syp->K, syp->alpha, syp->A, syp->lda,
                 syp->B, syp->lda, syp->beta, syp->C, syp->ldc);
}

@ROUT ATL_tsyrk
   @define TAC @T@
   @define TRANS @AtlasTrans@
   @define sadd @SADD@
   @define rt @syrk@
   @define styp @SCALAR@
@ROUT ATL_therk
   @define TAC @C@
   @define TRANS @AtlasConjTrans@
   @define sadd @&@
   @define rt @herk@
   @define styp @TYPE@
@ROUT ATL_tsyrk ATL_therk
#include "atlas_misc.h"
#include "atlas_threads.h"
#include "atlas_tlvl3.h"
#include Mstr(Mjoin(PRE,mm.h))

/*
 * Prototype functions in ATL_Xtsyrk
 */
int ATL_IsInitSYRK_M(void *vp);
void ATL_DoWorkSYRK_M(ATL_LAUNCHSTRUCT_t *lp, void *vp);
int ATL_tsyrkdecomp_M
   (ATL_TSYRK_M_t *syp, const enum ATLAS_UPLO Uplo, const enum ATLAS_TRANS TA,
    ATL_CINT N, ATL_CINT K, const void *alpha, const void *A, ATL_CINT lda,
    const void *beta, void *C, ATL_CINT ldc, ATL_CINT nb, const int mu,
    const int eltsh, const enum ATLAS_TRANS TB, double minmf,
    void (*gemmK)(ATL_CINT, ATL_CINT, ATL_CINT, const void*, const void *,
                  ATL_CINT,const void*, ATL_CINT, const void*, void*, ATL_CINT),
    void (*tvsyrk)(const enum ATLAS_UPLO, const enum ATLAS_TRANS, ATL_CINT,
                   ATL_CINT, const void*, const void*, ATL_CINT, const void*,
                   void*, ATL_CINT));
@beginskip
int ATL_tsyrkdecomp_N(ATL_TSYRK_N_t *psy, ATL_CINT nthr);
int ATL_IsInitSYRK_N(void *vp);
void ATL_DoWorkSYRK_N(ATL_LAUNCHSTRUCT_t *lp, void *vp);
@endskip
int ATL_tsyrkdecomp_K
   (ATL_TSYRK_K_t *psyrk, 
    void (*syrkK)(const enum ATLAS_UPLO, const enum ATLAS_TRANS, ATL_CINT,
                  ATL_CINT, const void*, const void*, ATL_CINT, const void*,
                  void*, ATL_CINT),
    const int eltsh, const int nb, const void *zero, const void *one,
    const enum ATLAS_UPLO Uplo, const enum ATLAS_TRANS Trans, 
    ATL_CINT N, ATL_CINT Kblks, const int kr, 
    const void *alpha, const void *A, ATL_CINT lda,
    const void *beta, void *C, ATL_CINT ldc);
void ATL_tsyrk_K_rec(ATL_TSYRK_K_t *syp, int np, ATL_CINT Nblks, ATL_CINT nr, 
                     ATL_CINT K, const void *A, void *C);
void ATL_DoWorkSYRK_K(ATL_LAUNCHSTRUCT_t *lp, void *vp);
int ATL_IsInitSYRK_K(void *vp);
@beginskip
int ATL_tsyrkdecomp_tr
   (ATL_TSYRK_t *psyrk, const int P, ATL_CINT Tblks, ATL_CINT tr, 
    ATL_CINT Mblks, ATL_CINT mr, ATL_CINT Nblks, ATL_CINT nr, ATL_CINT K,
    ATL_CINT ia, ATL_CINT ja, ATL_CINT ib, ATL_CINT jb, 
    ATL_CINT ic, ATL_CINT jc);
int ATL_StructIsInitSYRK(void *vp);
void ATL_DoWorkSYRK(ATL_LAUNCHSTRUCT_t *lp, void *vp);
void SortSYRKByFlopCount(int P, ATL_TSYRK_t *syp);
@endskip

void Mjoin(PATL,tv@(rt))
   (const enum ATLAS_UPLO Uplo, const enum ATLAS_TRANS Trans, 
    ATL_CINT N, ATL_CINT K, const void *alpha, const void *A, ATL_CINT lda, 
    const void *beta, void *C, ATL_CINT ldc)
{
@ROUT ATL_tsyrk
   Mjoin(PATL,@(rt))(Uplo, Trans, N, K, SVVAL((TYPE*)alpha), A, lda, 
                   SVVAL((TYPE*)beta), C, ldc);
@ROUT ATL_therk
   Mjoin(PATL,@(rt))(Uplo, Trans, N, K, *((TYPE*)alpha), A, lda, 
                   *((TYPE*)beta), C, ldc);
@ROUT ATL_tsyrk ATL_therk
}

@beginskip
static void ATL_init@up@(rt)_t
   (const int P, ATL_TSYRK_t *syp, const void *alpha, const void *beta, 
    const void *one, const void *zero, enum ATLAS_UPLO Uplo, 
    enum ATLAS_TRANS Trans, ATL_CINT K, const void *A, ATL_CINT lda, 
    void *C, ATL_CINT ldc)
{
   int i, nb;
   nb = Mjoin(PATL,GetNB)();

   for (i=0; i < P; i++)
   {
      syp[i].alpha = alpha;
      syp[i].beta  = beta ;
      syp[i].one   = one  ;
      syp[i].zero  = zero ;
      syp[i].Uplo = Uplo;
      syp[i].Trans = Trans;
      syp[i].A = A;
      syp[i].lda = lda;
      syp[i].ldc = ldc;
      syp[i].K = K;
      syp[i].C = C;
      syp[i].tvgemm = Mjoin(PATL,tvgemm);
      syp[i].tvsyrk = Mjoin(PATL,tv@(rt));
      syp[i].eltsh = Mjoin(PATL,shift);
      syp[i].ia = -1;  /* flag that this entry is not being used */
      syp[i].nb = nb;
   }
}
@endskip

static int CombineCw(ATL_TSYRK_K_t *me, ATL_TSYRK_K_t *him)
/*
 * This routine combines the data in him->Cw into my->Cw, if possible.
 * If his workspace is bigger than mine, I combine instead into his workspace,
 * and then set my pointer to his workspace.  The buffer that has been subsumed
 * is freed after the combine.
 * RETURNS: 0 if we are able to do the combine, non-zero if buffers are
 *          cannot be combined.
 */
{
   TYPE *w;
   size_t meB, meE, himB, himE, I, J;   /* begin,end of C range */
   #ifdef TREAL
      const TYPE ONE = 1.0;
   #else
      const TYPE ONE[2] = {1.0, 0.0};
   #endif

/*
 * If I'm the master (owner of original C), then I can always do combine
 * into the original C
 */
   if (me->nCw == 0)
   {
      if (him->Cw)  /* his malloc succeeded */
      {
         Mjoin(PATL,tradd)(him->Uplo, him->N, ATL_AlignPtr(him->Cw),
                           him->ldcw, ONE, (TYPE*)him->C, him->ldc);
         free(him->Cw);
      }
      else if (him->nCw)  /* must do GEMM since he couldn't malloc */
         him->tvsyrk(him->Uplo, him->Trans, him->N, him->K, him->alpha, 
                     him->A, him->lda, him->one, him->C, him->ldc);
      return(0);        /* successful combine */
   }
/*
 * *************************************************************************
 * Otherwise, I don't own C, so must combine work into my or his buffer when
 * possible, and return failure when not
 * *************************************************************************
 */
   meB  = (size_t) me->C; 
   meE  = meB + (((me->N*(me->ldc + 1)))<<(me->eltsh));
   himB = (size_t)him->C; 
   himE = himB + (((him->N*(him->ldc + 1)))<<(me->eltsh));
/*
 * If my workspace is a superset of his, use my workspace as the target buffer
 * if I was able to allocate it
 */
   if (meB <= himB && meE >= himE && me->Cw)
   {
/*
 *    Determine where our overlap is
 */
      I = (himB - meB)>>(him->eltsh);           /* gap in elts */
      J = I / him->ldc;                         /* col coord */
      I -= J*him->ldc;                          /* row coord */
      ATL_assert(I == J);
      w = ATL_AlignPtr(me->Cw);
      w += J*me->ldcw + I;
      if (him->Cw)  /* if he succeeded in malloc, combine his op with mine */
      {
         Mjoin(PATL,tradd)(him->Uplo, him->N, ATL_AlignPtr(him->Cw),
                           him->ldcw, ONE, w, him->ldcw);
         free(him->Cw);
      }
      else          /* must do SYRK since he didn't */
         him->tvsyrk(him->Uplo, him->Trans, him->N, him->K, him->alpha, 
                     him->A, him->lda, him->one, w, me->ldcw);
      return(0);        /* successful combine */
   }
/*
 * else if his workspace is a superset of mine, use his as target buffer if
 * he was able to allocate it
 */
   else if (himB <= meB && himE >= meE && him->Cw)
   {
/*
 *    Determine where our overlap is
 */
      I = (meB - himB)>>(him->eltsh);           /* gap in elements */
      J = I / him->ldc;                         /* column coord */
      I -= J*him->ldc;                          /* row coord */
      ATL_assert(I == J);
      w = ATL_AlignPtr(him->Cw);
      w += J*him->ldcw + I;
      if (me->Cw)  /* if I succeeded in malloc, combine my op with his */
      {
         Mjoin(PATL,tradd)(me->Uplo, me->N, ATL_AlignPtr(me->Cw),
                           me->ldcw, ONE, w, him->ldcw);
         free(me->Cw);
      }
      else          /* must do my SYRK into his workspace since I couldn't */
         him->tvsyrk(me->Uplo, me->Trans, me->N, me->K, me->alpha, 
                     me->A, me->lda, me->one, w, him->ldcw);
      me->C = him->C;
      me->Cw = him->Cw;
      me->ldcw = him->ldcw;
      me->N = him->N;
      me->K = him->K;
      return(0);        /* successful combine */
   }
   return(1);           /* unsuccessful combine */
}

void Mjoin(PATL,CombineStructs@up@(rt))
   (void *opstruct, const int myrank, const int hisrank)
/*
 * This routine written like GEMM, so that SYRK can have been split
 * with N, even though present code only splits K (so everyone is writing
 * to entire C).  I may want the extra functionality later, so programmed
 * it using GEMM as model.
 * NOTE: this version actually wouldn't work if we split both N & K for
 *       all cases; I later had to redesign the GEMM combine to account
 *       for the fact that you have to sum up the pieces of the original C
 *       you own, instead of always modifying C when you own only  a piece
 *       of it.  This problem only shows up on systems with non-power-of-2
 *       # of processors, where the launch recursive distribution doesn't
 *       match the recursive launch/combine procedure.  Will need to rewrite
 *       based on present GEMM combine if I ever go to true recursive
 *       distribution on both N & K.
 */
{
   #ifdef TREAL
      TYPE ONE = ATL_rone;
   #else
      TYPE ONE[2] = {ATL_rone, ATL_rzero};
   #endif
   ATL_TSYRK_K_t *me = ((ATL_TSYRK_K_t*)opstruct)+myrank; 
   ATL_TSYRK_K_t *him = ((ATL_TSYRK_K_t*)opstruct)+hisrank, *himcp, *mycp;
   int i, j;

/*
 * Need to combine only if joining thread has C in workspace
 */
   if (him->nCw)
   {
/*
 *    For all his workspaces, find out where to combine them into
 */
      for (i=0; i < him->nCw; i++)
      {
/*
 *       If I can't combine his data into my primary workspace, see if it
 *       can be combined with any of my other workspaces
 */
         if (CombineCw(me, him->Cinfp[i]))
         {
            for (j=1; j < me->nCw; j++)
               if (!CombineCw(me->Cinfp[j], him->Cinfp[i]))
                  break;
/*
 *          If I can't combine his data into any existing auxiliary space,
 *          add his node to my list of workspaces to be combined later
 */
            if (j == me->nCw)
            {
               me->Cinfp[j] = him->Cinfp[i];
               me->nCw = j + 1;
            }
         }
      }
   }
}

void Mjoin(PATL,t@(rt)_K_rec)
   (const enum ATLAS_UPLO Uplo, const enum ATLAS_TRANS Trans,
    ATL_CINT N, ATL_CINT K, const SCALAR alpha, const TYPE *A, ATL_CINT lda, 
    const SCALAR beta, TYPE *C, ATL_CINT ldc, ATL_CINT nb)
/*
 * This typed wrapper routine sets up type-specific data structures, and
 * calls the appropriate typeless recursive routine in order to recursively
 * cut N until workspace can be allocated, and then the K-dimension will be
 * threaded.  During the recursion, parallel performance is achieved by
 * calling the threaded GEMM.
 */
{
   ATL_CINT Nblks = N/nb, nr = N - nb*Nblks;
   ATL_TSYRK_K_t syp[ATL_NTHREADS];
@skip   ATL_LAUNCHSTRUCT_t ls;
@skip   ATL_thread_t tp[ATL_NTHREADS];
   #ifdef TCPLX
      TYPE ZERO[2] = {ATL_rzero, ATL_rzero}, ONE[2] = {ATL_rone, ATL_rzero};
   #else
      TYPE ZERO=ATL_rzero, ONE=ATL_rone;
   #endif
   int i;

@beginskip
   ls.opstructstride = (int) ( ((char*)(syp+1)) - (char*)syp );
   ls.OpStructIsInit = ATL_IsInitSYRK_K;
   ls.DoWork = ATL_DoWorkSYRK_K;
   ls.CombineOpStructs = Mjoin(PATL,CombineStructs@up@(rt));
   ls.rank2thr = tp;
   for (i=0; i < ATL_NTHREADS; i++)
   {
      tp[i].vp = &ls;
      tp[i].rank = i;
   }
   syp[0].lp = &ls;
@endskip
   syp[0].DoComb = Mjoin(PATL,CombineStructs@up@(rt));
   syp[0].Uplo = Uplo;
   syp[0].Trans = Trans;
@ROUT ATL_therk `   syp[0].TB = (Trans == AtlasNoTrans) ? AtlasConjTrans : AtlasNoTrans;`
@ROUT ATL_tsyrk `   syp[0].TB = (Trans == AtlasNoTrans) ? AtlasTrans : AtlasNoTrans;`
   syp[0].K = K;
   syp[0].alpha = SADD alpha;
   syp[0].beta = SADD beta;
   syp[0].zero = SADD ZERO;
   syp[0].one  = SADD ONE;
   syp[0].lda = lda;
   syp[0].ldc = ldc;
   syp[0].gemmT = Mjoin(PATL,tvgemm);
@skip   syp[0].gemmT = (Trans == AtlasNoTrans) ? 
@skip      Mjoin(PATL,tsvgemmN@(TAC)) : Mjoin(PATL,tsvgemm@(TAC)N);
   syp[0].tvsyrk = Mjoin(PATL,tv@(rt));
   syp[0].eltsh = Mjoin(PATL,shift);
   syp[0].nb = nb;
@skip   ls.opstruct = (char*) syp;
   ATL_tsyrk_K_rec(syp, Mjoin(PATL,threadMM)(Trans, syp[0].TB, N>>1, N>>1, K), 
                   Nblks, nr, K, A, C);
}

static int ATL_t@(rt)_M
   (const enum ATLAS_UPLO Uplo, const enum ATLAS_TRANS TA, ATL_CINT N,
    ATL_CINT K, const void *alpha, const TYPE *A, ATL_CINT lda,
    const void *beta, TYPE *C, ATL_CINT ldc)
{
@skip   ATL_thread_t tp[ATL_NTHREADS];
@skip   ATL_LAUNCHSTRUCT_t ls;
   ATL_TSYRK_M_t syp[ATL_NTHREADS];
   int i, p;
   p = ATL_tsyrkdecomp_M(syp, Uplo, TA, N, K, alpha, A, lda, beta, C, ldc,
                         ATL_AMM_LLCMU, ATL_AMM_LMU, Mjoin(PATL,shift), 
                         (TA == AtlasNoTrans) ? @(TRANS) : AtlasNoTrans,
                         ATL_TGEMM_PERTHR_MF, (TA == AtlasNoTrans) ? 
                         Mjoin(PATL,tsvgemmN@(TAC)):Mjoin(PATL,tsvgemm@(TAC)N),
                         Mjoin(PATL,tv@(rt)));
   if (p < 2)
      return(0);
   ATL_goparallel(p, ATL_DoWorkSYRK_M, syp, NULL);
@beginskip
   ls.opstruct = (char*) syp;
   ls.opstructstride = (int) ( ((char*)(syp+1)) - (char*)syp );
   ls.OpStructIsInit = ATL_IsInitSYRK_M;
   ls.DoWork = ATL_DoWorkSYRK_M;
   ls.CombineOpStructs = NULL;
   ls.rank2thr = tp;
   for (i=0; i < ATL_NTHREADS; i++)
   {
      tp[i].vp = &ls;
      tp[i].rank = i;
   }
   ATL_thread_start(tp, 0, 1, ATL_tlaunch, tp);
   ATL_thread_join(tp);
@endskip
   return(p);
}

@beginskip
#if ATL_NTHREADS == (1<<ATL_NTHRPOW2)
static int ATL_t@(rt)_N
   (const enum ATLAS_UPLO Uplo, const enum ATLAS_TRANS Trans, ATL_CINT N,
    ATL_CINT K, const void *alpha, const TYPE *A, ATL_CINT lda,
    const void *beta, TYPE *C, ATL_CINT ldc)
{
   ATL_thread_t tp[ATL_NTHREADS];
   ATL_LAUNCHSTRUCT_t ls;
   ATL_TSYRK_N_t psy[ATL_NTHREADS];
   int i, p;

   psy[0].gemmK = (Trans == AtlasNoTrans) ? 
      Mjoin(PATL,tsvgemmN@(TAC)) : Mjoin(PATL,tsvgemm@(TAC)N);
   psy[0].tvsyrk = Mjoin(PATL,tv@(rt));   
   psy[0].numthr = Mjoin(PATL,tNumGemmThreads);
   psy[0].A0 = A;
   psy[0].A1 = NULL;
   psy[0].T = C;
   psy[0].C = NULL;
   psy[0].alpha = alpha;
   psy[0].beta  = beta;
   psy[0].M = N;
   psy[0].N = 0;
   psy[0].K = K;
   psy[0].lda = lda;
   psy[0].ldc = ldc;
   psy[0].nb = Mjoin(PATL,GetNB)();
   psy[0].eltsh = Mjoin(PATL,shift);
   psy[0].Uplo = Uplo;
   psy[0].TA = Trans;
   psy[0].TB = (Trans == AtlasNoTrans) ? @(TRANS) : AtlasNoTrans;
   for (i=1; i < ATL_NTHREADS; i++)
      psy[i].K = 0;
   p = ATL_tsyrkdecomp_N(psy, ATL_NTHREADS);
   if (p < 2)
      return(0);
   ls.opstruct = (char*) psy;
   ls.opstructstride = (int) ( ((char*)(psy+1)) - (char*)psy );
   ls.OpStructIsInit = ATL_IsInitSYRK_N;
   ls.DoWork = ATL_DoWorkSYRK_N;
   ls.CombineOpStructs = NULL;
   ls.rank2thr = tp;
   for (i=0; i < ATL_NTHREADS; i++)
   {
      tp[i].vp = &ls;
      tp[i].rank = i;
   }
   ATL_thread_start(tp, 0, 1, ATL_tlaunch, tp);
   ATL_thread_join(tp);
   return(p);
}
#endif
@endskip

void Mjoin(PATL,t@(rt))
   (const enum ATLAS_UPLO Uplo, const enum ATLAS_TRANS Trans, ATL_CINT N,
@rout ATL_therk
    ATL_CINT K, const @(styp) alpha0, const TYPE *A, ATL_CINT lda,
    const @(styp) beta0, TYPE *C, ATL_CINT ldc)
@ROUT ATL_tsyrk
    ATL_CINT K, const @(styp) alpha, const TYPE *A, ATL_CINT lda,
    const @(styp) beta, TYPE *C, ATL_CINT ldc)
@ROUT ATL_tsyrk ATL_therk
{
@beginskip
   ATL_thread_t tp[ATL_NTHREADS];
   ATL_LAUNCHSTRUCT_t ls;
   ATL_TSYRK_t syrks[ATL_NTHREADS];
   ATL_TSYRK_K_t psyrks[ATL_NTHREADS];
@endskip
   #ifdef TREAL
      const TYPE ONE = ATL_rone, ZERO = ATL_rzero;
   #else
      const TYPE ONE[2]={ATL_rone, ATL_rzero}, ZERO[2]={ATL_rzero, ATL_rzero};
@ROUT ATL_therk `      const TYPE alpha[2]={alpha0, ATL_rzero}, beta[2]={beta0, ATL_rzero};`
   #endif
   size_t nblksN;
   int i, np, nb;
   void Mjoin(PATL,pt@(rt))
      (const enum ATLAS_UPLO Uplo, const enum ATLAS_TRANS Trans, ATL_CINT N,
       ATL_CINT K, const @(styp) alpha, const TYPE *A, ATL_CINT lda,
       const @(styp) beta, TYPE *C, ATL_CINT ldc);

   if (Mjoin(PATL,threadMM)(Trans, 
                            (Trans == AtlasNoTrans) ? AtlasTrans:AtlasNoTrans,
                            N, N>>1, K) < 2)
      goto DOSERIAL;
   if (N < 1)
      return;
@ROUT ATL_therk `   if (alpha0 == ATL_rzero || K < 1)`
@ROUT ATL_tsyrk `   if (SCALAR_IS_ZERO(alpha) || K < 1)`
   {
@ROUT ATL_therk
      if (beta0 != ATL_rone)
         Mjoin(PATL,hescal)(Uplo, N, N, beta0, C, ldc);
@ROUT ATL_tsyrk
      if (!SCALAR_IS_ONE(beta))
         Mjoin(PATL,trscal)(Uplo, N, N, beta, C, ldc);
@ROUT ATL_tsyrk ATL_therk
      return;
   }
@ROUT ATL_tsyrk
   #ifdef TREAL
   {
   @beginskip
      #ifdef ATL_AMM_98MB
         size_t nd = N / ATL_AMM_98MB;
      #else
         size_t nd = N / ATL_AMM_MAXMB;
      #endif
      nd = (nd*(nd-1))>>1;
      if (nd > ATL_NTHREADS)
   @endskip
      #ifdef ATL_AMM_98MB
         #define MY_MB ATL_AMM_98MB
      #else
         #define MY_MB ATL_AMM_MAXMB
      #endif
      #if ATL_NTHREADS > 8
          #define MY_NTHR 8
      #else
          #define MY_NTHR ATL_NTHREADS
      #endif
      if (N >= (MY_MB<<2) || 
          (N <= ATL_AMM_MAXNB && K > ATL_NTHREADS*(((size_t)ATL_AMM_MAXKB)<<8)))
      {
         void Mjoin(PATL,tsyrk_amm)
            (const enum ATLAS_UPLO, const enum ATLAS_TRANS, ATL_CINT N,
             ATL_CINT K, const SCALAR alpha, const TYPE *A, ATL_CINT lda, 
             const SCALAR beta, TYPE *C, ATL_CINT ldc);
         Mjoin(PATL,tsyrk_amm)(Uplo, Trans, N, K, alpha, A, lda, beta, C, ldc);
         return;
      }
   }
   #endif
@ROUT ATL_tsyrk ATL_therk

   nb = ATL_AMM_LLCMU;
   if (K > (N<<ATL_NTHRPOW2) && (((size_t)N)*N*sizeof(TYPE) <= ATL_PTMAXMALLOC))
   {
      Mjoin(PATL,t@(rt)_K_rec)(Uplo, Trans, N, K, alpha, A, lda, beta, 
                               C, ldc, nb);
@ROUT ATL_therk `      Mjoin(PATLU,zero)(N, C+1, lda+lda+2);  /* zero imag on diag */`
      return;
   }
@beginskip
#if 0 && ATL_NTHREADS == (1<<ATL_NTHRPOW2)
   np = ATL_t@(rt)_N(Uplo, Trans, N, K, SADD alpha, A, lda, 
                     SADD beta, C, ldc);
#endif
@endskip
   np = ATL_t@(rt)_M(Uplo, Trans, N, K, SADD alpha, A, lda, 
                     SADD beta, C, ldc);
   if (np < 2)
   {
DOSERIAL:
@ROUT ATL_tsyrk `      Mjoin(PATL,@(rt))(Uplo, Trans, N, K, alpha, A, lda, beta, C, ldc);`
@ROUT ATL_therk `      Mjoin(PATL,@(rt))(Uplo, Trans, N, K, alpha0, A, lda, beta0, C, ldc);`
      return;
   }
}
@beginskip
/*
 *  Can only use special N-only decomposition of #proc is a power of two
 */
#if ATL_NTHREADS == (1<<ATL_NTHRPOW2)

/*
 * Distribute N unless K dominates N, or N is degenerate 
 */
   nblksN = N/nb;
   nblksN = nblksN*nblksN - nblksN;
   if ( ((K+K)/N < N && nblksN >= ATL_NTHREADS+ATL_NTHREADS) || 
        (((size_t)N)*N*sizeof(TYPE) > ATL_PTMAXMALLOC) )
   if ( (N > (nb<<(ATLNTHRPOW2+1))) || N > K ||
        (((size_t)N)*N*sizeof(TYPE) > ATL_PTMAXMALLOC) ||
        ((K>>ATL_NTHRPOW2) < nb && N >= (nb<<ATL_NTHRPOW2-1)) ||
        (N >= K && N > (nb<<(ATL_NTHRPOW2+1))) )
    {
       if (ATL_t@(rt)_N(Uplo, Trans, N, K, SADD alpha, A, lda, 
                        SADD beta, C, ldc))
          return;
    }
#endif
@endskip
@beginskip
   np = ATL_tsyrkdecomp_K(psyrks, 
      Mjoin(PATL,tv@(rt)), Mjoin(PATL,shift), nb, SADD ZERO, SADD ONE, 
      Uplo, Trans, N, K/nb, K%nb, SADD alpha, A, lda, SADD beta, C, ldc);
   if (np < 2)
      goto DOSERIAL;
   ls.opstruct = (char*) psyrks;
   ls.opstructstride = (int) ( ((char*)(psyrks+1)) - (char*)psyrks );
   ls.OpStructIsInit = ATL_IsInitSYRK_K;
   ls.DoWork = ATL_DoWorkSYRK_K;
   ls.CombineOpStructs = Mjoin(PATL,CombineStructs@up@(rt));
   ls.rank2thr = tp;
   for (i=0; i < ATL_NTHREADS; i++)
   {
      tp[i].vp = &ls;
      tp[i].rank = i;
   }
   ATL_thread_start(tp, 0, 1, ATL_tlaunch, tp);
   ATL_thread_join(tp);
   return;
@ROUT ATL_therk `   Mjoin(PATLU,zero)(N, C+1, lda+lda+2);  /* zero imag on diag */`
   return;

   if (Uplo == AtlasLower && Trans == AtlasNoTrans)
   {
      ATL_init@up@(rt)_t(ATL_NTHREADS, syrks, @(sadd) alpha, @(sadd) beta, 
                        SADD ONE, SADD ZERO, Uplo, Trans, K, A, lda, C, ldc);
      nb = syrks[0].nb;
      np = ATL_tsyrkdecomp_tr(syrks, ATL_NTHREADS, N/nb, N%nb, 0, 0, 0, 0, K, 
                              0, 0, 0, 0, 0, 0);
      if (np < 2 || Mmin(N,K) < 8)
      {
         Mjoin(PATL,@(rt))(Uplo, Trans, N, K, alpha, A, lda, beta, C, ldc);
         return;
      }
      ls.opstruct = (char*) syrks;
@skip      ls.opstructstride = (int) ( ((char*)(syrks+1)) - (char*)syrks );
      ls.OpStructIsInit = ATL_StructIsInitSYRK;
      ls.DoWork = ATL_DoWorkSYRK;
@skip      ls.CombineOpStructs = NULL;
      ls.DoComb = NULL;
      ls.rank2thr = tp;
      for (i=0; i < ATL_NTHREADS; i++)
      {
         tp[i].vp = &ls;
         tp[i].rank = i;
      }
      ATL_thread_start(tp, 0, 1, ATL_tlaunch, tp);
      ATL_thread_join(tp);
   }
   else
   {
      SortSYRKByFlopCount(np, syrks);
      ls.opstruct = (char*) syrks;
@skip      ls.opstructstride = (int) ( ((char*)(syrks+1)) - (char*)syrks );
      ls.OpStructIsInit = ATL_StructIsInitSYRK;
      ls.DoWork = ATL_DoWorkSYRK;
      ls.DoComb = NULL;
@skip      ls.CombineOpStructs = NULL;
      ls.rank2thr = tp;
      for (i=0; i < ATL_NTHREADS; i++)
      {
         tp[i].vp = &ls;
         tp[i].rank = i;
      }
      ATL_thread_start(tp, 0, 1, ATL_tlaunch, tp);
      ATL_thread_join(tp);
   }
}
@endskip
@ROUT atlas_tlvl2.h
#ifndef ATLAS_TLVL2_H
   #define ATLAS_TLVL2_H

#include "atlas_threads.h"
#ifdef TYPE
   #include "atlas_lvl2.h"
#endif
 
#endif          /* end of ifndef ATLAS_TLVL2_H */
@ROUT atlas_tlapack.h
#ifndef ATLAS_TLAPACK_H
   #define ATLAS_TLAPACK_H

@skip #define ATL_LAUNCHORDER         /* we want static ATL_launchorder array */
#include "atlas_threads.h"
#include "atlas_lapack.h"

typedef struct
{
   ATL_INT M;       /* matrix rows to distribute across processors */
   ATL_INT N;       /* matrix columns */
   volatile ATL_INT *maxindx;  /* this array starts wt all values -1 */
   volatile ATL_INT *stage;    /* this ptr starts wt all values -1 */
   void *A;
   ATL_INT lda;
   int *ipiv;
   int rank, p, info;
   void *works;    /* ptr to array of ptrs */
} ATL_TGETF2_M_t;

typedef struct
{
   ATL_INT nblks, nr, K1, K2, inci, lda;
   void *A;
   const int *ipiv;
} ATL_TLASWP_N_t;
#endif                  /* end of ifndef ATLAS_TLAPACK_H */
@ROUT atlas_pca.h
#ifndef ATLAS_PCA_H
   #define ATLAS_PCA_H
/*
 * PowerPCs, POWERs and ARMs are weakly ordered, meaning that a given
 * processor's writes  may appear out-of-order to other processors, 
 * which breaks PCA's syncs since PCA depends on in-order writes.
 * To fix, we must issue a memory barrier call before giving the go-ahead.  
 * PowerPC: SYNC ensures that all prior stores complete before the next one.
 * POWER: DCS waits until all pending writes are written before preceeding
 * ARM: DMB (data mem barrier) - all prior mem accesses (in program order)
 *      complete before DMB returns
 *
 * Older x86's have a special mode where stores can become out-of-order, but
 * it was rarely enabled and does not seem to exist on modern hardware, so
 * we don't have to bother there.
 *
 * SPARCs do not change the order of stores.
 *
 * PowerPC and ARM syncs do not fix problem, so don't allow PCA on machines
 * with out-of-order write schemes.
 */
#if defined(ATL_ARCH_PPCG4) || defined(ATL_ARCH_PPCG5)
   #ifdef __GNUC__
      #define ATL_membarrier __asm__ __volatile__ ("sync")
/*      #define ATL_USEPCA 1 */
   #endif
#elif defined(ATL_ARCH_POWER3) || defined(ATL_ARCH_POWER4) || \
      defined(ATL_ARCH_POWER5) || defined(ATL_ARCH_POWER6) || \
      defined(ATL_ARCH_POWER7)
   #ifdef __GNUC__
      #define ATL_membarrier __asm__ __volatile__ ("dcs")
/*      #define ATL_USEPCA 1 */
   #endif
/*
 * Unfortunately, none of the memory fence instructions seems to work
 * adequately on ARM
 */
#elif defined(ATL_ARCH_ARMv7)
   #ifdef __GNUC__
      #define ATL_membarrier __asm__ __volatile__ ("dmb")
/*      #define ATL_USEPCA 1 */  
   #endif
#elif defined(ATL_ARCH_IA64Itan) || defined(ATL_ARCH_IA64Itan2)
   #ifdef __GNUC__
      #define ATL_membarrier __asm__ __volatile__ ("mf")
/*      #define ATL_USEPCA 1 */
   #endif
/*
 * All known x86 machines are strongly-ordered by default (can override
 * on PHI using special instructions).
 */
#elif defined(ATL_GAS_x8664) || defined (ATL_GAS_x8632)
   #define ATL_membarrier
   #define ATL_USEPCA 1
#else
   #define ATL_membarrier
#endif

#endif
@ROUT ATL_tgetf2
#include "atlas_tlapack.h"
#include "atlas_pca.h"
#include "atlas_level2.h"

void Mjoin(PATL,DoWorkGETF2_nowrk)(ATL_LAUNCHSTRUCT_t *lp, void *vp)
{
   ATL_thread_t *tp=vp;
   ATL_TGETF2_M_t *lup=((ATL_TGETF2_M_t*)lp->opstruct)+tp->rank;
   int *ipiv = lup->ipiv;
   ATL_CINT M=lup->M, N=lup->N, lda=lup->lda, MN = Mmin(M,N);
   const int p = lup->p, rank = lup->rank;
   ATL_CINT mp = M/p, mr = M - mp*p;
   ATL_INT m, locpiv, globpiv, k, j, i;
   #ifdef TCPLX
      ATL_CINT lda2 = lda+lda;
   #else 
      #define lda2 lda
      #define none ATL_rnone
   #endif
   TYPE *A, *Ac, *a, *v;
   TYPE pivval, apv, apv2;
   #ifdef TCPLX
      const TYPE none[2] = {ATL_rnone, ATL_rzero};
   #endif
   volatile ATL_INT *maxindx=lup->maxindx, *stage=lup->stage;
   void (*my_ger)(const int M, const int N, const SCALAR alpha, 
                  const TYPE *X, const int incX, 
                  const TYPE *Y, const int incY, TYPE *A, const int lda);

   #ifdef TCPLX
      my_ger = Mjoin(PATL,geru);
   #else
      my_ger = Mjoin(PATL,ger);
   #endif
   m = (rank) ? mp : mp+mr;
   Ac = A = lup->A;
   a = (rank) ? A + ((m*rank + mr)SHIFT) : A;
   for (j=0; j < MN; j++, Ac += lda2, a += lda2)
   {
      locpiv = cblas_iamax(m, a, 1);
/*
 *    Combine local pivot into global
 */
      if (!rank)
      {
         globpiv = j+locpiv;
         #ifdef TCPLX
            apv = Mabs(Ac[globpiv+globpiv]) + Mabs(Ac[globpiv+globpiv+1]);
         #else
            apv = Mabs(Ac[globpiv]);
         #endif
         for (i=1; i < p; i++)
         {
            while(stage[i] < j);
            k = maxindx[i];
            apv2 = Mabs(Ac[k SHIFT]);
            #ifdef TCPLX
               apv2 += Mabs(Ac[k+k+1]);
            #endif
            if (apv < apv2)
            {
               apv = apv2;
               globpiv = k;
            }
            maxindx[i] = -1;
         }
         ipiv[j] = globpiv;
         if (globpiv != j)
            cblas_swap(N, A+(j SHIFT), lda, A+(globpiv SHIFT), lda);
         ATL_membarrier;
         stage[0] = j;
         m--;                                           /* just finished */
         #ifdef TCPLX
            a += 2;                                     /* one row */
         #else
            a++;                                        /* one row */
         #endif
      }
      else /* all threads except 0 write their results, and await 0 */
      {
         maxindx[rank] = locpiv+rank*mp+mr;
         stage[rank] = j;
         while (stage[0] < j);
      }
      #ifdef TCPLX
         if (Ac[j+j] != ATL_rzero || Ac[j+j+1] != ATL_rzero)
         {
            TYPE inv[2];
            Mjoin(PATL,cplxinvert)(1, Ac+j+j, 1, inv, 1);
            cblas_scal(m, inv, a, 1);
         }
      #else
         pivval = Ac[j];
         if (pivval != ATL_rzero)
            cblas_scal(m, ATL_rone/pivval, a, 1);
      #endif
      else /* pivot is zero, we have a singular matrix! */
         lup->info = j;   /* all threads have same info */

      #ifdef TCPLX
         my_ger(m, N-j-1, none, a, 1, Ac+((j+lda)<<1), lda, a+lda2, lda);
         my_ger = Mjoin(PATL,geru_L2);
      #else
         my_ger(m, N-j-1, ATL_rnone, a, 1, Ac+j+lda, lda, a+lda, lda);
         my_ger = Mjoin(PATL,ger_L2);
      #endif
   }
}

void Mjoin(PATL,DoWorkGETF2)(ATL_LAUNCHSTRUCT_t *lp, void *vp0)
{
   ATL_thread_t *tp=vp0;
   ATL_TGETF2_M_t *lup=((ATL_TGETF2_M_t*)lp->opstruct)+tp->rank;
   int *ipiv = lup->ipiv;
   ATL_CINT M=lup->M, N=lup->N, lda=lup->lda, MN = Mmin(M,N);
   const int p = lup->p, rank = lup->rank;
   int pivrank;
   ATL_CINT mp = M/p, mr = M - mp*p;
   ATL_INT m, locpiv, globpiv, k, j, i, ldw, ldw0, ldw1;
   void *vp;
   TYPE *a, *W, *Wc, *w, **WRKS=lup->works, *v;
   TYPE pivval, apv, apv2, pv2;
   volatile ATL_INT *maxindx=lup->maxindx, *stage=lup->stage;
   #ifdef TCPLX
      const TYPE none[2] = {ATL_rnone, ATL_rzero};
   #endif

   m = (rank) ? mp : mp+mr;
   a = (rank) ? (((TYPE*)lup->A)+((mp*rank + mr)SHIFT)) : lup->A;
/*
 * Make ldw's a multiple of 16 bytes that is not a power of 2; 0's ldw 
 * is larger by mr than all other ldws (ldw1)
 */
#if defined(DREAL) || defined(SCPLX)
   ldw0 = ((mp+mr+1)>>1)<<1;
   ldw1 = ((mp+1)>>1)<<1;
   if (!(ldw0 & (ldw0-1)))
      ldw0 += 2;
   if (!(ldw1 & (ldw1-1)))
      ldw1 += 2;
#elif defined(SREAL)
   ldw0 = ((mp+mr+3)>>2)<<2;
   ldw1 = ((mp+3)>>2)<<2;
   if (!(ldw0 & (ldw0-1)))
      ldw0 += 4;
   if (!(ldw1 & (ldw1-1)))
      ldw1 += 4;
#else
   ldw0 = mp+mr;
   ldw1 = mp;
   if (!(ldw0 & (ldw0-1)))
      ldw0++;
   if (!(ldw1 & (ldw1-1)))
      ldw1++;
#endif
   ldw = (rank) ? ldw1 : ldw0;
   vp = malloc(ATL_MulBySize(ldw)*N+ATL_Cachelen);
/*
 * If anyone fails to allocate the space, free any allocated spaces and
 * call the no-copy version
 */
   j = (vp != NULL);
   if (!rank)
   {
      for (i=1; i < p; i++)
      {
         while (stage[i] != -2);
         j &= maxindx[i];
         maxindx[i] = -1;
      }
      *maxindx = j;
      stage[0] = -2;
   }
   else
   {
      maxindx[rank] = j;
      stage[rank] = -2;
      while (stage[0] != -2);
   }
   if (*maxindx == 0)
   {
      if (vp)
         free(vp);
      Mjoin(PATL,DoWorkGETF2_nowrk)(lp, vp0);
      return;
   }
   ATL_assert(vp);
   WRKS[rank] = w = W = ATL_AlignPtr(vp);
   Mjoin(PATL,gecopy)(m, N, a, lda, W, ldw);
   for (j=0; j < MN; j++, w += (ldw SHIFT))
   {
      locpiv = cblas_iamax(m, w, 1);
/*
 *    Combine local pivot into global
 */
      if (!rank)
      {
         globpiv = j+locpiv;
         pivrank = 0;
         apv = Mabs(w[locpiv SHIFT]);
         #ifdef TCPLX
            apv += Mabs(w[locpiv+locpiv+1]);
         #endif
         for (i=1; i < p; i++)
         {
            while(stage[i] < j);
            k = maxindx[i];
            apv2 = WRKS[i][(j*ldw1+k)SHIFT];
            apv2 = Mabs(apv2);
            #ifdef TCPLX
               apv2 += Mabs(WRKS[i][((j*ldw1+k)<<1)+1]);
            #endif
            if (apv < apv2)
            {
               apv = apv2;
               globpiv = k;
               pivrank = i;
            }
            maxindx[i] = -1;
         }
         if (pivrank)
         {
            ipiv[j] = mr+pivrank*mp+globpiv;
            cblas_swap(N, W+(j SHIFT), ldw, 
                       WRKS[pivrank]+(globpiv SHIFT), ldw1);
         }
         else
         {
            ipiv[j] = globpiv;
            if (globpiv != j)
               cblas_swap(N, W+(j SHIFT), ldw, W+(globpiv SHIFT), ldw);
         }
         ATL_membarrier;
         stage[0] = j;
         m--;                                           /* just finished */
         #ifdef TCPLX
            w += 2;                                     /* one row */
         #else
            w++;                                        /* one row */
         #endif
      }
      else /* all threads except 0 write their results, and await 0 */
      {
         maxindx[rank] = locpiv;
         stage[rank] = j;
         while (stage[0] < j);
      }
      #ifdef TCPLX
         v = &WRKS[0][(j*ldw0+j)SHIFT];
         if (*v != ATL_rzero || v[1] != ATL_rzero)
         {
            TYPE inv[2];
            Mjoin(PATL,cplxinvert)(1, v, 1, inv, 1);
            cblas_scal(m, inv, w, 1);
         }
      #else
         pivval = WRKS[0][j*ldw0+j];
         if (pivval != ATL_rzero)
            cblas_scal(m, ATL_rone/pivval, w, 1);
      #endif
      else /* pivot is zero, we have a singular matrix! */
         lup->info = j;   /* all threads have same info */

      #ifdef TCPLX
         Mjoin(PATL,geru_L2)(m, N-j-1, none, w, 1, 
                             WRKS[0]+((j*(ldw0+1)+ldw0)SHIFT), ldw0, 
                             w+ldw+ldw, ldw);
      #else
         Mjoin(PATL,ger_L2)(m, N-j-1, ATL_rnone, w, 1, WRKS[0]+j*(ldw0+1)+ldw0,
                            ldw0, w+ldw, ldw);
      #endif
   }
   stage[rank] = MN;  /* let core 0 know we are done */
/*
 * Copy answer back out of workspace and then free workspace
 */
   Mjoin(PATL,gecopy)(rank?mp:mp+mr, N, W, ldw, a, lda);
/*
 * Core 0 waits for all other cores to finish before he frees his work:
 * all non-zero cores access 0's workspace, but 0 does not access others' work
 * after iamax barrier
 */
   if (!rank)
   {
      for (i=1; i < p; i++)
         while(stage[i] != MN);
   }
   free(vp);
}

int Mjoin(PATL,StructIsInitGETF2)(void *vp)
{
   return(((ATL_TGETF2_M_t*)vp)->M);
}

@multidef trt Mjoin(PATL,DoWorkGETF2) Mjoin(PATL,DoWorkGETF2_nowrk)
@define pf @@
@whiledef pf _nocp
int Mjoin(PATL,tgetf2@(pf))(ATL_CINT M, ATL_CINT N, TYPE *A, ATL_CINT lda, int *ipiv)
{
@skip   ATL_thread_t tp[ATL_NTHREADS];
   ATL_TGETF2_M_t lu2s[ATL_NTHREADS];
@skip   ATL_LAUNCHSTRUCT_t ls;
   ATL_INT maxindx[ATL_NTHREADS], stage[ATL_NTHREADS];
   TYPE *works[ATL_NTHREADS];

   ATL_CINT MN = Mmin(M,N);
   ATL_INT p = ATL_NTHREADS, m, mr, i, j;

   if (M < 1 || N < 1)
      return(0);
   m = M / ATL_NTHREADS;
   mr = M - m*ATL_NTHREADS;
/*
 * This logic is necessary since tgetf2 assumes only one processor owns entire
 * logical block.  Can remove if we rewrite tgetf2 to allow the diagonal to
 * span multiple processors
 */
   if (m+mr < N)
   {
      p = M / N;
      if (p)
         m = M / p;
   }
   if (p < 2)   /* not enough rows, call serial algorithm */
      return(Mjoin(PATL,getf2)(M, N, A, lda, ipiv));
   for (i=0; i < p; i++)
   {
      stage[i] = maxindx[i] = -1;
@skip      j = ATL_launchorder[i];
      lu2s[i].M = M;
      lu2s[i].N = N;
      lu2s[i].A = A;
      lu2s[i].lda = lda;
      lu2s[i].ipiv = ipiv;  /* only thread 0 will write ipiv */
      lu2s[i].info = 0;
      lu2s[i].maxindx = maxindx;
      lu2s[i].stage = stage;
      lu2s[i].p = p;
      lu2s[i].rank = i;
      lu2s[i].works = works;
   }
   for (; i < ATL_NTHREADS; i++)
      lu2s[i].M = 0;
   ATL_goparallel(p, Mjoin(PATL,DoWorkGETF2), lu2s, NULL);
@beginskip
   ls.opstruct = (char*) lu2s;
   ls.opstructstride = (int) ( ((char*)(lu2s+1)) - (char*)(lu2s) );
   ls.CombineOpStructs = NULL;
   ls.OpStructIsInit = Mjoin(PATL,StructIsInitGETF2);
@skip   ls.DoWork = Mjoin(PATL,DoWorkGETF2);
   ls.DoWork = @(trt);
   ls.rank2thr = tp;
   for (i=0; i < ATL_NTHREADS; i++)
   {
      tp[i].vp = &ls;
      tp[i].rank = i;
   }
   ATL_thread_start(tp, 0, 1, ATL_tlaunch, tp);
   ATL_thread_join(tp);
@endskip
   return(lu2s[0].info);
}
   @undef trt
@endwhile
#ifndef TCPLX
   #undef lda2
#endif
@ROUT ATL_tlaswp
#include "atlas_tlapack.h"
#include "atlas_pca.h"

void Mjoin(PATL,DoWorkLASWP)(ATL_LAUNCHSTRUCT_t *lp, void *vp)
{
   ATL_thread_t *tp=vp;
   ATL_TLASWP_N_t *swpp=((ATL_TLASWP_N_t*)lp->opstruct)+tp->rank;
   const int *piv=swpp->ipiv, *ipiv;
   ATL_CINT nr=swpp->nr, K1=swpp->K1, K2=swpp->K2, inci=swpp->inci;
   ATL_INT nblks=swpp->nblks;
   TYPE *A = swpp->A;
   size_t lda = (swpp->lda)SHIFT;
   const int n = K2 - K1;
   size_t incA = lda << 5;
   int i, ip, i1, i2, KeepOn;
   register int h;
   TYPE *a0, *a1;
   #ifdef TCPLX
      register TYPE r0, r1;
   #else
      register TYPE r;
   #endif

   if (K2 < K1) return;
   if (inci < 0)
   {
      piv -= (K2-1) * inci;
      i1 = K2-1;
      i2 = K1;
   }
   else
   {
      piv += K1*inci;
      i1 = K1;
      i2 = K2-1;
   }

   if (nblks)
   {
      do
      {
         ipiv = piv;
         i = i1;
         do
         {
            ip = *ipiv; ipiv += inci;
            if (ip != i)
            {
               a0 = A + (i SHIFT);
               a1 = A + (ip SHIFT);
               for (h=32; h; h--)
               {
                  #ifdef TCPLX
                     r0 = *a0;
                     r1 = a0[1];
                     *a0 = *a1;
                     a0[1] = a1[1];
                     *a1 = r0;
                     a1[1] = r1;
                  #else
                     r = *a0;
                     *a0 = *a1;
                     *a1 = r;
                  #endif
                  a0 += lda;
                  a1 += lda;
               }
            }
            if (inci > 0) KeepOn = (++i <= i2);
            else KeepOn = (--i >= i2);
         }
         while(KeepOn);
         A += incA;
      }
      while(--nblks);
   }
   if (nr)
   {
      ipiv = piv;
      i = i1;
      do
      {
         ip = *ipiv; ipiv += inci;
         if (ip != i)
         {
            a0 = A + (i SHIFT);
            a1 = A + (ip SHIFT);
            for (h=nr; h; h--)
            {
               #ifdef TCPLX
                  r0 = *a0;
                  r1 = a0[1];
                  *a0 = *a1;
                  a0[1] = a1[1];
                  *a1 = r0;
                  a1[1] = r1;
               #else
                  r = *a0;
                  *a0 = *a1;
                  *a1 = r;
               #endif
               a0 += lda;
               a1 += lda;
            }
         }
         if (inci > 0) KeepOn = (++i <= i2);
         else KeepOn = (--i >= i2);
      }
      while(KeepOn);
   }
}

void Mjoin(PATL,tlaswp)(const int N, TYPE *A, const int lda, const int K1,
                        const int K2, const int *ipiv, const int inci)
{
   ATL_TLASWP_N_t swps[ATL_NTHREADS];
   ATL_INT i, p = ATL_NTHREADS, nblks, nlblks, neblks, nr, n;
   TYPE *ac;
   if (N < 128)
   {
      Mjoin(PATL,laswp)(N, A, lda, K1, K2, ipiv, inci);
      return;
   }
   nblks = N >> 5;
   nr = N - (nblks<<5);
   if (nblks < p)
   {
      p = nblks;
      nlblks = 1;
      neblks = 0;
   }
   else
   {
      nlblks = nblks / p;
      neblks = nblks - nlblks*p;
   }
   ac = A;
   for (i=0; i < p; i++)
   {
      size_t n;
      n = swps[i].nblks = (i < neblks) ? nlblks+1 : nlblks;
      swps[i].nr = (i == neblks) ? nr : 0;
      n = (n<<5)+swps[i].nr;
      swps[i].A = ac;
      swps[i].K1 = K1;
      swps[i].K2 = K2;
      swps[i].ipiv = ipiv;
      swps[i].inci = inci;
      swps[i].lda = lda;
      ac += (((size_t)lda)SHIFT)*n;
   }
   ATL_goparallel(p, Mjoin(PATL,DoWorkLASWP), swps, NULL);
}
@ROUT ATL_ger_L2
#include "atlas_misc.h"
#include "atlas_lvl2.h"
#include "atlas_lvl3.h"
// #include "atlas_r1.h"


void Mjoin(PATL,ger1k)
   (ATL_CINT M, ATL_CINT N, const SCALAR alpha,
    const TYPE *X, ATL_CINT incX, const TYPE *Y,
     ATL_CINT incY1, TYPE *A, ATL_CINT lda1);

#ifdef TREAL
   #define ATL_ger Mjoin(PATL,ger_L2)
   #define ATL_ger1 Mjoin(PATL,ger1k)
#else
   #ifdef Conj_
      #define ATL_ger Mjoin(PATL,gerc)
   #else
      #define ATL_ger Mjoin(PATL,geru)
      #define ATL_ger1 Mjoin(PATL,ger1u_a1_x1_yX)
   #endif
#endif
void ATL_ger(const int M, const int N, const SCALAR alpha,
             const TYPE *X, const int incX, const TYPE *Y, const int incY,
             TYPE *A, const int lda)
{
   int imb, mb, mb0, m=M, i;
   int incy=incY;
   #ifdef TREAL
      #define one ATL_rone
   #else
      static TYPE one[2] = {ATL_rone, ATL_rzero};
   #endif
   void *vx=NULL;
   size_t Aa, Ax;
   TYPE *x, *y = (TYPE*) Y;
   void (*getX)(const int N, const SCALAR alpha, const TYPE *X,
                const int incX, TYPE *Y, const int incY);
   #ifdef Conj_
      void (*ATL_ger1)(const int M, const int N, const SCALAR alpha,
                       const TYPE *X, const int incX, const TYPE *Y,
                       const int incY, TYPE *A, const int lda);
      ATL_ger1 = Mjoin(PATL,ger1c_a1_x1_yX);
   #endif

   if ( !M || !N || SCALAR_IS_ZERO(alpha) ) return;
   if (lda&1)
   {
//    fprintf(stderr, "WARNING: not using L2-tuned GER kernel!\n");
      Mjoin(PATL,ger)(M, N, alpha, X, incX, Y, incY, A, lda);
      return;
   }
  
//   fprintf(stderr, "in %s! M=%i, N=%i, lda=%i, alpha=%f, A[64]=%i.\n", 
//   __FILE__, M, N, lda, alpha, (int) (((long unsigned int) A) & 63));
//   fflush(stderr);

   imb = mb = M;

   Aa = (size_t) A;
   Ax = (size_t) X;
   if (Aa%16 != Ax%16 || incX != 1 || !SCALAR_IS_ONE(alpha))
   {
/*
 *    Apply alpha to Y if X has stride 1 & Y is MUCH smaller
 *    The LAPACK barfs if you  switch which vector alpha is applied to,
 *    since it tests tiny matrices, so make it apply to X when they are
 *    close to even
 */
      if (incX == 1 && N < (M>>4) && Aa%16 == Ax%16)
      {
         vx = malloc(ATL_Cachelen + ATL_MulBySize(N));
         ATL_assert(vx);
         y = ATL_AlignPtr(vx);
         #ifdef Conj_
            Mjoin(PATL,moveConj)(N, alpha, Y, incY, y, 1);
            ATL_ger1 = Mjoin(PATL,ger1u_a1_x1_yX);
         #else
            Mjoin(PATL,cpsc)(N, alpha, Y, incY, y, 1);
         #endif
         incy = 1;
         getX = NULL;
      }
      else
      {
         i = Mmax(imb,mb);
         i = Mmin(i,M);
         vx = malloc(2*ATL_Cachelen + ATL_MulBySize(i));
         ATL_assert(vx);
         Ax = (size_t) vx;
         for (i=Aa%16; Ax%16 != i; Ax++);
         x = (TYPE*) Ax;
         getX = Mjoin(PATL,cpsc);
      }
   }
   else getX = NULL;

   if (imb) mb0 = Mmin(imb,m);
   else mb0 = Mmin(mb,m);
   do
   {
      if (getX) getX(mb0, alpha, X, incX, x, 1);
      else x = (TYPE*) X;
      ATL_ger1(mb0, N, one, x, 1, y, incy, A, lda);
      A += mb0 SHIFT;
      X += mb0*incX SHIFT;
      m -= mb0;
      mb0 = Mmin(m,mb);
   }
   while(m);
   if (vx) free(vx);
}
@ROUT ATL_gerk
#include <xmmintrin.h>
#include "atlas_misc.h"
#include <stdio.h>

void Mjoin(PATL,ger1k)
   (ATL_CINT M, ATL_CINT N, const SCALAR alpha,
    const TYPE *X, ATL_CINT incX, const TYPE *Y,
    ATL_CINT incY1, TYPE *A, ATL_CINT lda1)
{/* BEGIN GER: nMU=1, MU=8, NU=4 */
   ATL_INT i, j;
// ATL_CINT MAp = ( (((((size_t)A)+15)>>4)<<4) - (((((size_t)A))>>4)<<4) )/sizeof(TYPE);
   ATL_CINT MAp = ( ((size_t)A)&(15) ) / sizeof(TYPE);
   ATL_CINT MA=M-MAp;
   ATL_CINT M8=((MA/8)*8)+MAp, M2=((MA>>1)<<1)+MAp, N4=((N/4)*4), lda2=lda1+lda1, incY2=incY1+incY1, lda3=lda2+lda1, incY3=incY2+incY1, lda4=lda3+lda1, incY4=incY3+incY1;
   __m128d x0, x1, x2, x3, x4, x5, x6, x7, y0, y1, y2, y3, a0_0, m0_0, a1_0, m1_0, a2_0, m2_0, a3_0, m3_0, a4_0, m4_0, a5_0, m5_0, a6_0, m6_0, a7_0, m7_0, a0_1, m0_1, a1_1, m1_1, a2_1, m2_1, a3_1, m3_1, a4_1, m4_1, a5_1, m5_1, a6_1, m6_1, a7_1, m7_1, a0_2, m0_2, a1_2, m1_2, a2_2, m2_2, a3_2, m3_2, a4_2, m4_2, a5_2, m5_2, a6_2, m6_2, a7_2, m7_2, a0_3, m0_3, a1_3, m1_3, a2_3, m2_3, a3_3, m3_3, a4_3, m4_3, a5_3, m5_3, a6_3, m6_3, a7_3, m7_3;

   for (j=0; j < N4; j += 4, A += lda4, Y += incY4)
   {/* BEGIN N-LOOP UR=4 */
      y0 = _mm_load1_pd(Y);
      y1 = _mm_load1_pd(Y+incY1);
      y2 = _mm_load1_pd(Y+incY2);
      y3 = _mm_load1_pd(Y+incY3);
      if (MAp)
      {/* peel to force X/A alignment */
         i=0;                                /* bug fix. */
         x0 = _mm_load_sd(X+i+0);
         a0_0 = _mm_load_sd(A+i+0);
         m0_0 = _mm_mul_sd(x0, y0);
         a0_0 = _mm_add_sd(a0_0, m0_0);
         _mm_store_sd(A+i+0, a0_0);
         a0_1 = _mm_load_sd(A+i+0+lda1);
         m0_1 = _mm_mul_sd(x0, y1);
         a0_1 = _mm_add_sd(a0_1, m0_1);
         _mm_store_sd(A+i+0+lda1, a0_1);
         a0_2 = _mm_load_sd(A+i+0+lda2);
         m0_2 = _mm_mul_sd(x0, y2);
         a0_2 = _mm_add_sd(a0_2, m0_2);
         _mm_store_sd(A+i+0+lda2, a0_2);
         a0_3 = _mm_load_sd(A+i+0+lda3);
         m0_3 = _mm_mul_sd(x0, y3);
         a0_3 = _mm_add_sd(a0_3, m0_3);
         _mm_store_sd(A+i+0+lda3, a0_3);
      } /* end force-align peel */

      for (i=MAp; i < M8; i += 8)
      {/* ----- BEGIN M-LOOP BODY ----- */
         /* --- BEGIN MUxNU UNROLL 0 --- */
         x0 = _mm_load_pd(X+i+0);
         a0_0 = _mm_load_pd(A+i+0);
         m0_0 = _mm_mul_pd(x0, y0);
         a0_0 = _mm_add_pd(a0_0, m0_0);
         _mm_store_pd(A+i+0, a0_0);
         x2 = _mm_load_pd(X+i+2);
         a2_0 = _mm_load_pd(A+i+2);
         m2_0 = _mm_mul_pd(x2, y0);
         a2_0 = _mm_add_pd(a2_0, m2_0);
         _mm_store_pd(A+i+2, a2_0);
         x4 = _mm_load_pd(X+i+4);
         a4_0 = _mm_load_pd(A+i+4);
         m4_0 = _mm_mul_pd(x4, y0);
         a4_0 = _mm_add_pd(a4_0, m4_0);
         _mm_store_pd(A+i+4, a4_0);
         x6 = _mm_load_pd(X+i+6);
         a6_0 = _mm_load_pd(A+i+6);
         m6_0 = _mm_mul_pd(x6, y0);
         a6_0 = _mm_add_pd(a6_0, m6_0);
         _mm_store_pd(A+i+6, a6_0);
         a0_1 = _mm_load_pd(A+i+0+lda1);
         m0_1 = _mm_mul_pd(x0, y1);
         a0_1 = _mm_add_pd(a0_1, m0_1);
         _mm_store_pd(A+i+0+lda1, a0_1);
         a2_1 = _mm_load_pd(A+i+2+lda1);
         m2_1 = _mm_mul_pd(x2, y1);
         a2_1 = _mm_add_pd(a2_1, m2_1);
         _mm_store_pd(A+i+2+lda1, a2_1);
         a4_1 = _mm_load_pd(A+i+4+lda1);
         m4_1 = _mm_mul_pd(x4, y1);
         a4_1 = _mm_add_pd(a4_1, m4_1);
         _mm_store_pd(A+i+4+lda1, a4_1);
         a6_1 = _mm_load_pd(A+i+6+lda1);
         m6_1 = _mm_mul_pd(x6, y1);
         a6_1 = _mm_add_pd(a6_1, m6_1);
         _mm_store_pd(A+i+6+lda1, a6_1);
         a0_2 = _mm_load_pd(A+i+0+lda2);
         m0_2 = _mm_mul_pd(x0, y2);
         a0_2 = _mm_add_pd(a0_2, m0_2);
         _mm_store_pd(A+i+0+lda2, a0_2);
         a2_2 = _mm_load_pd(A+i+2+lda2);
         m2_2 = _mm_mul_pd(x2, y2);
         a2_2 = _mm_add_pd(a2_2, m2_2);
         _mm_store_pd(A+i+2+lda2, a2_2);
         a4_2 = _mm_load_pd(A+i+4+lda2);
         m4_2 = _mm_mul_pd(x4, y2);
         a4_2 = _mm_add_pd(a4_2, m4_2);
         _mm_store_pd(A+i+4+lda2, a4_2);
         a6_2 = _mm_load_pd(A+i+6+lda2);
         m6_2 = _mm_mul_pd(x6, y2);
         a6_2 = _mm_add_pd(a6_2, m6_2);
         _mm_store_pd(A+i+6+lda2, a6_2);
         a0_3 = _mm_load_pd(A+i+0+lda3);
         m0_3 = _mm_mul_pd(x0, y3);
         a0_3 = _mm_add_pd(a0_3, m0_3);
         _mm_store_pd(A+i+0+lda3, a0_3);
         a2_3 = _mm_load_pd(A+i+2+lda3);
         m2_3 = _mm_mul_pd(x2, y3);
         a2_3 = _mm_add_pd(a2_3, m2_3);
         _mm_store_pd(A+i+2+lda3, a2_3);
         a4_3 = _mm_load_pd(A+i+4+lda3);
         m4_3 = _mm_mul_pd(x4, y3);
         a4_3 = _mm_add_pd(a4_3, m4_3);
         _mm_store_pd(A+i+4+lda3, a4_3);
         a6_3 = _mm_load_pd(A+i+6+lda3);
         m6_3 = _mm_mul_pd(x6, y3);
         a6_3 = _mm_add_pd(a6_3, m6_3);
         _mm_store_pd(A+i+6+lda3, a6_3);
         /* --- END MUxNU UNROLL 0 --- */
      }/* ----- END M-LOOP BODY ----- */
      if (M != M8)
      {/* ----- BEGIN VECTOR UNROLL M CLEANUP ----- */

         for (i=M8; i < M2; i += 2)
         {/* ----- BEGIN M-LOOP BODY ----- */
            /* --- BEGIN MUxNU UNROLL 0 --- */
            x0 = _mm_load_pd(X+i+0);
            a0_0 = _mm_load_pd(A+i+0);
            m0_0 = _mm_mul_pd(x0, y0);
            a0_0 = _mm_add_pd(a0_0, m0_0);
            _mm_store_pd(A+i+0, a0_0);
            a0_1 = _mm_load_pd(A+i+0+lda1);
            m0_1 = _mm_mul_pd(x0, y1);
            a0_1 = _mm_add_pd(a0_1, m0_1);
            _mm_store_pd(A+i+0+lda1, a0_1);
            a0_2 = _mm_load_pd(A+i+0+lda2);
            m0_2 = _mm_mul_pd(x0, y2);
            a0_2 = _mm_add_pd(a0_2, m0_2);
            _mm_store_pd(A+i+0+lda2, a0_2);
            a0_3 = _mm_load_pd(A+i+0+lda3);
            m0_3 = _mm_mul_pd(x0, y3);
            a0_3 = _mm_add_pd(a0_3, m0_3);
            _mm_store_pd(A+i+0+lda3, a0_3);
            /* --- END MUxNU UNROLL 0 --- */
         }/* ----- END M-LOOP BODY ----- */
         if (M != M2)
         {/* ----- BEGIN SCALAR M CLEANUP ----- */
            x0 = _mm_load_sd(X+i+0);
            a0_0 = _mm_load_sd(A+i+0);
            m0_0 = _mm_mul_sd(x0, y0);
            a0_0 = _mm_add_sd(a0_0, m0_0);
            _mm_store_sd(A+i+0, a0_0);
            a0_1 = _mm_load_sd(A+i+0+lda1);
            m0_1 = _mm_mul_sd(x0, y1);
            a0_1 = _mm_add_sd(a0_1, m0_1);
            _mm_store_sd(A+i+0+lda1, a0_1);
            a0_2 = _mm_load_sd(A+i+0+lda2);
            m0_2 = _mm_mul_sd(x0, y2);
            a0_2 = _mm_add_sd(a0_2, m0_2);
            _mm_store_sd(A+i+0+lda2, a0_2);
            a0_3 = _mm_load_sd(A+i+0+lda3);
            m0_3 = _mm_mul_sd(x0, y3);
            a0_3 = _mm_add_sd(a0_3, m0_3);
            _mm_store_sd(A+i+0+lda3, a0_3);
         }/* ----- END SCALAR M CLEANUP ----- */
      }/* ----- END VECTOR UNROLL M CLEANUP ----- */
   }/* END N-LOOP UR=4 */

   for (j=N4; j < N; j += 1, A += lda1, Y += incY1)
   {/* BEGIN N-LOOP UR=1 */
      y0 = _mm_load1_pd(Y);
      if (MAp)
      {/* peel to force X/A alignment */
         i=0;                                /* bug fix. */
         x0 = _mm_load_sd(X+i+0);
         a0_0 = _mm_load_sd(A+i+0);
         m0_0 = _mm_mul_sd(x0, y0);
         a0_0 = _mm_add_sd(a0_0, m0_0);
         _mm_store_sd(A+i+0, a0_0);
      } /* end force-align peel */

      for (i=MAp; i < M8; i += 8)
      {/* ----- BEGIN M-LOOP BODY ----- */
         /* --- BEGIN MUxNU UNROLL 0 --- */
         x0 = _mm_load_pd(X+i+0);
         a0_0 = _mm_load_pd(A+i+0);
         m0_0 = _mm_mul_pd(x0, y0);
         a0_0 = _mm_add_pd(a0_0, m0_0);
         _mm_store_pd(A+i+0, a0_0);
         x2 = _mm_load_pd(X+i+2);
         a2_0 = _mm_load_pd(A+i+2);
         m2_0 = _mm_mul_pd(x2, y0);
         a2_0 = _mm_add_pd(a2_0, m2_0);
         _mm_store_pd(A+i+2, a2_0);
         x4 = _mm_load_pd(X+i+4);
         a4_0 = _mm_load_pd(A+i+4);
         m4_0 = _mm_mul_pd(x4, y0);
         a4_0 = _mm_add_pd(a4_0, m4_0);
         _mm_store_pd(A+i+4, a4_0);
         x6 = _mm_load_pd(X+i+6);
         a6_0 = _mm_load_pd(A+i+6);
         m6_0 = _mm_mul_pd(x6, y0);
         a6_0 = _mm_add_pd(a6_0, m6_0);
         _mm_store_pd(A+i+6, a6_0);
         /* --- END MUxNU UNROLL 0 --- */
      }/* ----- END M-LOOP BODY ----- */
      if (M != M8)
      {/* ----- BEGIN VECTOR UNROLL M CLEANUP ----- */

         for (i=M8; i < M2; i += 2)
         {/* ----- BEGIN M-LOOP BODY ----- */
            /* --- BEGIN MUxNU UNROLL 0 --- */
            x0 = _mm_load_pd(X+i+0);
            a0_0 = _mm_load_pd(A+i+0);
            m0_0 = _mm_mul_pd(x0, y0);
            a0_0 = _mm_add_pd(a0_0, m0_0);
            _mm_store_pd(A+i+0, a0_0);
            /* --- END MUxNU UNROLL 0 --- */
         }/* ----- END M-LOOP BODY ----- */
         if (M != M2)
         {/* ----- BEGIN SCALAR M CLEANUP ----- */
            x0 = _mm_load_sd(X+i+0);
            a0_0 = _mm_load_sd(A+i+0);
            m0_0 = _mm_mul_sd(x0, y0);
            a0_0 = _mm_add_sd(a0_0, m0_0);
            _mm_store_sd(A+i+0, a0_0);
         }/* ----- END SCALAR M CLEANUP ----- */
      }/* ----- END VECTOR UNROLL M CLEANUP ----- */
   }/* END N-LOOP UR=1 */
}/* END GER: nMU=1, MU=8, NU=4 */
#ifdef MA
   #undef MA
#endif
#ifdef MAp
   #undef MAp
#endif
@ROUT ATL_threadMM
#include "atlas_misc.h"
#include "atlas_threads.h"
#include "atlas_tlvl3.h"
@skip #include Mstr(Mjoin(atlas_,Mjoin(Mjoin(Mjoin(PRE,tXover_),ATL_NCPU),p.h)))

#ifdef DEBUG
#define T2c(ta_) ((ta_) == AtlasNoTrans) ? 'N' : 'T'
#endif
#ifndef ATL_TXOVER_H
int Mjoin(PATL,threadMM)(const enum ATLAS_TRANS TA, const enum ATLAS_TRANS TB,
                         size_t M, size_t N, size_t K)
/*
 * This dummy routine used when crossover is not tuned 
 */
{
#if 0
   size_t minD, maxD;

   minD = Mmin(M,N);
   minD = Mmin(minD,K);
   maxD = Mmax(M,N);
   maxD = Mmax(maxD,K);
   if (M >= (NB<<(ATL_NTHRPOW2+2)))
      return(2);
   else if (minD >= 8 && maxD >= 2*NB)
      return(1);
   return(0);
#else
   int Mjoin(PATL,GemmWillThread)(ATL_CINT M, ATL_CINT N, ATL_CINT K);
   return(Mjoin(PATL,GemmWillThread)(M, N, K));
#endif
}
#else
int Mjoin(PATL,threadMM)(const enum ATLAS_TRANS TA, const enum ATLAS_TRANS TB,
                         size_t M, size_t N, size_t K)
/*
 * RETURNS: number of threads matmul should use to paralellize the problem
 */
{
   size_t i, j, smp2, bip2, xo, xom, D;
   const int *xop;
   int k;
   if (M < 256 && N < 256 && K < 256)   /* small matrix */
   {
/*
 *    For really small problems, table lookups too expensive, so do a quick
 *    return
 */
      j = Mmax(M,N);
      i = Mmin(M,N);
      i = Mmin(i,K);
      if (j <= NB+NB || i < NB)
         return(1);    /* quick return */
/*
 *    Make choice based on most restricted dimension
 */
      if (M < N && M < K)   /* M most restricted dim */
         goto SMALLM;
      else if (K < M && K < N)  /* K most restricted dim */
         goto SMALLK;
      else if (M == N && M == K)
         goto SQUARE;
      else  /* N is most restricted dim */
         goto SMALLN;
   }
/*
 * The following three shapes model recursive factorizations where
 * two dimensions are cut during the recursion, and a third remains large
 */
   else if (N <= 256 && K <= 256)  /* recursive shape that doesn't cut M */
   {                               /* LU uses this shape */
      i = Mmin(N, K);
      j = Mmax(N, K);
      if (i >= NB)
         i = (i+j)>>1;
      else if (i >= 8)
         i = (i+i+i+j)>>2;  /* 3/4 MIN, 1/4 MAX */
      for (bip2=1; bip2 < i; bip2 <<= 1);
      smp2 = (bip2 == i) ? bip2 : (bip2>>1);
      i = (bip2-i < i-smp2 && i > 16) ? bip2 : smp2;
      for (j=0; j < 9; j++)
         if (i & (1<<j)) break;
      D = M;
      if (TA == AtlasNoTrans)
         xop = (TB == AtlasNoTrans) ? ATL_tmmNN_SnkLm_XO : ATL_tmmNT_SnkLm_XO;
      else
         xop = (TB == AtlasNoTrans) ? ATL_tmmTN_SnkLm_XO : ATL_tmmTT_SnkLm_XO;
      #ifdef DEBUG
         printf("sNKlM_%c%c, M=%d, N=%d, K=%d rD=%d, D=%d\n", 
                T2c(TA), T2c(TB), M, N, K, j, D);
      #endif
   }
   else if (M <= 256 && N <= 256)  /* recursive shape that doesn't cut K */
   {                               /* QR uses, maybe in LARFT? */
      i = Mmin(M, N);
      j = Mmax(M, N);
      if (i >= NB)
         i = (i+j)>>1;
      else if (i >= 8)
         i = (i+i+i+j)>>2;  /* 3/4 MIN, 1/4 MAX */
      for (bip2=1; bip2 < i; bip2 <<= 1);
      smp2 = (bip2 == i) ? bip2 : (bip2>>1);
      i = (bip2-i < i-smp2 && i > 16) ? bip2 : smp2;
      for (j=0; j < 9; j++)
         if (i & (1<<j)) break;
      D = K;
      if (TA == AtlasNoTrans)
         xop = (TB == AtlasNoTrans) ? ATL_tmmNN_SmnLk_XO : ATL_tmmNT_SmnLk_XO;
      else
         xop = (TB == AtlasNoTrans) ? ATL_tmmTN_SmnLk_XO : ATL_tmmTT_SmnLk_XO;
      #ifdef DEBUG
         printf("sMNlK_%c%c, M=%d, N=%d, K=%d, rD=%d, D=%d\n", 
                T2c(TA), T2c(TB), M, N, K, j, D);
      #endif
   }
   else if (M <= 256 && K <= 256) /* recursive shape that doesn't cut N */
   {                              /* UNCONFIRMED: QR variant uses */
      i = Mmin(M, K);
      j = Mmax(M, K);
      if (i >= NB)
         i = (i+j)>>1;
      else if (i >= 8)
         i = (i+i+i+j)>>2;  /* 3/4 MIN, 1/4 MAX */
      for (bip2=1; bip2 < i; bip2 <<= 1);
      smp2 = (bip2 == i) ? bip2 : (bip2>>1);
      for (j=0; j < 9; j++)
         if (i & (1<<j)) break;
      D = N;
      if (TA == AtlasNoTrans)
         xop = (TB == AtlasNoTrans) ? ATL_tmmNN_SmkLn_XO : ATL_tmmNT_SmkLn_XO;
      else
         xop = (TB == AtlasNoTrans) ? ATL_tmmTN_SmkLn_XO : ATL_tmmTT_SmkLn_XO;
      #ifdef DEBUG
         printf("sNlMK_%c%c, M=%d, N=%d, K=%d, rD=%d, D=%d\n", 
                T2c(TA), T2c(TB), M, N, K, j, D);
      #endif
   }
/*
 * The three following shapes model static blocking, where two dimensions
 * are full, and the third is blocked
 */
   else if (K <= 256)           /* K dim small, as in right-looking LU/QR */
   {
SMALLK:
      D = Mmin(M,N);
      if (D >= NB+NB)
         D = (M+N)>>1;
      i = K;
      for (bip2=1; bip2 < i; bip2 <<= 1);
      smp2 = (bip2 == i) ? bip2 : (bip2>>1);
      i = (bip2-i < i-smp2 && i > 16) ? bip2 : smp2;
      for (j=0; j < 9; j++)
         if (i & (1<<j)) break;
      if (TA == AtlasNoTrans)
         xop = (TB == AtlasNoTrans) ? ATL_tmmNN_SkLmn_XO : ATL_tmmNT_SkLmn_XO;
      else
         xop = (TB == AtlasNoTrans) ? ATL_tmmTN_SkLmn_XO : ATL_tmmTT_SkLmn_XO;
      #ifdef DEBUG
         printf("sKlMN_%c%c, M=%d, N=%d, K=%d, rD=%d, D=%d\n", 
                 T2c(TA), T2c(TB), M, N, K, j, D);
      #endif
   }
   else if (M <= 256)          /* M dim small */
   {
SMALLM:
      D = Mmin(N,K);
      if (D >= NB+NB)
         D = (N+K)>>1;
      i = M;
      for (bip2=1; bip2 < i; bip2 <<= 1);
      smp2 = (bip2 == i) ? bip2 : (bip2>>1);
      i = (bip2-i < i-smp2 && i > 16) ? bip2 : smp2;
      for (j=0; j < 9; j++)
         if (i & (1<<j)) break;
      if (TA == AtlasNoTrans)
         xop = (TB == AtlasNoTrans) ? ATL_tmmNN_SmLnk_XO : ATL_tmmNT_SmLnk_XO;
      else
         xop = (TB == AtlasNoTrans) ? ATL_tmmTN_SmLnk_XO : ATL_tmmTT_SmLnk_XO;
      #ifdef DEBUG
         printf("sMlNK_%c%c, M=%d, N=%d, K=%d, rD=%d, D=%d\n", 
                T2c(TA), T2c(TB), M, N, K, j, D);
      #endif
   }
   else if (N <= 256)          /* N dim small */
   {                           /* QR uses this */
SMALLN:
      D = Mmin(M,K);
      if (D >= NB+NB)
         D = (M+K)>>1;
      i = N;
      for (bip2=1; bip2 < i; bip2 <<= 1);
      smp2 = (bip2 == i) ? bip2 : (bip2>>1);
      i = (bip2-i < i-smp2 && i > 16) ? bip2 : smp2;
      for (j=0; j < 9; j++)
         if (i & (1<<j)) break;
      if (TA == AtlasNoTrans)
         xop = (TB == AtlasNoTrans) ? ATL_tmmNN_SnLmk_XO : ATL_tmmNT_SnLmk_XO;
      else
         xop = (TB == AtlasNoTrans) ? ATL_tmmTN_SnLmk_XO : ATL_tmmTT_SnLmk_XO;
      #ifdef DEBUG
         printf("sNlMK_%c%c, M=%d, N=%d, K=%d, rD=%d, D=%d\n", 
                T2c(TA), T2c(TB), M, N, K, j, D);
      #endif
   }
   else                        /* all dim > 256, call it square */
   {
SQUARE:   /* near-square shape, N <= 256 if jumped here */
      D = (M+N+K+1)/3;
      j = 0;
      if (TA == AtlasNoTrans)
         xop = (TB == AtlasNoTrans) ? ATL_tmmNN_SQmnk_XO : ATL_tmmNT_SQmnk_XO;
      else
         xop = (TB == AtlasNoTrans) ? ATL_tmmTN_SQmnk_XO : ATL_tmmTT_SQmnk_XO;
      #ifdef DEBUG
         printf("SQ_%c%c, M=%d, N=%d, K=%d, D=%d\n", 
                T2c(TA), T2c(TB), M, N, K, D);
      #endif
   }

   xop += j*ATL_PDIM;
   for (k=ATL_PDIM-1; k >= 0; k--)
      if (xop[k] && D >= xop[k])
         return((k == ATL_PDIM-1) ? ATL_NTHREADS : (2<<k));
   return(1);
}
#endif
@ROUT ATL_tgemm_MKp ATL_tgemm_rkK_Np ATL_tgemm_rkK ATL_tgemm_bigMN_Kp
#include "atlas_misc.h"
#include "atlas_tcacheedge.h"
#ifndef CacheEdge
   #include "atlas_cacheedge.h"
   #ifndef CacheEdge
      #define CacheEdge 524288
   #endif
#endif
@ROUT ATL_tgemm_rkK ATL_tgemm_bigMN_Kp `#include "atlas_tlvl3.h"`
@ROUT ATL_tgemm_MKp ATL_tgemm_rkK_Np `#include "atlas_lvl3.h"`
#include "atlas_threads.h"
#include "atlas_tsumm.h"

@ROUT ATL_tgemm_MKp
static int ATL_NTHR = ATL_NTHREADS;
typedef struct
{
   void *aBcnt;           /* counter on partitions of B used in B copy */
   void *aAcnt;           /* count on the partitions of A */
   void *aCcnt;           /* count on columns of C */
   volatile int *chkin;   /* NTHR-len checkin array */
   TYPE **Aws;            /* preallocated thread copy areas */
   TYPE *Bw;              /* workspace for common B */
   const TYPE *A, *B;     /* original input matrices */
   TYPE *C;               /* original output matrix */
   SCALAR alpha;          
   SCALAR beta;
   ATL_INT Kp, nKp, klast;
   ATL_INT Mp, nMp, mlast;
   ATL_INT M, N, K, lda, ldb, ldc, nNb;
   enum ATLAS_TRANS TA, TB;
} ATL_TGEMM_MKP_t;
@ROUT ATL_tgemm_rkK_Np
typedef struct
{
   void *aNcnt;           /* count on col-panels of C */
   void *aMcnt;           /* count row-panels of A */
   volatile int *chkin;   /* ATL_NTHREAD-len checkin array */
   TYPE **Bws;            /* preallocated thread copy areas */
   TYPE *Aw;              /* workspace for common A */
   const TYPE *A, *B;     /* original input matrices */
   TYPE *C;               /* original output matrix */
   SCALAR alpha;          
   SCALAR beta;
   ATL_INT nKb, kr, kr8;
   ATL_INT nMb, mr, nNb, nr, nlblks, nrblks;
   ATL_INT M, N, K, lda, ldb, ldc;
   enum ATLAS_TRANS TA, TB;
} ATL_TGEMM_RKK_NP_t;
@ROUT ATL_tgemm_MKp ATL_tgemm_rkK_Np ATL_tgemm_rkK

/*
 * Matmul driver, loops over pre-copied A & B, operands are preblocked
 * so a column panel of B, the entire A and needed portion of C fit in the L2,
 * Therefore, use all of A against a single column panel of B, 
 * and thus do NMK loop order.
 */

#define genmm Mjoin(PATL,pKBmm)     /* cleans up any combin. of partial blks */
#define PKBmm Mjoin(PATL,pKBmm_b1)  /* cleans up full MB,NB, partial KB */
#define PNBmm Mjoin(PATL,pNBmm_b1)  /* cleans up full MB,KB, partial NB */
#define PMBmm Mjoin(PATL,pMBmm_b1)  /* cleans up full NB,KB, partial MB */

/*
 * This routine is a driver routine that makes all the appropriate calls
 * to the gemm kernel assuming both A & B are already copied to block-major
 * format and that you all you need is one traversal of the K loop.
 */
static void DoMM_K
(
   ATL_CINT mb,         /* # of rows in C <= MB */
   ATL_CINT nb,         /* # of cols in C <= NB */
   ATL_CINT nfKblks,    /* # of full blocks of K */
   ATL_CINT kr,         /* partial remainder block on K */
   const TYPE *A,       /* block-major A in (nfKblks*KB+kr) x NB panel */
   const TYPE *B,       /* block-major B in (nfKblks*KB+kr) x NB panel */
   const SCALAR beta,   /* scale C by this value */
   TYPE *C,             /* ldcxnb array for result */
   ATL_CINT ldc         /* stride betweent elts in a row of C */
)
{
#ifdef TREAL
   ATL_INT k;
   ATL_CINT incA = mb*KB, incB = KB*nb;
   NBMM0 mmk, mmk_kr=genmm, mmk_bX;

   ATL_assert(mb <= MB && nb <= NB);
   if (!nfKblks)  /* only partial block to do! */
   {
      if (mb != MB || nb != NB)
      {
         if (SCALAR_IS_ZERO(beta))
            Mjoin(PATL,gezero)(mb, nb, C, ldc);
         Mjoin(PATL,pKBmm)(mb, nb, kr, ATL_rone, A, kr, B, kr, beta, C, ldc);
      }
      else
      {
         if (SCALAR_IS_ONE(beta))
            Mjoin(PATL,pKBmm_b1)(mb, nb, kr, ATL_rone, A, kr, B, kr, beta, 
                                 C, ldc);
         else if (SCALAR_IS_ZERO(beta))
            Mjoin(PATL,pKBmm_b0)(mb, nb, kr, ATL_rone, A, kr, B, kr, beta, 
                                 C, ldc);
         else
            Mjoin(PATL,pKBmm_bX)(mb, nb, kr, ATL_rone, A, kr, B, kr, beta, 
                                 C, ldc);
      }
      return;
   }
   if (mb != NB && nb != NB)
      mmk_bX = mmk = genmm;
   else if (mb != NB)
   {
      mmk = PMBmm;
      if (SCALAR_IS_ONE(beta))
         mmk_bX = Mjoin(PATL,pMBmm_b1);
      else if (SCALAR_IS_ZERO(beta))
         mmk_bX = Mjoin(PATL,pMBmm_b0);
      else
         mmk_bX = Mjoin(PATL,pMBmm_bX);
   }
   else if (nb != NB)
   {
      mmk = PNBmm;
      if (SCALAR_IS_ONE(beta))
         mmk_bX = Mjoin(PATL,pNBmm_b1);
      else if (SCALAR_IS_ZERO(beta))
         mmk_bX = Mjoin(PATL,pNBmm_b0);
      else
         mmk_bX = Mjoin(PATL,pNBmm_bX);
   }
   else
   {
      mmk = NBmm;
      mmk_kr = PKBmm;
      if (SCALAR_IS_ONE(beta))
         mmk_bX = NBmm_b1;
      else if (SCALAR_IS_ZERO(beta))
         mmk_bX = NBmm_b0;
      else
         mmk_bX = NBmm_bX;
   }
   mmk_bX(mb, nb, KB, ATL_rone, A, KB, B, KB, beta, C, ldc);  /* apply beta */
   A += incA; B += incB;
   for (k=1; k < nfKblks; k++, A += incA, B += incB)  /* full blocks */
      mmk(mb, nb, KB, ATL_rone, A, KB, B, KB, ATL_rone, C, ldc);
   if (kr)  /* partial remainder */
      mmk_kr(mb, nb, kr, ATL_rone, A, kr, B, kr, ATL_rone, C, ldc);
#else  /* complex code */
   ATL_INT k;
   ATL_CINT incA = mb*KB*2, incB = 2*KB*nb;
   const TYPE one[2] = {ATL_rone, ATL_rzero};
   const TYPE *bet = beta;
   NBMM0 mmk, mmk_kr=genmm, mmk_bX;

   ATL_assert(mb <= MB && nb <= NB);
   if (!nfKblks)  /* only partial block to do! */
   {
      if (beta[1] == ATL_rzero && beta[0] != ATL_rzero)
         Mjoin(PATL,pKBmm)(mb, nb, kr, ATL_rone, A, kr, B, kr, *beta, C, ldc);
      else
      {
         if (*beta == ATL_rzero && beta[1] == ATL_rzero)
            Mjoin(PATL,gezero)(mb, nb, C, ldc);
         else
            Mjoin(PATL,gescal)(mb, nb, beta, C, ldc);
         Mjoin(PATL,pKBmm)(mb, nb, kr, ATL_rone, A, kr, B, kr, ATL_rone,C, ldc);
      }
      return;
   }
   if (mb == MB && nb == NB)
   {
      if (beta[1] == ATL_rzero)  /* real scalar */
      {
         if (*beta == ATL_rone)
         {
            NBmm_b1(mb, nb, KB, ATL_rone, A, KB, B, KB, ATL_rone, C, ldc);
         }
         else if (*beta == ATL_rzero)
         {
            NBmm_b0(mb, nb, KB, ATL_rone, A, KB, B, KB, ATL_rzero, C, ldc);
         }
         else
         {
            NBmm_bX(mb, nb, KB, ATL_rone, A, KB, B, KB, *beta, C, ldc);
         }
      }
      else /* must scale for complex beta */
      {
         Mjoin(PATL,gescal)(mb, nb, beta, C, ldc);
         NBmm_b1(mb, nb, KB, ATL_rone, A, KB, B, KB, ATL_rone, C, ldc);
      }
      A += incA; B += incB;
      for (k=1; k < nfKblks; k++, A += incA, B += incB)  /* full blocks */
         NBmm_b1(mb, nb, KB, ATL_rone, A, KB, B, KB, ATL_rone, C, ldc);
      if (kr)  /* partial remainder */
         Mjoin(PATL,pKBmm)(mb, nb, kr, ATL_rone, A, kr, B, kr, ATL_rone,C, ldc);
      return;
   }
   if (mb != MB)
   {
      if (nb != NB)  /* both blocks partial */
      {
         if (*beta == ATL_rzero && beta[1] == ATL_rzero)
         {
            Mjoin(PATL,gezero)(mb, nb, C, ldc);
            bet = one;
         }
         else if (beta[1] != ATL_rzero)
         {
            Mjoin(PATL,gescal)(mb, nb, beta, C, ldc);
            bet = one;
         }
         mmk = mmk_bX =  Mjoin(PATL,pKBmm);
      }
      else  /* only M block is partial */
      {
         if (beta[1] != ATL_rzero)
         {
            Mjoin(PATL,gescal)(mb, nb, beta, C, ldc);
            bet = one;
         }
         mmk = Mjoin(PATL,pMBmm_b1);
         if (*bet == ATL_rone)
            mmk_bX = Mjoin(PATL,pMBmm_b1);
         else if (*bet == ATL_rzero)
            mmk_bX = Mjoin(PATL,pMBmm_b0);
         else
            mmk_bX = Mjoin(PATL,pMBmm_bX);
      }
   }
   else if (nb != MB)  /* only N block is partial */
   {
      if (beta[1] != ATL_rzero)
      {
         Mjoin(PATL,gescal)(mb, nb, beta, C, ldc);
         bet = one;
      }
      mmk = Mjoin(PATL,pNBmm_b1);
      if (*bet == ATL_rone)
         mmk_bX = Mjoin(PATL,pNBmm_b1);
      else if (*bet == ATL_rzero)
         mmk_bX = Mjoin(PATL,pNBmm_b0);
      else
         mmk_bX = Mjoin(PATL,pNBmm_bX);
   }
   mmk_bX(mb, nb, KB, ATL_rone, A, KB, B, KB, *bet, C, ldc);  /* apply beta */
   A += incA; B += incB;
   for (k=1; k < nfKblks; k++, A += incA, B += incB)  /* full blocks */
      mmk(mb, nb, KB, ATL_rone, A, KB, B, KB, ATL_rone, C, ldc);
   if (kr)  /* partial remainder */
      Mjoin(PATL,pKBmm)(mb, nb, kr, ATL_rone, A, kr, B, kr, ATL_rone, C, ldc);
#endif
}

@ROUT ATL_tgemm_MKp ATL_tgemm_rkK_Np
/*
 * This routine is a driver routine that makes all the appropriate calls
 * to the gemm kernel assuming both A & B are already copied to block-major
 * format.
 */
static void DoMM_NMK
(
   ATL_CINT nfMblks,    /* # of full blocks of M */
   ATL_CINT mr,         /* partial remainder block on M */
   ATL_CINT nfNblks,    /* # of full blocks of N */
   ATL_CINT nr,         /* partial remainder block on N */
   ATL_CINT nfKblks,    /* # of full blocks of K */
   ATL_CINT kr,         /* partial remainder block on K */
   const TYPE *A,       /* block-major A in (nfKblks*KB+kr)xNB panels */
   const TYPE *B,       /* block-major B in (nfKblks*KB+kr)xNB panels */
   TYPE *C,             /* ldaxN array for result */
   ATL_CINT ldc         /* stride betweent elts in a row of C */
)
{
   ATL_INT j;
   ATL_CINT K = nfKblks*NB + kr;

   for (j=0; j < nfNblks; j++)
   {
      const TYPE *Bc = B + K*NB*j;
      TYPE *Cc = C + j*NB*ldc;
      ATL_INT i;

      for (i=0; i < nfMblks; i++)
      {
         const TYPE *a = A + K*NB*i;
         const TYPE *b = Bc;
         TYPE *c = Cc + i*NB;
         ATL_INT k;

         for (k=0; k < nfKblks; k++)
         {
            NBmm(MB, NB, KB, ATL_rone, a, KB, b, KB, ATL_rone, c, ldc);
            a += NBNB;
            b += NBNB;
         }
         if (kr)  /* partial KB requires PKBmm */
            PKBmm(MB, NB, kr, ATL_rone, a, kr, b, kr, ATL_rone, c, ldc);
      }
      if (mr) /* can use PMBmm for all full blocks of N & K */
      {
         const TYPE *b = Bc;
         const TYPE *a = A + K*NB*nfMblks;
         TYPE *c = Cc + nfMblks*NB;
         ATL_INT k;
         const int mrNB = mr*NB;

         for (k=0; k < nfKblks; k++)
         {
            PMBmm(mr, NB, KB, ATL_rone, a, KB, b, KB, ATL_rone, c, ldc);
            a += mrNB;
            b += NBNB;
         }
         if (kr)  /* partial KB requires genmm */
            genmm(mr, NB, kr, ATL_rone, a, kr, b, kr, ATL_rone, c, ldc);
      }
   }
   if (nr) /* can use PNBmm for all full blks of M/K, must use genmm rest */
   {
      const TYPE *Bc = B + K*nfNblks*NB;
      TYPE *Cc = C + nfNblks*NB*ldc;
      ATL_CINT nrNB = nr * NB;
      ATL_INT i;

      for (i=0; i < nfMblks; i++)
      {
         const TYPE *a = A + K*NB*i, *b=Bc;
         TYPE *c = Cc + i*NB;
         ATL_INT k;

         for (k=0; k < nfKblks; k++)
         {
            PNBmm(MB, nr, KB, ATL_rone, a, KB, b, KB, ATL_rone, c, ldc);
            a += NBNB;
            b += nrNB;
         }
         if (kr)  /* partial KB&NB requires genmm */
            genmm(MB, nr, kr, ATL_rone, a, kr, b, kr, ATL_rone, c, ldc);
      }
      if (mr) /* must use genmm for two or more partial dim */
      {
         const TYPE *a = A + K*NB*nfMblks, *b = Bc;
         TYPE *c = Cc + nfMblks*NB;
         const int mrNB = mr*NB;
         ATL_INT k;

         for (k=0; k < nfKblks; k++)
         {
            genmm(mr, nr, KB, ATL_rone, a, KB, b, KB, ATL_rone, c, ldc);
            a += mrNB;
            b += nrNB;
         }
         if (kr)  /* partial KB requires genmm */
            genmm(mr, nr, kr, ATL_rone, a, kr, b, kr, ATL_rone, c, ldc);
      }
   }
}
@ROUT ATL_tgemm_MKp ATL_tgemm_rkK_Np ATL_tgemm_rkK

/* 
 * Takes a MxN block and expands it to an ldaxM block wt zero padding in-place
 */
#ifdef TREAL
static void ExpandBlock
(
   ATL_CINT M, /* the number of rows that should be expanded to lda */
   ATL_CINT N, /* number of columns in block */
   TYPE *A,    /* in: MxN block, out: ldaxN blk, wt zero-padding in lda-M gap */
   ATL_INT lda /* desired stride between columns */
)
{
   TYPE *a, *c;
   ATL_CINT gap = lda - M;
   ATL_INT j;

   if (gap < 1)   /* already done if lda == M */
      return;

//fprintf(stderr, "ExpandBlock, M=%d, N=%d, A=%p, lda=%d\n", M, N, A, lda);
   a = A + N*M - 1;
   c = A + N*lda - 1;
   for (j=N; j; j--)
   {
      TYPE *stop = c - gap;
      do 
         *c-- = ATL_rzero;
      while (c != stop);
      stop =  c - M;
      do
         *c-- = *a--;
      while (c != stop);
   }
}
#endif

@ROUT ATL_tgemm_rkK_Np
/*
 * This routine assumes A has already been copied to block-major, row-panel
 * format and had ALPHA applied to it, and threads should now cooperate to:
 * (a) Copy an K8xNB piece of B to their cache in their workspace
 * (b) Use that copied piece against all of A to update a col-panel of C
 * (c) Repeat until all column-panels of B have been applied
 * They determine what col panel of B to operate on using the aNcnt variable.
 */
static void ATL_tloopN
(
@skip   void **aNcnts,       /* NTHR Atomic counters on col panels of C */
   void *aNcnt,         /* Atomic counter on col panels of C */
   int iam,             /* my rank; */
   ATL_CINT nfMblks,    /* # of full NB blocks in Mp */
   int mr,              /* M%NB */
   ATL_CINT nfNblks,    /* # of full NB blocks along N */
   int nr,              /* N%NB */
@skip   int nlblks,          /* nfNblks / ATL_NTHREADS */
@skip   int nrblks,          /* nfNblks % ATL_NTHREADS */
   ATL_CINT nfKblks,    /* # of full NB blocks along K8 */
   int kr,              /* K%NB */
   int krpad,           /* # of entries to add to K-dim to make mul of 8 */
   const TYPE *A,       /* A in blk-major, row-panel (K8xMB) format */
   enum ATLAS_TRANS TB, /* Transpose setting of B */
   const TYPE *B,       /* B in original column-major format */
   ATL_CINT ldb,        /* leading dim of B */
   TYPE *pB,            /* my private workspace to copy col-panels into */
   TYPE *C,             /* original C matrix */
   ATL_CINT ldc         /* leading dim of C */
)
{
   ATL_CINT M = nfMblks*NB+mr, N = nfNblks*NB+nr, K = nfKblks*NB+kr, K8=K+krpad;
   ATL_CINT kr8 = kr+krpad, nnblks = nr ? nfNblks+1 : nfNblks;
   ATL_CINT kr8f = (kr8 >= NB) ? 0 : kr8;
   ATL_CINT nfKblksf = (kr8 >= NB) ? nfKblks+1 : nfKblks;
   ATL_INT jblk, p;
   ATL_CINT BNOTRANS = (TB == AtlasNoTrans);
   ATL_CINT bmul = (BNOTRANS) ? ldb : 1, nNblks=nfNblks+1;

   while (jblk = ATL_DecGlobalAtomicCount(aNcnt, iam))
   {
      ATL_INT nb, nnb, nbr;
      size_t j;

      if (jblk != nNblks)
      {
         nnb = 1;
         nbr = 0;
         nb = NB;
      }
      else
      {
         nnb = 0;
         nbr = nr;
         nb = nr;
      }
      j = (jblk-1)*(NB SHIFT);
/*
 *    Copy the specified col-panel to my private workspace
 */
      if (BNOTRANS)
         Mjoin(PATL,col2blk_a1)(K, nb, B+j*ldb, ldb, pB, ATL_rone);
      else
         Mjoin(PATL,row2blkT_a1)(K, nb, B+j, ldb, pB, ATL_rone);
/*
 *    If kr is nonzero and not a multiple of 8, pad K with K8-K zeros
 *    This allows us to cause less K-cleanup kernels to be loaded, as
 *    well as ensuring we keep things aligned for vectorized cleanup kernels
 *    This will tend to depress perf when kr is small, and improve it when
 *    kr is large and not a multiple of the vector length
 */
      if (krpad)
         ExpandBlock(kr, nb, pB+nfKblks*NB*nb, kr8);
/* 
 *    Perform the rank-K8 update of the column panel of C
 */
      DoMM_NMK(nfMblks, mr, nnb, nbr, nfKblksf, kr8f, A, pB, C+j*ldc, ldc);
   }
}

@ROUT ATL_tgemm_rkK_Np
void Mjoin(PATL,DoWork_rkK_Np)(ATL_LAUNCHSTRUCT_t *lp, void *vp)
/* 
 * This routine has everyone cooperate to copy row-panels of A, and then
 * calls tloopN to perform the rank-K update
 */
{
   ATL_thread_t *tp=vp;
   ATL_TGEMM_RKK_NP_t *pd=lp->vp;
   ATL_CINT iam = tp->rank;
   volatile int *chkin = pd->chkin;
   TYPE *Bw=pd->Bws[iam], *Aw=pd->Aw;
   void *aMcnt=pd->aMcnt;
   ATL_CINT K = pd->K;
   ATL_CINT BNOTRANS = (pd->TB == AtlasNoTrans);
   ATL_CINT ANOTRANS = (pd->TA == AtlasNoTrans);
   const TYPE *A=pd->A, *B=pd->B;
   TYPE *C = pd->C;
   SCALAR alpha = pd->alpha;
   ATL_CINT nMb=pd->nMb, mr=pd->mr, nKb=pd->nKb, kr=pd->kr, kr8=pd->kr8;
   ATL_CINT nablks = (mr) ? nMb+1 : nMb, incA = (nKb*NB + kr8)*NB;
   ATL_CINT lda=pd->lda, amul = (ANOTRANS) ? (1 SHIFT) : (lda SHIFT);
   ATL_INT iblk;
   MAT2BLK A2blk;
   int k;

   if (ANOTRANS)
      A2blk = (SCALAR_IS_ONE(alpha)) ? 
              Mjoin(PATL,row2blkT_a1) : Mjoin(PATL,row2blkT_aX);
   else
      A2blk = (SCALAR_IS_ONE(alpha)) ? 
              Mjoin(PATL,col2blk_a1) : Mjoin(PATL,col2blk_aX);
/*
 * Use the AtomicCounter aMcnt to copy the nMb full row panels of A, 
 * and possibly one partial mr-wide row panel.  Since K is not long, copying
 * them a rowpanel at a time shouldn't kill us on the TLB.
 */
   while(iblk = ATL_DecGlobalAtomicCount(aMcnt, iam))
   {
      int mb;
      size_t i, ia;

      mb = (iblk-- == nablks && mr) ? mr : NB;
      i = iblk*NB;
      i *= amul;
      A2blk(K, mb, A+i, lda, Aw+iblk*incA, alpha);
      if (kr8 != kr)
         ExpandBlock(kr, mb, Aw+iblk*incA+nKb*NB*mb, kr8);
   }
/*
 * Tell everyone I have finished copying A, and then loop until everyone
 * else signals they've copied their pieces
 */
   chkin[iam] = 1;
   for (k=0; k < ATL_NTHREADS; k++)
      while(!chkin[k]) ATL_POLL;
/*
 * Perform the rank-K update with a fully-copied A
 */
   ATL_tloopN(pd->aNcnt, iam, nMb, mr, pd->nNb, pd->nr, 
              nKb, kr, kr8-kr, Aw, pd->TB, pd->B, pd->ldb, pd->Bws[iam], 
              pd->C, pd->ldc);
}
@ROUT ATL_tgemm_rkK
void Mjoin(PATL,DoWork_rkK)(ATL_LAUNCHSTRUCT_t *lp, void *vp)
/* 
 * This routine has everyone cooperate to copy row-panels of A, and then
 * loops over atomic counters on N & M to perform the rank-K update
 */
{
   ATL_thread_t *tp=vp;
   ATL_TGEMM_RKK_t *pd=lp->opstruct;
   ATL_CINT iam = tp->rank;
   volatile int *chkin = pd->chkin;
   TYPE *Bw=pd->Bws[iam], *Aw=pd->Aw;
   void *aMcnt=pd->aMcnt, *aNcnt=pd->aNcnt, **aMcnts = pd->aMcnts;
   ATL_CINT K = pd->K;
   ATL_CINT BNOTRANS = (pd->TB == AtlasNoTrans);
   ATL_CINT ANOTRANS = (pd->TA == AtlasNoTrans);
   const TYPE *A=pd->A, *B=pd->B;
   TYPE *C = pd->C;
   #ifdef TREAL
      TYPE alpha = pd->alpha;
      TYPE beta  = pd->beta;
   #else
      const SCALAR alpha = pd->alpha;
      const SCALAR beta  = pd->beta;
   #endif
   ATL_CINT nMb=pd->nMb, mr=pd->mr, nNb=pd->nNb, nr=pd->nr;
   ATL_CINT nKb=pd->nKb, kr=pd->kr, kr8=pd->kr8, krpad = kr8-kr;
   size_t incA = (nKb*KB + kr8)*(MB SHIFT), ldc = pd->ldc;
   size_t lda=pd->lda, amul = (ANOTRANS) ? (1 SHIFT) : (lda SHIFT);
   size_t ldb=pd->ldb, P=ATL_NTHREADS;
   ATL_CINT tnMblks = (mr) ? nMb+1 : nMb, tnNblks = (nr) ? nNb+1 : nNb;
   ATL_CINT kr8f = (kr8 >= NB) ? 0 : kr8;
   ATL_CINT nfKblks = (kr8 >= NB) ? nKb+1 : nKb;
   ATL_CINT bmul = (BNOTRANS) ? (ldb SHIFT) : (1 SHIFT);
   ATL_INT iblk, jblk;
/*   #define PRINTTOTALS */
   #ifdef PRINTTOTALS
      int myblks = 0, hisblks=0;
      static int myarr[ATL_NTHREADS], hisarr[ATL_NTHREADS];
   #endif
   MAT2BLK A2blk;
   #ifdef TREAL
      MAT2BLK B2blk = (BNOTRANS) ? Mjoin(PATL,col2blk_a1) : 
                                   Mjoin(PATL,row2blkT_a1);
   #else
      MAT2BLK B2blk = (BNOTRANS) ? Mjoin(PATL,col2blk_a1) : 
                                  (pd->TB == AtlasTrans) ?
                                   Mjoin(PATL,row2blkT_a1):
                                   Mjoin(PATL,row2blkC_a1);
   #endif
   int k;
   #ifdef TCPLX
      const TYPE one[2] = {ATL_rone, ATL_rzero};
   #else
      #define one ATL_rone
   #endif
#ifdef TREAL
   if (ANOTRANS)
      A2blk = (SCALAR_IS_ONE(alpha)) ? 
              Mjoin(PATL,row2blkT_a1) : Mjoin(PATL,row2blkT_aX);
   else
      A2blk = (SCALAR_IS_ONE(alpha)) ? 
              Mjoin(PATL,col2blk_a1) : Mjoin(PATL,col2blk_aX);
#else
   if (ANOTRANS)
   {
      if (alpha[1] == ATL_rzero)
      {
         if (*alpha == ATL_rone)
            A2blk = Mjoin(PATL,row2blkT_a1);
         else
            A2blk = Mjoin(PATL,row2blkT_aXi0);
      }
      else
         A2blk = Mjoin(PATL,row2blkT_aX);
   }
   else if (pd->TA == AtlasConjTrans)
   {
      if (alpha[1] == ATL_rzero)
      {
         if (*alpha == ATL_rone)
            A2blk = Mjoin(PATL,col2blkConj_a1);
         else
            A2blk = Mjoin(PATL,col2blkConj_aXi0);
      }
      else
         A2blk = Mjoin(PATL,col2blkConj_aX);
   }
   else  /* TA == AtlasTrans */
   {
      if (alpha[1] == ATL_rzero)
      {
         if (*alpha == ATL_rone)
            A2blk = Mjoin(PATL,col2blk_a1);
         else
            A2blk = Mjoin(PATL,col2blk_aXi0);
      }
      else
         A2blk = Mjoin(PATL,col2blk_aX);
   }
#endif
/*
 * Use the AtomicCounter aMcnt to copy the nMb full row panels of A, 
 * and possibly one partial mr-wide row panel.  Since K is not long, copying
 * them a rowpanel at a time shouldn't kill us on the TLB.
 */
   while(iblk = ATL_DecGlobalAtomicCount(aMcnt, iam))
   {
      int mb;
      size_t i, ia;

      mb = (iblk-- == tnMblks && mr) ? mr : MB;
      i = iblk*NB;
      i *= amul;
      A2blk(K, mb, A+i, lda, Aw+iblk*incA, alpha);
      #ifdef TREAL
         if (kr8 != kr)
            ExpandBlock(kr, mb, Aw+iblk*incA+nKb*KB*mb, kr8);
      #endif
   }
/*
 * Tell everyone I have finished copying A, and then loop until everyone
 * else signals they've copied their pieces
 */
   if (iam == 0)
   {
      for (k=1; k < P; k++)
         while(!chkin[k]) 
            ATL_POLL;
      chkin[0] = 1;
   }
   else
   {
      chkin[iam] = 1;
      while (!chkin[0]);
   }
/*
 * Perform the rank-K update with a fully-copied A by looping over column-panels
 * of C in reverse order
 */
   ATL_mutex_lock(pd->Mlocks[iam]);
   while (jblk = ATL_DecGlobalAtomicCount(aNcnt, iam))
   {
      ATL_INT nb, pL;
      size_t j;
      void *aCrow;
      TYPE *c;

      nb = (jblk != tnNblks || !nr) ? NB : nr;
      pd->Js[iam] = j = (jblk-1)*NB;
      c = C + j*(ldc SHIFT);
/*
 *    Copy the specified column-panel of B to my private workspace
 */
      B2blk(K, nb, B+j*bmul, ldb, Bw, one);
/*
 *    If kr is nonzero and not a multiple of 8, pad K with K8-K zeros
 *    This allows us to cause less K-cleanup kernels to be loaded, as
 *    well as ensuring we keep things aligned for vectorized cleanup kernels
 *    This will tend to depress perf when kr is small, and improve it when
 *    kr is large and not a multiple of the vector length
 */
      #ifdef TREAL
      if (krpad)
         ExpandBlock(kr, nb, Bw+nKb*KB*nb, kr8);
      #endif
/*
 *    Given that I'm working on col-panel j, determine the percentage of its
 *    blocks that I reserve for myself based on how many columns are left.
 *    If there are plenty of columns left, do all local counters which will
 *    reduce the counter cost by something like a factor of 10.  If we are
 *    getting close to running out of col-panels, reserve less and less of
 *    the problem for my exclusive use.
 */
      if (jblk >= P+P)
         pL =  100;
      else if (jblk <= 2)
         pL = 0;
      else
         pL = (jblk > P) ? 50 : 100/P;
      aCrow = aMcnts[iam];
      ATL_ResetGlobalAtomicCount(aCrow, tnMblks, pL);
      ATL_mutex_unlock(pd->Mlocks[iam]);
      while (iblk = ATL_DecGlobalAtomicCount(aCrow, 0))
      {
         const int mb = (!mr || iblk != 1) ? MB : mr;
         const size_t i = (tnMblks-iblk)*(NB SHIFT);
         #ifdef PRINTTOTALS
            myblks++;
         #endif

         iblk = tnMblks - iblk;
         DoMM_K(mb, nb, nfKblks, kr8f, Aw+iblk*incA, Bw, beta, c+i, ldc);
      }
      ATL_mutex_lock(pd->Mlocks[iam]);
   }
   ATL_mutex_unlock(pd->Mlocks[iam]);
   chkin[iam] = -3;  /* let everyone know I've finished my columns */
/*
 * When no more col-panels of C are available, it is time to see if I can
 * help other workers finish their columns; As long as someone hasn't
 * signaled his completion (negative # in chkin), continue trying to steal.
 */
   do
   {
/*
 *    If anyone is still working, continue looking to steal his work
 */
      for (k=0; k < P && chkin[k] <= 0; k++);
      if (k == P)
         break;    /* everyone done, quit */
      for (; k < P; k++)
      {
         const int rk = k;
         void *aCrow = aMcnts[rk];
    
         Bw = pd->Bws[rk];
         ATL_mutex_lock(pd->Mlocks[rk]);
         if (ATL_GetGlobalAtomicCount(aCrow, 1))
         {
            TYPE *c = C + pd->Js[rk]*(((size_t)ldc)SHIFT);
            int nb;
   
            nb = pd->N - pd->Js[rk];
            nb = Mmin(NB, nb);
            while (iblk = ATL_DecGlobalAtomicCount(aCrow, 1))
            {
               const int mb = (iblk != 1 || !mr) ? MB : mr;
               const size_t i = (tnMblks-iblk)*(NB SHIFT);
      
               #ifdef PRINTTOTALS
                  hisblks++;
               #endif
               iblk = tnMblks - iblk;
               DoMM_K(mb, nb, nfKblks, kr8f, Aw+(iblk*incA), Bw, beta,
                      c+i, ldc);
            }
         }
         ATL_mutex_unlock(pd->Mlocks[rk]);
      }
   }
   while(1);
/*
 * Master process is waiting on thread 0, so 0 stays here until all nodes
 * complete their operations
 */
   chkin[iam] = -2;
   #ifdef PRINTTOTALS
      myarr[iam] = myblks;
      hisarr[iam] = hisblks;
   #endif
   if (pd->Sync0 && !iam)
   {
      #ifdef PRINTTOTALS
         int lblks, rblks;
      #endif
      for (k=1; k < P; k++)
         while(chkin[k] != -2)
            ATL_POLL;
      #ifdef PRINTTOTALS
          printf(" myblks : %4d", myarr[0]);
          lblks = myarr[0];
          for (k=1; k < P; k++)
          {
             lblks += myarr[k];
             printf(",%4d", myarr[k]);
          }
          printf(" = %d\n", lblks);
          printf("hisblks : %4d", hisarr[0]);
          rblks = hisarr[0];
          for (k=1; k < P; k++)
          {
             rblks += hisarr[k];
             printf(",%4d", hisarr[k]);
          }
          printf(" = %d\n", rblks);
          printf("Total = %d, expected=%d\n", lblks+rblks, 
                 ((pd->M+NB-1)/NB)*((pd->N+NB-1)/NB));
      #endif
   }
}
@ROUT  ATL_tgemm_bigMN_Kp
void Mjoin(PATL,DoWork_bigMN_Kp)(ATL_LAUNCHSTRUCT_t *lp, void *vp)
{
   ATL_thread_t *tp = vp;
   const int iam = tp->rank, P = tp->P;
   ATL_TGEMM_RKK_t *pd = lp->opstruct;
   volatile int *chkin = pd->chkin+P, *hischk = pd->chkin;
   ATL_CINT K = pd->K, Kp = pd->kr, nkb = Kp / NB;
   ATL_CINT nnb = (pd->nr) ? pd->nNb+1 : pd->nNb;
   ATL_CINT nmb = (pd->mr) ? pd->nMb+1 : pd->nMb;
   const size_t incA = (pd->TA == AtlasNoTrans) ? (pd->lda SHIFT) : (1 SHIFT); 
   const size_t incB = (pd->TB == AtlasNoTrans) ? (1 SHIFT) : (pd->ldb SHIFT);
   ATL_INT k, kb;
   int i, n;
   const TYPE *A = pd->A, *B = pd->B;
   void Mjoin(PATL,DoWork_rkK)(ATL_LAUNCHSTRUCT_t *lp, void *vp);
   #ifdef TCPLX
      const TYPE one[2] = {ATL_rone, ATL_rzero};
   #else
      #define one ATL_rone
   #endif

   for (k=0; k < K; k += Kp)
   {
      kb = K - k;
      kb = Mmin(kb, Kp);
/*
 *    To avoid race conditions, thread 0 sets everything up, and rest of
 *    threads await his signal to begin
 */
      if (iam == 0)
      {
         n = chkin[0] + 1;
         for (i=1; i < P; i++)
            while(chkin[i] < n)
               ATL_POLL;
         for (i=0; i < P; i++)
            hischk[i] = 0;
         pd->beta = (k) ? one : pd->beta;
         pd->A = A + k*incA;
         pd->B = B + k*incB;
         ATL_ResetGlobalAtomicCount(pd->aNcnt, nnb, 0);
         ATL_ResetGlobalAtomicCount(pd->aMcnt, nmb, 0);
         pd->K = kb;
         if (kb == Kp)
         {
            pd->kr = pd->kr8 = 0;
            pd->nKb = nkb;
         }
         else
         {
            pd->nKb = kb / KB;
            pd->kr = kb - pd->nKb * KB;
            #ifdef TREAL
               pd->kr = kb - pd->nKb * KB;
               pd->kr8 = ((pd->kr+7)>>3)<<3;
               if (pd->kr8 > KB)
                  pd->kr8 = KB;
            #else
               pd->kr8 = pd->kr = kb - pd->nKb * KB;
            #endif
         }
         chkin[0] = n;
      }
      else
      {
         n = ++chkin[iam];
         while (chkin[0] < n)
            ATL_POLL;
      }
      Mjoin(PATL,DoWork_rkK)(lp, vp);  /* do rank-K update */
   }
/*
 * Master process waiting on thread 0, who therefore must block until everyone
 * signals completion
 */
   n = ++chkin[iam];
   if (!iam)
      for (i=1; i < P; i++)
         while (chkin[i] < n)
            ATL_POLL;
}
#ifdef TREAL
   #undef one
#endif
   

int Mjoin(PATL,tgemm_bigMN_Kp)
   (const enum ATLAS_TRANS TA, const enum ATLAS_TRANS TB, ATL_CINT M,
    ATL_CINT N, ATL_CINT K, const SCALAR alpha, const TYPE *A, ATL_CINT lda,
    const TYPE *B, ATL_CINT ldb, const SCALAR beta, TYPE *C, ATL_CINT ldc)
/*
 * This routine handles the asymtotic case where A & B are too large to be
 * copied at once, and we loop over CacheEdge-sized partitions of K.
 * Implements very large gemm as a series of synchronized rank-Kp updates.
 */
{
   ATL_TGEMM_RKK_t pd;   /* problem definition */
   size_t sz, Kp;
   void *vp;
   int i;
   #ifdef FindingCE
      extern int FoundCE, CompCE;
      #define MY_CE FoundCE
   #else
      #define MY_CE CacheEdge
   #endif

   #ifdef TREAL
      Kp = (ATL_DivBySize(MY_CE) - MB*NB)/(MB + 2*NB);
   #else
      Kp = (ATL_DivBySize(MY_CE) - 2*MB*NB)/(2*MB + 4*NB);
   #endif
   Kp = (Kp/KB)*KB;
   #ifdef FindingCE
      if (MY_CE == 0)
         Kp = K;
      if (Kp < KB)
         Kp = KB;
      if (CompCE)
      {
         CompCE = Kp;
         return;
      }
   #else
      if (Kp < KB)
         return(1);  /* not going to be efficient */
   #endif

   pd.kr = Kp;
   pd.TA = TA;
   pd.TB = TB;
   pd.M = M;
   pd.N = N;
   pd.K = K;
   pd.alpha = alpha;
   pd.A = A;
   pd.lda = lda;
   pd.B = B;
   pd.ldb = ldb;
   pd.beta = beta;
   pd.C = C;
   pd.ldc = ldc;
   pd.nMb = M / MB;
   pd.mr = M - pd.nMb*MB;
   pd.nNb = N / NB;
   pd.nr = N - pd.nNb*NB;

   sz = ATL_MulBySize(Kp)*M + ATL_Cachelen;  /* sizeof A workspace */
   sz += ATL_NTHREADS*(Kp*ATL_MulBySize(NB)+ATL_Cachelen);  /* B workspaces */
   sz += ATL_NTHREADS*3*sizeof(int);  /* chkin1/2 & Js arrays */
   sz += ATL_NTHREADS*sizeof(TYPE*);  /* Bws array */
   sz += ATL_NTHREADS*2*sizeof(void*); /* aMcnts & Mlocks arrays */
   if (sz > ATL_NTHREADS*ATL_PTMAXMALLOC+ATL_MaxMalloc)
      return(2);  /* can't allocate space */
   pd.Bws = malloc(sz);
   if (!pd.Bws)
      return(3);
   pd.aMcnts = (void**)(pd.Bws+ATL_NTHREADS);
   pd.Mlocks = (pd.aMcnts+ATL_NTHREADS);
   pd.chkin = (volatile int*)(pd.Mlocks+ATL_NTHREADS);
   pd.Js = (int*)(pd.chkin + 2*ATL_NTHREADS);
   pd.Aw = (TYPE *) (pd.Js + ATL_NTHREADS);
   pd.Aw = ATL_AlignPtr(pd.Aw);
   pd.Sync0 = 0;
   vp = pd.Aw + M * (Kp SHIFT); 
   pd.Bws[0] = ATL_AlignPtr(vp);
   for (i=1; i < ATL_NTHREADS; i++)
   {
      vp = pd.Bws[i-1] + (NB SHIFT)*Kp;
      pd.Bws[i] = ATL_AlignPtr(vp);
   }
   for (i=0; i < ATL_NTHREADS; i++)
   {
      pd.Mlocks[i] = ATL_mutex_init();
      pd.aMcnts[i] = ATL_SetGlobalAtomicCount(1, 0, 0);
      pd.chkin[i] = pd.chkin[ATL_NTHREADS+i] = 0;
      pd.Js[i] = 0;
   }
   pd.aMcnt = ATL_SetGlobalAtomicCount(ATL_NTHREADS, 1, 0);
   pd.aNcnt = ATL_SetGlobalAtomicCount(ATL_NTHREADS, 1, 0);

   ATL_goparallel(ATL_NTHREADS, Mjoin(PATL,DoWork_bigMN_Kp), &pd, NULL);
/*
 * Free allocated resources
 */
   ATL_FreeGlobalAtomicCount(pd.aMcnt);
   ATL_FreeGlobalAtomicCount(pd.aNcnt);
   for (i=0; i < ATL_NTHREADS; i++)
   {
      ATL_mutex_free(pd.Mlocks[i]);
      ATL_FreeGlobalAtomicCount(pd.aMcnts[i]);
   }
   free(pd.Bws);
   return(0);
}
@ROUT ATL_tgemm_rkK_Np 
   @define sf @_Np@
@ROUT ATL_tgemm_rkK
   @define sf @@
@ROUT ATL_tgemm_rkK_Np ATL_tgemm_rkK
int Mjoin(PATL,tgemm_rkK@(sf))
   (const enum ATLAS_TRANS TA, const enum ATLAS_TRANS TB, ATL_CINT M,
    ATL_CINT N, ATL_CINT K, const SCALAR alpha, const TYPE *A, ATL_CINT lda,
    const TYPE *B, ATL_CINT ldb, const SCALAR beta, TYPE *C, ATL_CINT ldc)
/*
 * Does a rank-K update on dynamically scheduled column panels of C
 */
{
   ATL_TGEMM_RKK@up@(sf)_t pd;   /* problem definition */
   size_t sz;
   volatile int *chkin;
   TYPE **Bws, *Aw;
   ATL_CINT nKb = ATL_DivByNB(K), kr = K - ATL_MulByNB(nKb);
   ATL_CINT K8 = ATL_MulByNB(nKb) + (((kr+7)>>3)<<3);
   ATL_INT nlblks, nrblks;
   int i, nDb, dr;
   void **acnts;

@ROUT ATL_tgemm_rkK_Np
   sz = ATL_MulBySize(K8)*(M + NB*ATL_NTHREADS) + ATL_Cachelen +
        ATL_NTHREADS*(sizeof(TYPE*)+sizeof(int)+ATL_Cachelen);
@ROUT ATL_tgemm_rkK
   sz = ATL_MulBySize(K8)*(M + NB*ATL_NTHREADS) + ATL_Cachelen +
        ATL_NTHREADS*(sizeof(TYPE*)+2*sizeof(int)+ATL_Cachelen+2*sizeof(void*));
@ROUT ATL_tgemm_rkK_Np ATL_tgemm_rkK
   if (sz > ATL_NTHREADS*ATL_PTMAXMALLOC)
      return(1);
   Bws = malloc(sz);
   if (!Bws)
      return(2);
   chkin = (volatile int*) (Bws + ATL_NTHREADS);
@ROUT ATL_tgemm_rkK
   pd.Sync0 = 1;
   pd.Js = (int*) (chkin+ATL_NTHREADS);
   acnts = (void**) (pd.Js+ATL_NTHREADS);
   pd.Mlocks = acnts + ATL_NTHREADS;
   Aw = (TYPE*)(pd.Mlocks+ATL_NTHREADS);
@ROUT ATL_tgemm_rkK_Np
   Aw = (TYPE*) (chkin+ATL_NTHREADS);
@ROUT ATL_tgemm_rkK_Np ATL_tgemm_rkK
   Aw = ATL_AlignPtr(Aw);
   Bws[0] = Aw + K8*(M SHIFT);
   Bws[0] = ATL_AlignPtr(Bws[0]);
@ROUT ATL_tgemm_rkK `   pd.Js[0] = 0;`
   chkin[0] = 0;
   for (i=1; i < ATL_NTHREADS; i++)
   {
      Bws[i] = Bws[i-1] + K8*(NB SHIFT);
      Bws[i] = ATL_AlignPtr(Bws[i]);
      chkin[i] = 0;
@ROUT ATL_tgemm_rkK `      pd.Js[i] = 0;`
   }
   pd.chkin = chkin;
   pd.Aw = Aw;
   pd.Bws = Bws;
   pd.nMb = nDb = ATL_DivByNB(M);
   pd.mr = dr = M - ATL_MulByNB(nDb);
   nDb = (dr) ? nDb+1 : nDb;
   pd.aMcnt = ATL_SetGlobalAtomicCount(ATL_NTHREADS, nDb, 0);
@ROUT ATL_tgemm_rkK
   for (i=0; i < ATL_NTHREADS; i++)
   {
      pd.Mlocks[i] = ATL_mutex_init();
      acnts[i] = ATL_SetGlobalAtomicCount(1, 0, 0);
   }
   pd.aMcnts = acnts;
@ROUT ATL_tgemm_rkK_Np ATL_tgemm_rkK
   pd.nNb = nDb = ATL_DivByNB(N);
   pd.nr = dr = N - ATL_MulByNB(nDb);
   pd.aNcnt = ATL_SetGlobalAtomicCount(ATL_NTHREADS, dr ? nDb+1 : nDb, 0);
   pd.nKb = nKb; pd.kr = kr; 
   #ifdef TREAL
      pd.kr8 = ((kr+7)>>3)<<3;
      if (pd.kr8 > KB)
         pd.kr8 = KB;
   #else
      pd.kr8 = kr;
   #endif
   pd.A = A; pd.B = B; pd.C = C;
   pd.lda = lda; pd.ldb = ldb; pd.ldc = ldc;
   pd.M = M; pd.N = N; pd.K = K;
   pd.TA = TA; pd.TB = TB;
   pd.alpha = alpha; pd.beta = beta;

   ATL_goparallel(ATL_NTHREADS, Mjoin(PATL,DoWork_rkK@(sf)), &pd, NULL);

   ATL_FreeGlobalAtomicCount(pd.aMcnt);
   ATL_FreeGlobalAtomicCount(pd.aNcnt);
@ROUT ATL_tgemm_rkK
   for (i=0; i < ATL_NTHREADS; i++)
   {
      ATL_FreeGlobalAtomicCount(acnts[i]);
      ATL_mutex_free(pd.Mlocks[i]);
   }
@ROUT ATL_tgemm_rkK_Np ATL_tgemm_rkK
   free(Bws);
   return(0);
}
@ROUT ATL_tgemm_MKp
/*
 * This routine assumes B has already been copied to block-major, column-panel
 * format and had ALPHA applied to it, and threads should now cooperate to:
 * (a) Copy a piece of A to their cache
 * (b) Use that copied piece against all of B to update a row-panel of C
 * (c) Repeat until all rows of A have been applied
 * They determine what row panel of A to operate on using the aAcnt variable.
 */
static void ATL_tloopA
(
   void *aAcnt,         /* Atomic counter on row panels of A */
   int iam,             /* my rank; */
   int nMp,             /* how many M partitions are there? */
   int Mp,              /* size of all but last M partition */
   int mlast,           /* how many elts in last M partition? */
   int nfMblks,         /* # of full NB blocks in Mp */
   int nfNblks,         /* # of full NB blocks along N */
   int nr,              /* N%NB */
   int nfKblks,         /* # of full NB blocks along Kp */
   int kr,              /* K%NB */
   enum ATLAS_TRANS TA,
   const TYPE *A,       /* original A matrix */
   ATL_CINT lda,        /* leading dim of A */
   TYPE *pA,            /* my private workspace to copy row-panels into */
   const TYPE *B,       /* B in blk-major, column-panel format */
   TYPE *C,             /* original C matrix */
   ATL_CINT ldc         /* leading dim of C */
)
{
   ATL_CINT M = ((nMp-1)*Mp)+mlast, N = nfNblks*NB + nr, K = nfKblks*NB + kr;
   ATL_INT iblk;
   size_t i;

   while (iblk = ATL_DecGlobalAtomicCount(aAcnt, iam))
   {
      ATL_INT mb, mymr, mymblks;
      if (iblk-- == nMp)
      {
         mb = mlast;
         mymblks = (mlast/NB);
         mymr = mlast - mymblks*NB;
      }
      else
      {
         mb = Mp;
         mymr = 0;
         mymblks = nfMblks;
      }
      i = iblk*Mp;
/*
 *    Copy the specified row-panel (may be multiple MB-wide row-panels) 
 *    to my private workspace
 */
      if (TA == AtlasNoTrans)
         Mjoin(PATL,row2blkT2_a1)(mb, K, A+i, lda, pA, ATL_rone);
      else
         Mjoin(PATL,col2blk_a1)(K, mb, A+i*lda, lda, pA, ATL_rone);
      DoMM_NMK(mymblks, mymr, nfNblks, nr, nfKblks, kr, pA, B, C+i, ldc);
   }
}

void Mjoin(PATL,DoWork_MKp)(ATL_LAUNCHSTRUCT_t *lp, void *vp)
/* 
 * This routine loops over Kp-sized chunks of K
 */
{
   ATL_thread_t *tp=vp;
   ATL_TGEMM_MKP_t *pbdef=lp->opstruct;
   ATL_CINT iam = tp->rank;
   ATL_INT k, kb;
   volatile int *chkin = pbdef->chkin;
   TYPE *Bw=pbdef->Bw, *Aw=pbdef->Aws[iam];
   void *aAcnt=pbdef->aAcnt, *aBcnt=pbdef->aBcnt;
   ATL_CINT BNOTRANS = (pbdef->TB == AtlasNoTrans);
   ATL_CINT ANOTRANS = (pbdef->TA == AtlasNoTrans);
   const TYPE *A=pbdef->A, *B=pbdef->B;
   TYPE *C = pbdef->C;
   ATL_CINT M = pbdef->M, N = pbdef->N, K = pbdef->K, klast = pbdef->klast;
   ATL_CINT nNb = pbdef->nNb,  Kp = pbdef->Kp;
   ATL_CINT lda = pbdef->lda, ldb = pbdef->ldb, ldc = pbdef->ldc;
   ATL_CINT nfMblks = pbdef->Mp/NB, nfNblks = N/NB;
   ATL_CINT nr = pbdef->N-nfNblks*NB; 
   ATL_CINT nMp=pbdef->nMp, Mp=pbdef->Mp, mlast=pbdef->mlast;
   ATL_CINT nfKblks0 = Kp / NB;
   ATL_INT nfKblks, kr, i;

/*
 * Threads cooperate to scale C if beta is not one; the unit of work is 1 col
 * After this block, we can always use BETA=1 for all GEMM calls
 */
   if (SCALAR_IS_ZERO(pbdef->beta))
   {
      register int j;
      void *aCcnt = pbdef->aCcnt;
      while (j = ATL_DecGlobalAtomicCount(aCcnt, iam))
      {
         j--;
         Mjoin(PATL,zero)(M, C+ldc*j, 1);
      }
   }
   else if (pbdef->aCcnt)   /* cheap test for BETA != 1.0 */
   {
      register int j;
      const SCALAR beta = pbdef->beta;
      void *aCcnt = pbdef->aCcnt;

      while (j = ATL_DecGlobalAtomicCount(aCcnt, iam))
      {
         j--;
         Mjoin(PATL,scal)(M, beta, C+ldc*j, 1);
      }
   }
/*
 * Loop over partitions of K that have been chosen to fit operands in the
 * cache so that A can be reused across all of B;  go from last to first.
 * This will result in CEIL(K/Kp) writes of C
 */
   kb = klast;
   nfKblks = kb / NB;
   kr = kb - nfKblks * NB;
   if (K > klast)
      ATL_assert(nfKblks0*NB == Kp);
   k = K-klast;
   B += (BNOTRANS) ?  k : k*ldb;
   A += (ANOTRANS) ? k*lda : k;

   for (; k >= 0; k -= Kp)
   {
      int jblk;
/*
 *    If we need row-major access on B, measly parallel speedup not worth
 *    doing row-access, so first node to arrive here just does the entire copy
 */
      if (!BNOTRANS)  /* transpose case */
      {
         if (ATL_DecGlobalAtomicCount(pbdef->aBcnt, iam) == nNb)
         {
            const SCALAR alpha = pbdef->alpha;
            if (SCALAR_IS_ONE(alpha))
               Mjoin(PATL,row2blkT2_a1)(N, kb, B, ldb, Bw, alpha);
            else
               Mjoin(PATL,row2blkT2_aX)(N, kb, B, ldb, Bw, alpha);
         }
         B -= Kp*ldb;
      }
/*
 *    Copy nNb column panels of B in parallel using the atomic counter aBcnt
 */
      else  /* No-Tranpose case */
      {
         const SCALAR alpha = pbdef->alpha;
         MAT2BLK B2blk = (SCALAR_IS_ONE(alpha)) ? 
                         Mjoin(PATL,col2blk_a1) : Mjoin(PATL,col2blk_aX);

         while(jblk = ATL_DecGlobalAtomicCount(pbdef->aBcnt, iam))
         {
            ATL_INT nn, j;
            jblk--;
            j = jblk * NB;
            nn = N - j;
            nn = Mmin(nn, NB);
            B2blk(kb, nn, B+j*ldb, ldb, Bw+(size_t)jblk*kb*NB, alpha);
         }
         B -= Kp;
      } 
/*
 *    Node P-1 makes sure everyone has finished, then he sets our counters
 *    (safe since everyone else waiting below) before signaling his completion
 */
      if (iam == ATL_NTHREADS-1)
      {
         const int mycnt = chkin[iam];
         for (i=0; i < ATL_NTHREADS-1; i++)
            while(chkin[i] <= mycnt);
         ATL_ResetGlobalAtomicCount(pbdef->aBcnt, nNb, 0);/* for next k-it */
         ATL_ResetGlobalAtomicCount(pbdef->aAcnt, nMp, 0);/* starting A again */
         chkin[iam]++;
      }
      else
      {
         const int mycnt = chkin[iam];
         chkin[iam]++;
         for (i=0; i < ATL_NTHREADS; i++)
            while(chkin[i] <= mycnt);
      }
      ATL_tloopA(aAcnt, iam, nMp, Mp, mlast, nfMblks, nfNblks, nr, 
                 nfKblks, kr, pbdef->TA, A, lda, Aw, Bw, C, ldc);
      A -= (ANOTRANS) ? Kp*lda : Kp;
      kb = Kp;
      nfKblks = nfKblks0;
      kr = 0;
      if (k)  /* must sync before copying B at top of loop */
      {
         const int mycnt = chkin[iam];
         chkin[iam]++;
         for (i=0; i < ATL_NTHREADS; i++)
            while(chkin[i] <= mycnt);
      }
   }  
@beginskip
   if (0)
   {
         const int mycnt = chkin[iam];
         chkin[iam]++;
         for (i=0; i < ATL_NTHREADS; i++)
            while(chkin[i] <= mycnt);
   }
@endskip
}

int Mjoin(PATL,tgemm_MKp)
   (const enum ATLAS_TRANS TA, const enum ATLAS_TRANS TB, ATL_CINT M, 
    ATL_CINT N, ATL_CINT K, const SCALAR alpha, const TYPE *A, ATL_CINT lda, 
    const TYPE *B, ATL_CINT ldb, const SCALAR beta, TYPE *C, ATL_CINT ldc)
{
   TYPE *Bw, *p;
   int *chkin;
   void *amcnt, *vp;
   TYPE **Aws;
   size_t wrksz;
   ATL_INT Kp, Mp, i, k, m, klast, mlast, nKp, nMp, nNb;
   ATL_TGEMM_MKP_t pbdef;

@beginskip
   if (0)
   {
      TYPE *pB, *pA;
      void *vp;

      vp = malloc(M*K*sizeof(TYPE) + N*K*sizeof(TYPE) + 2*ATL_Cachelen);
      ATL_assert(vp);
      pB = ATL_AlignPtr(vp);
      pA = pB + N*K;
      pA = ATL_AlignPtr(pA);
      if (TB == AtlasNoTrans)
      {
         if (SCALAR_IS_ONE(alpha))
            Mjoin(PATL,col2blk_a1)(K, N, B, ldb, pB, alpha);
         else
            Mjoin(PATL,col2blk_aX)(K, N, B, ldb, pB, alpha);
      }
      else if (SCALAR_IS_ONE(alpha))
         Mjoin(PATL,row2blkT2_a1)(N, K, B, ldb, pB, alpha);
      else
         Mjoin(PATL,row2blkT2_aX)(N, K, B, ldb, pB, alpha);
      if (TA == AtlasNoTrans)
         Mjoin(PATL,row2blkT2_a1)(M, K, A, lda, pA, ATL_rone);
      else
         Mjoin(PATL,col2blk_a1)(K, M, A, lda, pA, ATL_rone);
      if (!SCALAR_IS_ONE(beta))
         Mjoin(PATL,gescal)(M, N, beta, C, ldc);
      DoMM_NMK(M/NB, M-(M/NB)*NB, N/NB, N-(N/NB)*NB, K/KB, K-(K/KB)*KB,
               pA, pB, C, ldc);
      free(vp);
      return(0);
   }
@endskip
/*
 * =====================================================================
 * Compute the Kp and Mp partitionings.  We want all our operands to fit
 * in cache, assuming algorithmic movement and LRU replacement, so:
 *    Mp*Kp + 2*(NB*Kp + Mp*NB) = CE
 * =====================================================================
 */
/*
 * See if we must cut K in order to fit in CE; if so, Mp=MB=NB, and:
 *    MB*Kp + 2*(NB*Kp + MB*NB) = CE  -> Kp = (CE - MB*NB)/(MB + 2*NB)
 *      A          B       C
 */
   if (MB*K + 2*NB*K + MB*NB > CacheEdge)  /* Mp=MB; solve for Kp */
   {
      Kp = (CacheEdge - MB*NB)/(MB + 2*NB);
      Kp = (Kp > KB) ? (Kp/KB)*KB : KB;
      Mp = MB;
   }
/*
 * Otherwise, Kp = K, and we solve for Mp
 *    Mp*K + 2*(NB*K + Mp*NB) = CE  -> Mp = (CE - 2*NB*K) / (K+NB)
 *     A         B       C
 */
   else  /* Kp = K, solve for Mp */
   {
      Mp = (CacheEdge - 2*NB*K) / (K+NB);
      if (Mp > MB)
      {
         while (M/Mp < 3*ATL_NTHREADS)
            Mp >>= 1;
         Mp = (Mp > MB) ? (Mp/MB)*MB : MB;
      }
      else
         Mp = MB;
      Kp = K;
   }

/*
 * Find size of partial block at end.  If it is <= NB, absorb it into previous
 * block and have no partial block for both M & K
 */
   nKp = K / Kp;          /* floor(K/Kp) */
   klast = K - nKp*Kp;
   if (klast)
   {
      if (klast < NB)
         klast += Kp;
      else
         nKp++;         /* nKp now includes klast */
   }
   else
      klast = Kp;  /* K is even multiple of Kp */
   nMp = M / Mp;        /* floor (K/Kp) */
   mlast = M - Mp*nMp;
   if (mlast)
   {
      if (mlast < MB)
         mlast += Mp;   /* absorb partial block into last part */
      else
         nMp++;         /* nMp now includes all partitions */
   }
   else
      mlast = Mp;    /* M even multiple of Mp */
   nNb = (N+NB-1)/NB;  /* ceil(N/NB) */
/* 
 * Worksize: let k = MAX(Kp,CEIL(klast/NB)*NB), then:
 * All threads share the N*k B workspace, which is split into
 * nNb column panels; we round the B workspace up as if N were a multiple of NB
 * Then all threads need a private k*NB piece of A, and finally we need
 * nthr-len integer checkin array and a nthr-len array of pointers, which
 * we use to pass all the A work pointers to the worker nodes.
 */
   k = Mmax(Kp, klast);
   m = Mmax(Mp, mlast);
   wrksz = (2+ATL_NTHR)*ATL_Cachelen + ATL_MulBySize(NB)*nNb*k + 
           ATL_NTHR*(sizeof(int)+sizeof(TYPE*) + ATL_MulBySize(m)*k);
   if (ATL_NTHR*ATL_PTMAXMALLOC < wrksz)
      return(1);
   vp = malloc(wrksz);
   if (!vp)
      return(2);
   Aws = vp;                            /* A work ptrs first array */
   chkin = (int*)(Aws+ATL_NTHR);        /* then checkin array */
   Bw = (TYPE*)(chkin+ATL_NTHR);        /* then workspace for B */
   Bw = ATL_AlignPtr(Bw);               /* B must be aligned */
   chkin[0] = 0;
   p = Bw + k*N;                        /* first A wrkspc after B wrkspc */
   Aws[0] = ATL_AlignPtr(p);            /* A wrkspcs must be aligned */
   for (i=1; i < ATL_NTHR; i++)         /* init rest of nthr-len arrays */
   {
      chkin[i] = 0;
      p = Aws[i-1] + m*k;
      Aws[i] = ATL_AlignPtr(p);
   }
   pbdef.chkin = chkin;
   pbdef.alpha = alpha; pbdef.beta = beta;
   pbdef.A = A; pbdef.B = B; pbdef.C = C;
   pbdef.M = M; pbdef.N = N; pbdef.K = K;
   pbdef.lda = lda; pbdef.ldb = ldb; pbdef.ldc = ldc;
   pbdef.Kp = Kp; pbdef.klast = klast; pbdef.nKp = nKp;
   pbdef.Mp = Mp; pbdef.mlast = mlast; pbdef.nMp = nMp;
   pbdef.nNb = nNb;
   pbdef.TA = TA; pbdef.TB = TB;
   pbdef.Bw = Bw; pbdef.Aws = Aws;
   pbdef.aBcnt = ATL_SetGlobalAtomicCount(ATL_NTHREADS, nNb, 0);
   pbdef.aAcnt = ATL_SetGlobalAtomicCount(ATL_NTHREADS, nMp, 0);
   if (SCALAR_IS_ONE(beta))
      pbdef.aCcnt = NULL;
   else
      pbdef.aCcnt = ATL_SetGlobalAtomicCount(ATL_NTHREADS, N, 0);
//   printf("Mp=%d, nMp=%d, mlast=%d, Kp=%d, nKp=%d, klast=%d\n", 
//          Mp, nMp, mlast, Kp, nKp, klast);
   ATL_goparallel(ATL_NTHREADS, Mjoin(PATL,DoWork_MKp), &pbdef, NULL);
   free(vp);
   return(0);
}
@ROUT ATL_tgemm2
#include "atlas_misc.h"
#include "atlas_lvl3.h"
#include "atlas_threads.h"
void Mjoin(PATL,tgemm2)
   (const enum ATLAS_TRANS TA, const enum ATLAS_TRANS TB, ATL_CINT M, 
    ATL_CINT N, ATL_CINT K, const SCALAR alpha, const TYPE *A, ATL_CINT lda, 
    const TYPE *B, ATL_CINT ldb, const SCALAR beta, TYPE *C, ATL_CINT ldc)
{
/*
 * See if it looks like a rank-K problem that can be split on N */
 */
   if (K <= 4*NB && N > ATL_NTHREADS*NB && M < N+N) /* rank-K split on N */
   {
      if (!Mjoin(PATL,tgemm_Np)(TA, TB, M, N, K, alpha, A, lda, B, ldb, 
                                beta, C, ldc))
         return;
   }
/*
 * Is it a problem that can really only be split on K?
 */
   if ( (K > 4*ATL_NTHREADS*NB && M < 2*NB && N < 2*NB) || 
        (K >= 4*M && K >= 4*N && M*(size_t)N*K*sizeof(TYPE) < ATL_PTMAXMALLOC) )
   {
      if (!Mjoin(PATL,tgemm_Kp)(TA, TB, M, N, K, alpha, A, lda, B, ldb, 
                                beta, C, ldc))
         return;
   }
/*
 * Is it a problem that can easily be split solely on M
 */
   if (M > N && M > 4*ATL_NTHREADS*NB)
   {
      if (!Mjoin(PATL,tgemm_Mp)(TA, TB, M, N, K, alpha, A, lda, B, ldb, 
                                beta, C, ldc))
         return;
   }
/*
 * Is it a problem that can easily be split solely on N
 */
   if (N > M && N > 4*ATL_NTHREADS*NB)
   {
      if (!Mjoin(PATL,tgemm_Np)(TA, TB, M, N, K, alpha, A, lda, B, ldb, 
                                beta, C, ldc))
         return;
   }
/*
 * For low-rank problems, use 3-d distribution with minimal copying.  This
 * should be a specialized routine where we know we can copy the entire
 * problem up front; no-copy routs should be handled by 1-D distros
 */
/*
 * Large-rank problems using 3-D with work stealing.
 */

/*
 * Otherwise, call serial gemm; this will later be replaced by _MpNp, which
 * is not allowed to fail (return non-zero).  Can we use work-stealing?
 */
   Mjoin(PATL,gemm)(TA, TB, M, N, K, alpha, A, lda, B, ldb, beta, C, ldc);
}
@ROUT ATL_tgemm_Kp
   @define D @K@
   @define I @k@
@ROUT ATL_tgemm_Np
   @define D @N@
   @define I @j@
@ROUT ATL_tgemm_Mp
   @define D @M@
   @define I @i@
@ROUT ATL_tgemm_Mp ATL_tgemm_Np ATL_tgemm_Kp
@define d @@low@(D)@
#include "atlas_misc.h"
#include "atlas_lvl3.h"
#include "atlas_threads.h"
typedef struct
{
@ROUT ATL_tgemm_Kp
   TYPE **Cws;            /* P-len array of C work areas, Cws[0] = C */
   void *chklck;          /* 0/1 ctr serves as lock for chkin array */
   volatile int *chkin;   /* P-len checkin array, init to 0 */
   ATL_INT ldw;           /* leading dim of workspaces */
   void *lockC;           /* lock around C combine data structs */
@ROUT ATL_tgemm_Mp ATL_tgemm_Np ATL_tgemm_Kp
   void *a@(D)cnt;           /* which @(D) blk are we doing? */
   const TYPE *A, *B;     /* original input matrices */
   TYPE *C;               /* original output matrix */
   SCALAR alpha;
   SCALAR beta;
   ATL_INT @(D)p, n@(D)p, @(d)last;
   ATL_INT M, N, K, lda, ldb, ldc;
   enum ATLAS_TRANS TA, TB;
} ATL_TGEMM_@(D)PART_t;

@ROUT ATL_tgemm_Kp
static void DoCombine
(
   int iam,               /* rank of this thread */
   ATL_INT M,             /* # of rows in C */
   ATL_INT N,             /* # of cols in C */
   ATL_INT ldc,           /* leading dim of C (>=M) */
   ATL_INT ldw,           /* leading dim of all workspaces */
   volatile int *chkin,   /* check-in array initialized to 0 */
   void *lockC,           /* lock protecting the chkin array */
   TYPE **Cws             /* array of C workspaces to combine */
)
{
   #ifdef TCPLX
      TYPE one[2] = {ATL_rone, ATL_rzero};
   #else
      #define one ATL_rone
   #endif
   int i, combrank;
   ATL_CINT myldc = (iam) ? ldw : ldc;
/*
 * See if there are any C arrays I can combine with mine
 */
   do
   {
      combrank = -1;
      for (i=0; i < ATL_NTHREADS && chkin[i] != 1; i++);
/*
 *    If there are no arrays available, lock the structure and signal that
 *    I have left the building by storing a 1 in my structure
 */
      if (i == ATL_NTHREADS)
      {
         if (iam == 0)  /* rank=zero loops until all matrices are combined */
         {
             for (i=1; i < ATL_NTHREADS && chkin[i] == 3; i++);
             if (i != ATL_NTHREADS)
                continue;
             else
                return;
         }
/*
 *       non-zero nodes simply leave if there are no matrices to combine
 */
         ATL_mutex_lock(lockC);
         for (i=0; i < ATL_NTHREADS && chkin[i] != 1; i++);
         if (i == ATL_NTHREADS)
         {
            chkin[iam] = 1;   /* signal that my array is available to combine */
            ATL_mutex_unlock(lockC);
            return;
         }
         else
         {
            combrank = i;
            chkin[i] = 2;
         }
         ATL_mutex_unlock(lockC);
      }
/*
 *    I found an uncombined array, see if I can take it
 */
      else
      {
         ATL_mutex_lock(lockC);
         if (chkin[i] != 1)  /* if that one is gone, see if another is there */
         {
            for (i=0; i < ATL_NTHREADS && chkin[i] != 1; i++);
         }
         if (i < ATL_NTHREADS)
         {
            combrank = i;
            chkin[i] = 2;
         }
         else if (iam)
         {
            chkin[iam] = 1;   /* signal that my array is available to comb */
            ATL_mutex_unlock(lockC);
            return;
         }
         else   /* nothing to combine & I'm node zero */
         {
            ATL_mutex_unlock(lockC);
            continue; /* nothing more to do until someone checks in */
         }
      }
/*
 *    If I get to here, then combrank should be set, and I must do the combine
 */
      Mjoin(ATL,geadd)(M, N, one, Cws[combrank], ldw, one, Cws[iam], myldc);
      chkin[combrank] = 3;  /* safe since I can only do this after lockC */
   }                        /* and nobody writes to loc with 2 in it but me */
   while(1);
}
#ifndef TCPLX
   #undef one
#endif


@ROUT ATL_tgemm_Mp ATL_tgemm_Np ATL_tgemm_Kp
void Mjoin(PATL,DoWork_part@(D))(ATL_LAUNCHSTRUCT_t *lp, void *vp)
{
   ATL_thread_t *tp=vp;
   ATL_TGEMM_@(D)PART_t *pd=lp->vp;
   void *a@(D)cnt = pd->a@(D)cnt;
   const TYPE *A=pd->A, *B=pd->B;
   ATL_CINT iam = tp->rank, ANOTRANS = (pd->TA == AtlasNoTrans);
   ATL_CINT M=pd->M, N=pd->N, K=pd->K;
   ATL_CINT n@(D)p = pd->n@(D)p, @(D)p = pd->@(D)p, @(d)last=pd->@(d)last;
   ATL_CINT lda=pd->lda,  ldb=pd->ldb;
   const enum ATLAS_TRANS TA=pd->TA, TB=pd->TB;
   const SCALAR alpha=pd->alpha; 
@ROUT ATL_tgemm_Mp ATL_tgemm_Np
   TYPE *C=pd->C;
   ATL_CINT ldc = pd->ldc;
   const SCALAR beta=pd->beta;
@ROUT ATL_tgemm_Kp
   ATL_CINT ldc = (iam) ? pd->ldw : pd->ldc;
   TYPE *C = pd->Cws[iam];
   #ifdef TCPLX
      const TYPE one[2] = {ATL_rone, ATL_rzero};
      const TYPE zero[2] = {ATL_rzero, ATL_rzero};
      const TYPE *beta= (iam) ? zero : pd->beta;
   #else
      TYPE beta = (iam) ? ATL_rzero : pd->beta;
   #endif
   volatile int *chkin=pd->chkin;
   ATL_INT kblk;
   ATL_CINT amul = ((TA == AtlasNoTrans || TA == AtlasConj) ? lda : 1)SHIFT;
   ATL_CINT bmul = ((TB == AtlasNoTrans || TB == AtlasConj) ? 1 : ldb)SHIFT;

   while (kblk = ATL_DecAtomicCount(a@(D)cnt))
   {
      const TYPE *a;
      size_t ka, kb;
      ATL_INT i, k;

      if (kblk-- == nKp)
      {
         k = klast;
         i = pd->K - k;
      }
      else
      {
         i = kblk*Kp;
         k = Kp;
      }
      ka = i*amul;
      kb = i*bmul;
      Mjoin(PATL,gemm)(TA, TB, M, N, k, alpha, A+ka, lda, B+ka, ldb, beta, 
                       C, ldc);
      #ifdef TCPLX
         beta = one;
      #else
         beta = ATL_rone;
      #endif
   }
   DoCombine(iam, M, N, pd->ldc, pd->ldw, pd->chkin, pd->lockC, pd->Cws);
@ROUT ATL_tgemm_Np
   ATL_CINT bmul = (TB == AtlasNoTrans) ? (ldb SHIFT) : (1 SHIFT);
   ATL_INT jblk;

   while (jblk = ATL_DecAtomicCount(aNcnt))
   {
      ATL_INT n;
      size_t jb, jc;

      if (jblk-- == nNp)
      {
         n = nlast;
         jc = pd->N - n;
      }
      else
      {
         jc = jblk*Np;
         n = Np;
      }
      jb = jc*bmul;
      jc *= (ldc SHIFT);
      Mjoin(PATL,gemm)(TA, TB, M, n, K, alpha, A, lda, B+jb, ldb, beta, 
                       C+jc, ldc);
   }
@ROUT ATL_tgemm_Mp
   ATL_INT iblk;

   while (iblk = ATL_DecAtomicCount(a@(D)cnt))
   {
      const TYPE *a;
      ATL_INT i, m;

      if (iblk-- == nMp)
      {
         m = mlast;
         i = pd->M - m;
      }
      else
      {
         i = iblk*Mp;
         m = Mp;
      }
      a = (ANOTRANS) ? A+(i SHIFT) : A+i*(lda SHIFT);
      Mjoin(PATL,gemm)(TA, TB, m, N, K, alpha, a, lda, B, ldb, beta, 
                       C+(i SHIFT), ldc);
   }
@ROUT ATL_tgemm_Mp ATL_tgemm_Np ATL_tgemm_Kp
}

int Mjoin(PATL,tgemm_@(D)p)
   (const enum ATLAS_TRANS TA, const enum ATLAS_TRANS TB, ATL_CINT M,
    ATL_CINT N, ATL_CINT K, const SCALAR alpha, const TYPE *A, ATL_CINT lda,
    const TYPE *B, ATL_CINT ldb, const SCALAR beta, TYPE *C, ATL_CINT ldc)
/*
 * This routine divides @(D) into roughly 4*P chunks, and calls serial GEMM.
 */
{
   ATL_INT n@(D)p, @(D)p, @(d)last;
   ATL_TGEMM_@(D)PART_t pd;  /* problem definition */
@ROUT ATL_tgemm_Kp 
   ATL_INT ldw, i;
   size_t totsz;
   void *vp=NULL;
@ROUT ATL_tgemm_Mp ATL_tgemm_Np ATL_tgemm_Kp

   @(D)p = @(D) / (2*ATL_NTHREADS);
   if (@(D)p < NB)
      return(0);
   @(D)p = (@(D)p >= NB+NB) ? @(D)p>>1 : @(D)p;
   n@(D)p = @(D) / @(D)p;
   @(d)last = @(D) - n@(D)p*@(D)p;
   if (@(d)last)
   {
      if (@(d)last < @(D)B)
         @(d)last += @(D)p;   /* absorb partial block into last full partition */
      else
         n@(D)p++;
   }
   else
      @(d)last = @(D)p;       /* last part same size as all parts */
   pd.M = M; pd.N = N; pd.K = K;
   pd.lda = lda; pd.ldb = ldb; pd.ldc = ldc;
   pd.TA = TA; pd.TB = TB;
   #ifdef TCPLX
      pd.alpha = (TYPE*)alpha; pd.beta = (TYPE*)beta;
   #else
      pd.alpha = alpha; pd.beta = beta;
   #endif
   pd.A = A; pd.B = B; pd.C = C;
   pd.@(D)p = @(D)p; pd.n@(D)p = n@(D)p; pd.@(d)last = @(d)last;
   pd.a@(D)cnt = ATL_SetAtomicCount(n@(D)p);
@ROUT ATL_tgemm_Kp
  
   ldw = ((M+7)>>8)<<8;   /* make ldw a multiple of 8 */
   if (!(ldw & (ldw-1)))  /* if ldw is a power of 2 */
      ldw += 8;           /* change it so it is not */
   totsz = (ATL_MulBySize(ldw)*N+ATL_Cachelen) * (ATL_NTHREADS-1) + 
           ATL_NTHREADS*sizeof(TYPE*);
   if (totsz > ATL_PTMAXMALLOC*ATL_NTHREADS)
      return(1);
   pd.Cws = malloc(totsz);
   pd.Cws[0] = C;
   pd.ldw = ldw;
   pd.Cws[1] = (TYPE*) (pd.Cws + ATL_NTHREADS);
   pd.Cws[1] = ATL_AlignPtr(pd.Cws[1]);
   pd.chkin[0] = pd.chkin[1] = 0;  /* at least 2 threads, so safe */
   for (i=2; i < ATL_NTHREADS; i++)
   {
      void *vp;
      const size_t inc=ldw*N;
      vp = pd.Cws[i-1] + inc;
      pd.Cws[i] = ATL_AlignPtr(vp);
      pd.chkin[i] = 0;
   }
@ROUT ATL_tgemm_Mp ATL_tgemm_Np ATL_tgemm_Kp
   ATL_goparallel(ATL_NTHREADS, Mjoin(PATL,DoWork_part@(D)), &pd, NULL);
   return(0);
}
@ROUT ATL_tgemm_MpNpKp
int Mjoin(PATL,tgemm_Mp)
   (const enum ATLAS_TRANS TA, const enum ATLAS_TRANS TB, ATL_CINT M,
    ATL_CINT N, ATL_CINT K, const SCALAR alpha, const TYPE *A, ATL_CINT lda,
    const TYPE *B, ATL_CINT ldb, const SCALAR beta, TYPE *C, ATL_CINT ldc)
{
   ATL_INT nmblks, nnblks, nkblks, nblks, m, n, k;
   int cutKOK;

   nmblks = nnblks = nkblks = 1;
   m = M; n = N; k = K;
   do
   {
/*
 *    cost of operation : (2*M*N*K)/P
 *    worst case combine: (P*M*N)
 *    If this ratio is large, then we can afford to cut K
 */
      cutKOK = (k+k)/((nkblks+1)*(nkblks+1)) >= 200;
      if (cutKOK && k > m && k > n)
      {
         nkblks++;
         k = K / nkblks;
         if (k < KB)
         {
            nkblks--;
            break;
         }
      }
      else if (n > m)
      {
         nnblks++;
         n = N / nnblks;
         if (n < NB)
         {
            nnblks--;
            break;
         }
      }
      else /* M largest dim, cut it */
      {
         nmblks++;
         m = M / nmblks;
         if (m < MB)
         {
            nmblks--;
            break;
         }
      }
      nblks = nmblks * nnblks * nkblks;
   }
   while (nblks < 2*ATL_NTHREADS);
}
@ROUT ATL_tgemm_rKp
typedef struct
{
   void **aNcnts;         /* P counters on col-panels of C */
   void *aMcnt;           /* count row-panels of A */
   void *donecnt;         /* Counter used to detect if op is complete */
   volatile int *chkin;   /* ATL_NTHREAD-len checkin array */
   volatile int Riam;     /* 1st arriver on cooperative operation */
   volatile int DONE;     /* set by Riam when cooperative op is complete */
   TYPE **Bws;            /* preallocated thread copy areas */
   TYPE *Aw;              /* workspace for common A */
   const TYPE *A, *B;     /* original input matrices */
   TYPE *C;               /* original output matrix */
   SCALAR alpha;
   SCALAR beta;
   ATL_INT Kp, nKp, klast;
   ATL_INT nMb, mr, nNb, nr, nlblks, nrblks;
   ATL_INT M, N, K, lda, ldb, ldc;
   enum ATLAS_TRANS TA, TB;
} ATL_TGEMM_RKP_t;

void Mjoin(PATL,DoWork_NKp)(ATL_LAUNCHSTRUCT_t *lp, void *vp)
/*
 * This routine loops over Kp-sized chunks of K
 */
{
   ATL_thread_t *tp=vp;
   ATL_TGEMM_RKP_t *pbdef=lp->vp;
   ATL_CINT iam = tp->rank;
   ATL_INT k, kb;
   volatile int *chkin = pbdef->chkin;
   TYPE *Aw=pbdef->Bw, *Bw=pbdef->Bws[iam];
   void *aAcnt=pbdef->aAcnt, *aBcnt=pbdef->aBcnt;
   ATL_CINT BNOTRANS = (pbdef->TB == AtlasNoTrans);
   ATL_CINT ANOTRANS = (pbdef->TA == AtlasNoTrans);
   const TYPE *A=pbdef->A, *B=pbdef->B;
   TYPE *C = pbdef->C;
   ATL_CINT M = pbdef->M, N = pbdef->N, K = pbdef->K, klast = pbdef->klast;
   ATL_CINT nNb = pbdef->nNb,  Kp = pbdef->Kp;
   ATL_CINT lda = pbdef->lda, ldb = pbdef->ldb, ldc = pbdef->ldc;
   ATL_CINT nfMblks = pbdef->nMb, nfNblks = nNb;
   ATL_CINT nr = pbdef->nr;
   ATL_CINT nfKblks0 = Kp / NB;
   ATL_INT nfKblks, kr, i;

/*
 * Threads cooperate to scale C if beta is not one; the unit of work is 1 col
 * After this block, we can always use BETA=1 for all GEMM calls
 */
   if (SCALAR_IS_ZERO(pbdef->beta))
   {
      register int j;
      int Ciam = iam+1;
      void *aCcnt = pbdef->aCcnt;

      j = ATL_DecAtomicCount(aCcnt);
      if (j == N)
      {
         pbdef.Riam  = iam;  /* I'm responsible for certifying C is finished */
      }
      if (j)
      {
         Mjoin(PATL,zero)(M, C+ldc*(j-1), 1);
         while (j = ATL_DecAtomicCount(aCcnt))
            Mjoin(PATL,zero)(M, C+ldc*(j-1), 1);
      }
      if (iam == pbdef.Riam)
      {
         pbdef.Riam = -1;
      }
      else
         while (!pbdef->DONE);
   }
   else if (pbdef->aCcnt)   /* cheap test for BETA != 1.0 */
   {
      register int j;
      const SCALAR beta = pbdef->beta;
      void *aCcnt = pbdef->aCcnt;

      while (j = ATL_DecAtomicCount(aCcnt))
      {
         j--;
         Mjoin(PATL,scal)(M, beta, C+ldc*j, 1);
      }
   }
/*
 * Loop over partitions of K that have been chosen to fit operands in the
 * cache so that A can be reused across all of B;  go from last to first.
 * This will result in CEIL(K/Kp) writes of C
 */
   kb = klast;
   nfKblks = kb / NB;
   kr = kb - nfKblks * NB;
   if (K > klast)
      ATL_assert(nfKblks0*NB == Kp);
   k = K-klast;
   B += (BNOTRANS) ?  k : k*ldb;
   A += (ANOTRANS) ? k*lda : k;

   for (; k >= 0; k -= Kp)
   {
      int jblk;
/*
 *    If we need row-major access on B, measly parallel speedup not worth
 *    doing row-access, so first node to arrive here just does the entire copy
 */
      if (!BNOTRANS)  /* transpose case */
      {
         if (ATL_DecAtomicCount(pbdef->aBcnt) == nNb)
         {
            const SCALAR alpha = pbdef->alpha;
            if (SCALAR_IS_ONE(alpha))
               Mjoin(PATL,row2blkT2_a1)(N, kb, B, ldb, Bw, alpha);
            else
               Mjoin(PATL,row2blkT2_aX)(N, kb, B, ldb, Bw, alpha);
         }
         B -= Kp*ldb;
      }
/*
 *    Copy nNb column panels of B in parallel using the atomic counter aBcnt
 */
      else  /* No-Tranpose case */
      {
         const SCALAR alpha = pbdef->alpha;
         MAT2BLK B2blk = (SCALAR_IS_ONE(alpha)) ?
                         Mjoin(PATL,col2blk_a1) : Mjoin(PATL,col2blk_aX);

         while(jblk = ATL_DecAtomicCount(pbdef->aBcnt))
         {
            ATL_INT nn, j;
            jblk--;
            j = jblk * NB;
            nn = N - j;
            nn = Mmin(nn, NB);
            B2blk(kb, nn, B+j*ldb, ldb, Bw+(size_t)jblk*kb*NB, alpha);
         }
         B -= Kp;
      }
/*
 *    Node P-1 makes sure everyone has finished, then he sets our counters
 *    (safe since everyone else waiting below) before signaling his completion
 */
      if (iam == ATL_NTHREADS-1)
      {
         const int mycnt = chkin[iam];
         for (i=0; i < ATL_NTHREADS-1; i++)
            while(chkin[i] <= mycnt);
         ATL_ResetAtomicCount(pbdef->aBcnt, nNb);  /* reset for next k-it */
         ATL_ResetAtomicCount(pbdef->aAcnt, nMp);  /* starting A again */
         chkin[iam]++;
      }
      else
      {
         const int mycnt = chkin[iam];
         chkin[iam]++;
         for (i=0; i < ATL_NTHREADS; i++)
            while(chkin[i] <= mycnt);
      }
      ATL_tloopA(aAcnt, iam, nMp, Mp, mlast, nfMblks, nfNblks, nr,
                 nfKblks, kr, pbdef->TA, A, lda, Aw, Bw, C, ldc);
      A -= (ANOTRANS) ? Kp*lda : Kp;
      kb = Kp;
      nfKblks = nfKblks0;
      kr = 0;
      if (k)  /* must sync before copying B at top of loop */
      {
         const int mycnt = chkin[iam];
         chkin[iam]++;
         for (i=0; i < ATL_NTHREADS; i++)
            while(chkin[i] <= mycnt);
      }
   }
}
int Mjoin(PATL,tgemm_rKp)
   (const enum ATLAS_TRANS TA, const enum ATLAS_TRANS TB, ATL_CINT M,
    ATL_CINT N, ATL_CINT K, const SCALAR alpha, const TYPE *A, ATL_CINT lda,
    const TYPE *B, ATL_CINT ldb, const SCALAR beta, TYPE *C, ATL_CINT ldc)
{
   TYPE *Aw, *p;
   void **acnts;
   void *amcnt, *vp;
   TYPE **Bws;
   size_t wrksz;
   ATL_INT Kp, Mp, i, k, m, klast, mlast, nKp, nMp, nNb;
   ATL_TGEMM_RKP_t pbdef;

/*
 * =====================================================================
 * Compute the Kp partitioning.  We want all our operands to fit
 * in cache, assuming algorithmic movement and LRU replacement, so:
 *    M*Kp + 2*(NB*Kp + M*NB) = CE
 * =====================================================================
 */
/*
 * See if we must cut K in order to fit in CE; if so, Mp=MB=NB, and:
 *    MB*Kp + 2*(NB*Kp + MB*NB) = CE  -> Kp = (CE - MB*NB)/(MB + 2*NB)
 *      A          B       C
 */
   if (MB*K + 2*NB*K + MB*NB > CacheEdge)  /* Mp=MB; solve for Kp */
   {
      Kp = (CacheEdge - MB*NB)/(MB + 2*NB);
      Kp = (Kp > KB) ? (Kp/KB)*KB : KB;
   }
   else  /* otherwise, its a rank-K update, call special routine */
      return(Mjoin(PATL,tgemm_rkK_Np)(TA, TB, M, N, K, alpha, A, lda, B, ldb, 
                                      beta, C, ldc));
/*
 * Find size of partial block at end.  If it is <= NB, absorb it into previous
 * block and have no partial block 
 */
   nKp = K / Kp;        /* floor(K/Kp) */
   klast = K - nKp*Kp;
   if (klast)
   {
      if (klast < NB)
         klast += Kp;   /* absorb partial block into last K block */
      else
         nKp++;         /* nKp now includes klast, which is > NB */
   }
   else
      klast = Kp;       /* K is even multiple of Kp */
   nMb = M/NB;          /* floor(M/NB) */
   nNb = N/NB;          /* floor(N/NB) */
   if (nNb < ATL_NTHREADS*2)
      return(3);
/*
 * Worksize: let k = MAX(Kp,CEIL(klast/NB)*NB), then:
 * All threads share the N*k B workspace, which is split into
 * nNb column panels; we round the B workspace up as if N were a multiple of NB
 * Then all threads need a private k*NB piece of A, and finally we need
 * nthr-len integer checkin array and a nthr-len array of pointers, which
 * we use to pass all the A work pointers to the worker nodes.
 */
   k = ((klast+NB-1)/NB)*NB;
   k = Mmax(Kp, k);
   wrksz = (2+ATL_NTHREADS)*ATL_Cachelen + ATL_MulBySize(MB)*nMb*k +
           ATL_NTHREADS*(sizeof(void*)+sizeof(TYPE*) + ATL_MulBySize(NB)*k);
   if (ATL_NTHREADS*ATL_PTMAXMALLOC < wrksz)
      return(1);
   vp = malloc(wrksz);
   if (!vp)
      return(2);
   Bws = vp;                            /* B work ptrs first array */
   acnts = (void*)(Bws+ATL_NTHREADS);   /* then atomic count array */
   Aw = (TYPE*)(acnts+ATL_NTHREADS);    /* then workspace for common A */
   Aw = ATL_AlignPtr(Aw);               /* A must be aligned */
   p = Aw + M*k;                        /* first B wrkspc after A wrkspc */
   Bws[0] = ATL_AlignPtr(p);            /* B wrkspcs must be aligned */
   for (i=1; i < ATL_NTHR; i++)         /* init rest of nthr-len arrays */
   {
      p = Bws[i-1] + N*k;
      Bws[i] = ATL_AlignPtr(p);
   }
   pbdef.alpha = alpha; pbdef.beta = beta;
   pbdef.A = A; pbdef.B = B; pbdef.C = C;
   pbdef.M = M; pbdef.N = N; pbdef.K = K;
   pbdef.lda = lda; pbdef.ldb = ldb; pbdef.ldc = ldc;
   pbdef.Kp = Kp; pbdef.klast = klast; pbdef.nKp = nKp;
   pbdef.nMb = nMb; pbdef.nNb = nNb;
   pbdef.mr = M - nMb * NB; pbdef.nr = N - nNb * NB;
   pbdef.TA = TA; pbdef.TB = TB;
   pbdef.Aw = Aw; pbdef.Bws = Bws;
   i = nr ? nNb+1 : nNb;
   pbdef.aBcnt = ATL_SetAtomicCount(i);
   pbdef.donecnt = ATL_SetAtomicCount(i);
   if (SCALAR_IS_ONE(beta))
      pbdef.aCcnt = NULL;
   else
      pbdef.aCcnt = ATL_SetAtomicCount(N);
//   printf("Mp=%d, nMp=%d, mlast=%d, Kp=%d, nKp=%d, klast=%d\n",
//          Mp, nMp, mlast, Kp, nKp, klast);
   ATL_goparallel(ATL_NTHREADS, Mjoin(PATL,DoWork_NKp), &pbdef, NULL);
   free(vp);
   return(0);
}
@ROUT ATL_gccstackfix
#include "atlas_asm.h"
/*
 * This routine necessary to fix gcc's 32-bit ABI violation:
 *    http://math-atlas.sourceforge.net/errata.html#gccCrazy
 * This assembly is only called on 32-bit Windows, where all windows calls
 * call this wrapper function, which takes a pointer to a void pointer array;
 * the first entry is a function pointer to the function usually threaded,
 * while the second is a pointer to its argument.  This assembly routine
 * manually aligns the stack so that gcc's ABI violation doesn't kill
 * the spawned thread.
 */

/*
 *                       4(%esp) 
 * void *ATL_gccstackfix(void *vp)
 */
        .text
.globl ATL_asmdecor(ATL_gccstackfix)
ALIGN16
ATL_asmdecor(ATL_gccstackfix):
#define FSIZE 12                /* (SP0, arg, func) */
ATL_gccstackfix:
   mov %esp, %eax               /* get unaligned stack ptr */
   and $0xF0, %al               /* align to 32 bytes */
   mov %esp, %edx               /* save original value of SP */
   lea -FSIZE(%eax), %esp       /* SP is aligned and has room for FSIZE */
   movl %edx, 4(%esp)           /* stack has original value of SP */
   movl 4(%edx), %eax           /* get ptr to func and arg ptrs */
   movl 4(%eax), %ecx           /* get arg ptr */
   movl %ecx, (%esp)            /* put arg as parameter for called func */
   movl 0(%eax), %ecx           /* get address of func to call */
   call *%ecx                   /* call func, pushing ret @ to stack */
   mov 4(%esp), %esp            /* restore stack ptr */
   ret
@ROUT ATL_tDistMemTouch
#include "atlas_misc.h"
#include "atlas_threads.h"

typedef struct
{
   int *mem;   /* ptr to memory location */
   size_t N;   /* length of mem in sizeof(int) chunks */
} ATL_dmt_t;

/*
 * This function will make it so that a first-touch allocation policy
 * still results in the pages of mem being allocated in cyclic fashion
 * amongst the nodes.  Without calling this function, all allocated memory
 * that is serially initialized will be owned by only one processor, which
 * can cause slowdown on parallel machines.  For AMD machines with HT-assist
 * putting all pages in one local memory essentially reduces the global
 * cache size to the local cache size, with a corresponding disastrous
 * drop in performance.
 * The first page is always assigned to rank=0; if you allocate a bunch of
 * small spaces, they will all be owned by rank=0!
 */
void ATL_dmt_DOWORK(ATL_LAUNCHSTRUCT_t *lp, void *vp)
{
   ATL_thread_t *tp=vp;
   const int iam = tp->rank;
   ATl_dmt_t *pd = lp->opstruct;
   const size_t pgsz = 4096 / sizeof(int), stride=pgsz * ATL_NTHREADS;
   const size_t N = pd->N;
   int *mp = pd->mem, *end = pd->mem + N;

   mp += iam*pgsz;
   while (mp < end)
   {
      *mp = iam+1;
      mp += stride;
   }
}

void ATL_tDistMemTouch  /* use first-touch policy to force cyclic page distro */
(
   size_t N,   /* length of allocated memory */
   void *vp    /* allocated memory */
)
{
   ATL_dmt_t pd;
   pd.N = N / sizeof(int);
   pd.mem = vp;
   ATL_goparallel(ATL_NTHREADS, ATL_dmt_DOWORK, &pd, NULL);
}
@ROUT amm
struct amm
{
   int ntexp;     /* # of threads who must at least call before return */
   int ntactive;  /* # of threads who've checked in for work */
   int ntcomp;    /* # of threads presently doing computations */
   int ntdone;    /* # of threads idling for finish of computation */
   int ngcntK;    /* # of glocal counters used for K dims */
   int mb, nb, kb;/* blocking for kernel */
   int mr, nr, kr;/* size of partial last block.  if 0, size is full block */
   int nfmblks, nfnblks, nfkblks;  /* CEIL(N/NB) for each dim */
   int nmblks, nnblks, nkblks;  /* FLOOR(N/NB) for each dim */
   int nAcp, nBcp;             /* # of panels of A/B copied so far */
   int *bvA, *bvB; /* bitvecs for tracking copy of A/B, one bit per K-panel */
   int *bvC;  /* bitvec for blocks of C, which are our compute tasks */
   void **Mstart;  /* nmblks different counters for perf copy of panels of A */
   void **Nstart;  /* nnblks different counters for perf copy of panels of B */
   void **Mdone;   /* nmblks different counters for finishing copy of A */
   void **Ndone;   /* nmblks different counters for finishing copy of B */
}

void setuptAMM()
{
   int int ncnt, i, nfmblks, nfnblks, nfkblks;
   nfmblks = ap->nfmblks = M/mb;
   nfnblks = ap->nfnblks = N/nb;
   nfkblks = ap->nfkblks = K/kb;
   ap->mr = M - nfmblks*mb;
   ap->nr = N - nfnblks*nb;
   ap->kr = K - nfkblks*kb;
   ap->nmblks = ap->mr ? nfmblks+1 : nfmblks;
   ap->nnblks = ap->nr ? nfnblks+1 : nfnblks;
   ap->nkblks = ap->kr ? nfkblks+1 : nfkblks;
   ap->Mstart = malloc(sizeof(void*)*2*(nfnblks+nfmblks));
   ATL_assert(ap->Mstart);
   ap->Mdone = ap->Mstart + nfmblks;
   ap->Nstart = ap->Mdone + nfmblks;
   ap->Ndone = ap->Nstart + nfnblks;
   ncnt = (nfkblks > 64) ? nfkblks >> 5 : 1;
   #if ATL_NTHREADS > 8
      if (ncnt > (ATL_NTHREADS>>3))
         ncnt = ATL_NTHREADS>>3;
      ncnt = (ncnt > 8) ? 8 : ncnt;
   #else
      ncnt = 1;
   #endif
   ap->ngcntK = ncnt;
   for (i=0; i < nfmblks; i++)
   {
      ap->Mstart[i] = ATL_SetGlobalAtomicCount(ncnt, nfkblks, 0);
      ap->Mdone[i] = ATL_SetGlobalAtomicCount(ncnt, nfkblks, 0);
   }
   for (i=0; i < nfnblks; i++)
   {
      ap->Nstart[i] = ATL_SetGlobalAtomicCount(ncnt, nfkblks, 0);
      ap->Ndone[i] = ATL_SetGlobalAtomicCount(ncnt, nfkblks, 0);
   }
}
void FreetAMM()
{
   for (i=0; i < nfmblks; i++)
   {
      ATL_FreeGlobalAtomicCount(ap->Mstart[i]);
      ATL_FreeGlobalAtomicCount(ap->Mdone[i]);
   }
   for (i=0; i < nfnblks; i++)
   {
      ATL_FreeGlobalAtomicCount(ap->Nstart[i]);
      ATL_FreeGlobalAtomicCount(ap->Ndone[i]);
   }
   free(ap->Mstart);
}

{
   SELECTOR:
      TRY_TO_COMPUTE:
/*
 *       If the number computing equals the number of available jobs,
 *       I cannot do any computation yet
 */
         if (tp->njobs == tp->ncomp)
         {
            if (tp->njobs < tp->ntasks)  /* if possible, go enable more jobs */
               goto TRY_TO_COPY;
            else                         /* otherwise await completion */
               goto DONE;
         }
         else /* Looks like there's a job I can grab */
         {
            LOCK 
            if njobs == ncomp
            {
               UNLOCK
               bail as before;
            }
            get job, update data
            UNLOCK
            call job
         }
      TRY_TO_COPY:
/*
 *       If we've copied everything, can only do computation or finish
 */
         if (tp->nAcp == tp->nmblks && tp->nBcp == tp->nnblks)
            goto TRY_TO_COMPUTE;
   goto SELECTOR
DONE:
   LOCK
     tp->ntdone++;
   UNLOCK
   while(tp->ntdone < tp->ntexp)
      ATL_thread_yield();
}
@ROUT ATL_tammm_tMN
#include "atlas_misc.h"
#define ATL_ESTNCTR 1
#include "atlas_level1.h"
#include "atlas_tlvl3.h"
void Mjoin(PATL,DoWork_amm_tMN)(void *vpp, int rank, int vrank)
{
   ATL_tpool_t *pp=vpp;
   ATL_tamm_tMN_t *pd = pp->PD;
   cm2am_t a2blk = pd->a2blk, b2blk = pd->b2blk;
   ammkern_t amm = pd->amm_b0;
   TYPE *wA = pd->w + vrank*pd->szW;
   TYPE *wB;
   TYPE *wC;
   const int nkblks = pd->nkblks, kb=pd->kb, KB0=pd->KB0, kb0=pd->kb0,
             M=pd->M, N=pd->N;
   const size_t incA = (pd->TA) ? 1:pd->lda;
   const size_t incB = (pd->TB) ? pd->ldb:1;
   int kctr;

   wA = ATL_AlignPtr(wA);
   wB = wA + pd->szA;
   wC = wB + pd->szB;
   while ((kctr = ATL_DecGlobalAtomicCount(pd->KbCtr, vrank)))
   {
      const int kblk = nkblks - kctr;
      size_t k;
@skip printf("%d,%d: nkblks=%d, kblk=%d\n", vrank,rank, nkblks, kblk);
/*
 *    Normal case, doing full K blocks
 */
      if (kblk)
      {
         size_t k = kb0 + kb*(kblk-1);
         const TYPE *a = pd->A + k*incA, *b = pd->B + k*incB;
         a2blk(kb, M, ATL_rone, a, pd->lda, wA);
         b2blk(kb, N, ATL_rone, b, pd->ldb, wB);
         amm(pd->nmu, pd->nnu, kb, wA, wB, wC, wA, wB, wC);
      }
/*
 *    Doing possibly partial kb0 at beginning
 */
      else
      {
         amm = (amm == pd->amm_b1) ? pd->ammK_b1 : pd->ammK_b0;
         a2blk(kb0, M, ATL_rone, pd->A, pd->lda, wA);
         b2blk(kb0, N, ATL_rone, pd->B, pd->ldb, wB);
         amm(pd->nmu, pd->nnu, KB0, wA, wB, wC, wA, wB, wC);
      }
      amm = pd->amm_b1;
   }
/*
 * If I did no work, zero my wC so combine isn't messed up!
 */
   if (amm == pd->amm_b0)
      Mjoin(PATL,zero)(pd->nC, wC, 1);
}

void Mjoin(PATL,DoComb_amm_tMN)
   (void *vpp, int rank, int vrank, int hisvrank)
{
   ATL_tpool_t *pp=vpp;
   ATL_tamm_tMN_t *pd = pp->PD;
   TYPE *mC, *hC;

   mC = pd->w + vrank*pd->szW;
   mC = ATL_AlignPtr(mC);
   mC += pd->szA + pd->szB;

   hC = pd->w + hisvrank*pd->szW;
   hC = ATL_AlignPtr(hC);
   hC += pd->szA + pd->szB;
   Mjoin(PATL,axpy)(pd->nC, ATL_rone, hC, 1, mC, 1);
}

/*
 * This routine handles the case where M <= maxMB && N <= maxNB so C is
 * only one block.  It gets its own case because it requires minimal
 * workspace.  K will need to fairly long, and due to streaming A & B
 * through the cache with only intra-kernel reuse, its parallel efficiency
 * tends to be limited by bus speed.
 */
int Mjoin(PATL,tammm_tMN)
(
   enum ATLAS_TRANS TA,
   enum ATLAS_TRANS TB,
   ATL_CINT M,
   ATL_CINT N,
   ATL_CINT K,
   const SCALAR alpha,
   const TYPE *A,
   ATL_CINT lda,
   const TYPE *B,
   ATL_CINT ldb,
   const SCALAR beta,
   TYPE *C,
   ATL_CINT ldc
)
{
   ATL_SZT nmblks;
   amminfo_t mminfo;
   unsigned int i, mb, nb, kb, kb0, KB0, nmu, nnu, mu, nu, ku, P, mr;
   ATL_INT nkblks;
   ATL_tamm_tMN_t pd;  /* problem definition structure */
   void *vp;
   TYPE *wC;

/*
 * Special case for tiny N&K, and large M
 */
   if (N > ATL_AMM_MAXNB || M > ATL_AMM_MAXMB ||
       K < Mmin(8,ATL_NTHREADS)*ATL_AMM_MAXKB)
      return(1);
/*
 * Not uncommon to recur and hit this degenerate case that's a dot product
 */
   if (M == 1 && N == 1)
   {
      TYPE dot;
      dot = Mjoin(PATL,dot)(K, A, (TA == AtlasNoTrans)?lda:1, 
                            B, (TB == AtlasNoTrans) ? 1:ldb);
      if (SCALAR_IS_ZERO(beta))
         *C = alpha * dot;
      else
         *C = alpha * dot + beta * (*C);
      return(0);
   }
   nkblks = K / ATL_AMM_66KB;
   mb = Mjoin(PATL,tGetAmmmInfo)(&mminfo, Mmin(nkblks, ATL_NTHREADS), TA,
                                 TB, M, N, K, alpha, beta);

   if (!SCALAR_IS_ONE(alpha)) { ATL_assert(mb == 2); }
   pd.a2blk = mminfo.a2blk;
   pd.b2blk = mminfo.b2blk;
   pd.amm_b0 = mminfo.amm_b0;
   pd.amm_b1 = mminfo.amm_b1;
   pd.TA = (TA == AtlasTrans);
   pd.TB = (TB == AtlasTrans);
   pd.M = M;
   pd.N = N;
   pd.K = K;
   pd.A = A;
   pd.B = B;
   pd.C = C;
   pd.lda = lda;
   pd.ldb = ldb;
   pd.ldc = ldc;
   mu = mminfo.mu;
   nu = mminfo.nu;
   ku = mminfo.ku;
   pd.nmu = nmu = (M+mu-1)/mu;
   pd.nnu = nnu = (N+nu-1)/nu;
   kb = pd.kb = mminfo.kb;
   nkblks = K / kb;
   KB0 = kb0 = K - nkblks*kb;
   if (!kb0)
   {
      kb0 = KB0 = kb;
      pd.ammK_b0 = mminfo.amm_b0;
      pd.ammK_b1 = mminfo.amm_b1;
   }
   else
   {
      nkblks++;
      #if ATL_AMM_MAXKMAJ > 1
         if (ATL_AMMFLG_KMAJOR(mminfo.flag))
         {
            KB0 = ((kb0+ku-1)/ku)*ku;
            if (ATL_AMMFLG_KRUNTIME(mminfo.flag))
            {
               pd.ammK_b0 = mminfo.amm_b0;
               pd.ammK_b1 = mminfo.amm_b1;
            }
            else
            {
               pd.ammK_b0 = mminfo.amm_k1_b0;
               pd.ammK_b1 = mminfo.amm_k1_b1;
            }
         }
         else
      #endif
      {
         if (ATL_AMMFLG_KRUNTIME(mminfo.flag) && kb0 == (kb0/ku)*ku &&
             kb0 > mminfo.kbmin)
         {
            pd.ammK_b0 = mminfo.amm_b0;
            pd.ammK_b1 = mminfo.amm_b1;
         }
         else
         {
            pd.ammK_b0 = mminfo.amm_k1_b0;
            pd.ammK_b1 = mminfo.amm_k1_b1;
         }
      }
   }
   pd.nkblks = nkblks;
   pd.KB0 = KB0;
   pd.kb0 = kb0;
/*
 * Maximum scale is limited by NTHREADS or max number of K-blocks
 */
   P = (ATL_NTHREADS <= nkblks) ? ATL_NTHREADS : nkblks;
   mb = nmu * mu;
   nb = nnu * nu;
   pd.nC = mb*nb;
   pd.szA = mb*kb;
   pd.szB = nb*kb;
   pd.szW = pd.szA + pd.szB + pd.nC + ATL_Cachelen;

   vp = malloc(ATL_MulBySize(pd.szW*P + mu*nu*ku));
   if (!vp)
      return(2);
   pd.w = vp;
   pd.KbCtr = ATL_SetGlobalAtomicCount(ATL_EstNctr(nkblks, P), nkblks, 0);

/*   #define DEBUG1 */
   #ifdef DEBUG1
   {
      ATL_tpool_t *pp=ATL_TP_PTR;
      if (!pp)
         pp = ATL_NewThreadPool(1, 0, NULL);
      ATL_assert(pp);
      pp->PD = &pd;
      Mjoin(PATL,DoWork_amm_tMN)(pp, 0, 0);
      if (pp != ATL_TP_PTR)
         ATL_FreeThreadPool(pp);
   }
   #else
      ATL_goParallel(P, Mjoin(PATL,DoWork_amm_tMN), Mjoin(PATL,DoComb_amm_tMN),
                     &pd, NULL);
   #endif
   ATL_FreeGlobalAtomicCount(pd.KbCtr);
/*
 * Copy answer back out while scaling by alpha and beta
 */
   wC = ATL_AlignPtr(pd.w);
   wC += pd.szA + pd.szB;
   mminfo.Cblk2cm(M, N, alpha, wC, beta, C, ldc);

   free(vp);
   return(0);
}
@ROUT ATL_ttrsm_amm
#include "atlas_misc.h"
#define ATL_ESTNCTR 1
#include "atlas_tlvl3.h"
#include "atlas_bitvec.h"

static void cpyAblk(ATL_ttrsm_amm_t *pd)
{
   const int mb=pd->mb;
   int actr, ablk, kb;
   size_t i, j;
   const TYPE *a=pd->A;
   TYPE *wA;

   actr = ATL_DecAtomicCount(pd->AblkCtr);
   if (!actr)
      return;
   ablk = pd->nablks - actr;
   wA = pd->wA + ablk*pd->blkszA;
   Mtblk2coord(pd->nmblks, ablk, i, j);
   i = pd->mb0 + (i-1)*mb;
   if (j)
   {
      j = pd->mb0 + (j-1)*mb;
      kb = mb;
   }
   else
      kb = pd->mb0;
   if (pd->uplo == AtlasLower)
      a += j*pd->lda + i;
   else
      a += i*pd->lda + j;
   pd->a2blk(kb, mb, ATL_rnone, a, pd->lda, wA);
   ATL_mutex_lock(pd->Acpymut);
   ATL_SetBitBV(pd->AcpyBV, ablk);
   if (ATL_FindFirstUnsetBitBV(pd->AcpyBV, 0) == -1)
      pd->AcpyDone = 1;
   ATL_mutex_unlock(pd->Acpymut);
}

#define trsmK Mjoin(PATL,trsm)
void Mjoin(PATL,DoTrsm_amm)(void *vpp, int rank, int vrank)
{
   ATL_tpool_t *pp=vpp;
   ATL_ttrsm_amm_t *pd = pp->PD;
   TYPE *wrk;
   int rhs;
   void *rhsCtr = pd->rhsCtr;

   wrk = pd->w + vrank*pd->wsL;
   if ((rhs = ATL_DecGlobalAtomicCount(rhsCtr, vrank)))
   {
      TYPE *wB=wrk, *X = pd->X;
      TYPE alpha = pd->alpha;
      const size_t ldx=pd->ldx, lda=pd->lda;
      const int mb=pd->mb, mb0=pd->mb0, MB0=pd->MB0, nmu=pd->nmu;
      const int nmblks=pd->nmblks, nnblks=pd->nnblks;
      const int blkszC = pd->blkszB, blkszA = pd->blkszA;
      const enum ATLAS_DIAG diag = pd->diag;
      const enum ATLAS_DIAG uplo = pd->uplo;
      const enum ATLAS_TRANS TA=pd->TA;
      cm2am_t b2blk = pd->b2blk;
      ablk2cmat_t blk2c=pd->blk2c;
      ammkern_t amm_b0=pd->amm_b0, amm = pd->amm_b1;

      do
      {
         const int rblk = nnblks - rhs;
         const int nb = (rblk == nnblks-1) ? pd->nbf : pd->nb;
         const int nnu = (rblk == nnblks-1) ? pd->nnuf : pd->nnu;
         TYPE *x = X + ldx*rblk*pd->nb;
         TYPE *wA=pd->wA, *wC=wB+blkszC;
         const TYPE *A = pd->A;
/*
 *       Solve top block using first diag blk of A (no need to copy)
 */
         trsmK(AtlasLeft, uplo, TA, diag, mb0, nb, alpha, A, lda, x, ldx);
/*
 *       If there are more RHS blocks, subtract solved X from them
 */
         if (nmblks > 1)
         {
            int i;
            TYPE *wc=wC, *a = wA;
            int ba=0;
/*
 *          Copy solved part of X, and subtract it from C, which will later be
 *          used to update the B values before solving them to X
 */
            b2blk(mb0, nb, ATL_rone, x, ldx, wB);
            for (i=1; i < nmblks; i++, ba++)
            {
               TYPE *cn = (i != nmblks-1) ? wc + blkszC:wC, *an = a + blkszA;
/*
 *             If the block of A we need hasn't been copied, copy A until it is.
 */
               if (!pd->AcpyDone)
               {
                  while(!ATL_IsBitSetBV(pd->AcpyBV, ba))
                     cpyAblk(pd);
               }
               amm_b0(nmu, nnu, MB0, a, wB, wc, an, wB, cn);
               wc = cn;
               a = an;
            }
/*
 *          For each remaining RHS block, we solve it, then subtract its
 *          part of the equation from all the blocks below it, until alg
 *          finishes.
 */
            A += mb0*(lda+1);
            x += mb0;
            for (i=1; i < nmblks; i++)
            {
/*
 *             Apply alpha to X, and then subtract of solved equations, before
 *             solving using the current mbxmb diagonal block of A to form X
 */
               blk2c(mb, nb, ATL_rone, wC, alpha, x, pd->ldx);
               wC += blkszC;
               trsmK(AtlasLeft, uplo, TA, diag, mb, nb, ATL_rone, A, lda, 
                     x, ldx);
/*
 *             Subtract solved equations from remaining RHS blocks
 */
               if (i != nmblks-1)
               {
                  int k;

                  b2blk(mb, nb, ATL_rone, x, ldx, wB);
                  wc = wC;
                  for (k=i+1; k < nmblks; k++, ba++)
                  {
                     TYPE *cn = (k != nmblks-1) ? wc+blkszC : wC; 
                     TYPE *an = a + blkszA;
                     if (!pd->AcpyDone)
                     {
                        while(!ATL_IsBitSetBV(pd->AcpyBV, ba))
                           cpyAblk(pd);
                     }
                     amm(nmu, nnu, mb, a, wB, wc, an, wB, cn);
                     wc = cn;
                     a = an;
                  }
               }
               A += mb*(lda+1);
               x += mb;
            }
         }
      }
      while ((rhs = ATL_DecGlobalAtomicCount(rhsCtr, vrank)));
   }
/*
 * See if any A remains to be copying before finishing; only if I never got
 * any work is it possible to reach here before A is completely copied.
 */
   else if (!pd->AcpyDone)
      while (ATL_GetAtomicCount(pd->AblkCtr))
         cpyAblk(pd);
}

/*
 * This routine handles case where A is on left of B, A X = B
 */
static int ttrsm_ammL
   (const enum ATLAS_UPLO uplo, const enum ATLAS_TRANS TA, 
    const enum ATLAS_DIAG diag, ATL_CINT M, ATL_CINT N, const SCALAR alpha,
    const TYPE *A, ATL_CINT lda, TYPE *B, ATL_CINT ldb)
{
   ATL_ttrsm_amm_t pd;
   int P = (ATL_TP_PTR) ? ATL_TP_PTR->nthr : ATL_NTHREADS;
   int i;
   size_t sz, szA, szL;
   void *vp;

   P = Mjoin(PATL,tGetTrsmInfo)(&pd, P, TA, M, N, alpha);
   if (P < 2)
   {
      Mjoin(PATL,trsm)(AtlasLeft, uplo, TA, diag, M, N, alpha, A, lda, B, ldb);
      return(0);
   }
// printf("M=%d, N=%d, P=%d, mb=%d, nb=%d\n", M, N, P, pd.mb, pd.nb);
   pd.nablks = sz = ((pd.nmblks-1)*pd.nmblks)>>1;
   if (sz != pd.nablks)    /* if # of blocks overflow ints */
      return(1);              /* tell caller to recur until it doesn't */
   if (((size_t)pd.nmblks)*pd.nnblks != pd.nxblks)
      return(1);
   pd.TA = TA;
   pd.uplo = uplo;
   pd.diag = diag;
   pd.M = M;
   pd.N = N;
   pd.alpha = alpha;
   pd.lda = lda;
   pd.ldx = ldb;
   pd.A = A;
   pd.X = B;
   pd.blkszB = pd.mb * pd.nb;
   pd.blkszA = pd.mb * pd.mb;
   pd.panszC = (pd.nmblks-1) * pd.blkszB;
   szA = pd.nablks * pd.blkszA;
   pd.wsL = szL = pd.blkszB + pd.panszC;
   sz = szA + P*szL + pd.mu*pd.nu*pd.ku;
   sz = ATL_MulBySize(sz) + ATL_Cachelen;
   if ((sz>>20) > ATL_PTMAXMALLOC_MB)
      return(2);
   vp = malloc(sz);
   if (!vp)
      return(2);
   pd.AcpyDone = (pd.mb >= M);
   pd.wA = ATL_AlignPtr(vp);
   pd.w = pd.wA + szA;
/*
 * Select the number of cores to perform A copy: to many and they just
 * fight for the bus, to few and algorithm starts slower
 */
   if (P >= 8)
   {
      i = P>>2;
      i = (i > 4) ? 4 : i;
   }
   else if (P >= 4)
      i = P>>1;
   else
      i = P;
   pd.AblkCtr = ATL_SetAtomicCount(pd.nablks);  /* w/o glob, is in-order */
   pd.rhsCtr = ATL_SetGlobalAtomicCount(ATL_EstNctr(pd.nnblks,P), pd.nnblks, 0);
   pd.AcpyBV = ATL_NewBV(pd.nablks);
   pd.Acpymut = ATL_mutex_init();
/*   #define DEBUG1  */
   #ifdef DEBUG1
   {
      ATL_tpool_t *pp=ATL_TP_PTR;
      if (!pp)
         pp = ATL_NewThreadPool(1, 0, NULL);
      ATL_assert(pp);
      pp->PD = &pd;
      Mjoin(PATL,DoTrsm_amm)(pp, 0, 0);
      if (pp != ATL_TP_PTR)
         ATL_FreeThreadPool(pp);
   }
   #else
      ATL_goParallel(P, Mjoin(PATL,DoTrsm_amm), NULL, &pd, NULL);
   #endif
   ATL_FreeAtomicCount(pd.AblkCtr);
   ATL_FreeGlobalAtomicCount(pd.rhsCtr);
   ATL_FreeBV(pd.AcpyBV);
   ATL_mutex_free(pd.Acpymut);
   free(vp);
   return(0);
}

/*
 * Simple recursion for Left, Lower, Notrans case (used in LU)
 */
#include "atlas_tcacheedge.h"
#if CacheEdge > ATL_PTMAXMALLOC_MB*1024*1024
   #undef CacheEdge
#endif
#ifndef CacheEdge
   #define CacheEdge 524288
#endif

static void trsmREC_LLN
   (const int STOP_EARLY, const enum ATLAS_DIAG diag, ATL_CSZT M, ATL_CSZT N,
    const SCALAR alpha, const TYPE *A, ATL_CSZT lda, TYPE *B, ATL_CSZT ldb)
{
   ATL_CSZT Ml=M>>1, Mr = M-Ml;
   TYPE *B1=B+(Ml SHIFT);
   const TYPE *A10=A+(Ml SHIFT), *L11=A10+Ml*(lda SHIFT);
   #ifdef TCPLX
      TYPE ONE[2] = {ATL_rone, ATL_rzero};
      TYPE NONE[2] = {ATL_rnone, ATL_rzero};
   #else
      #define ONE ATL_rone
      #define NONE ATL_rnone
   #endif
   size_t sz;  /* size(A) + 2*pan(B) */
/*
 * Stop recursion when A & panel of B fit in cache
 */
   sz = ((M*M)>>1) + M*(ATL_AMM_98KB<<1);
   sz = ATL_MulBySize(sz);
   if (STOP_EARLY || sz < CacheEdge)
   {
      if (!ttrsm_ammL(AtlasLower, AtlasNoTrans, diag, M, N, alpha, 
                      A, lda, B, ldb))
         return;
/*
 *    If we can't allocate any space, call serial & hope it can work
 */
      if (M < 80)
      {
         Mjoin(PATL,trsm)(AtlasLeft, AtlasLower, AtlasNoTrans, diag, M, N, 
                          alpha, A, lda, B, ldb);
         return;
      }
   }
   trsmREC_LLN(STOP_EARLY, diag, Ml, N, alpha, A, lda, B, ldb);
   Mjoin(PATL,tgemm)(AtlasNoTrans, AtlasNoTrans, Mr, N, Ml, NONE, A10, lda,
                     B, ldb, alpha, B1, ldb);
   trsmREC_LLN(STOP_EARLY, diag, Mr, N, ONE, L11, lda, B1, ldb);
}

static void trsmREC_LUT
   (const int STOP_EARLY, const enum ATLAS_DIAG diag, ATL_CSZT M, ATL_CSZT N,
    const SCALAR alpha, const TYPE *A, ATL_CSZT lda, TYPE *B, ATL_CSZT ldb)
{
   ATL_CSZT Ml=M>>1, Mr = M-Ml;
   TYPE *B1=B+(Ml SHIFT);
   const TYPE *A10=A+(Ml*lda SHIFT), *L11=A10+(Ml SHIFT);
   #ifdef TCPLX
      TYPE ONE[2] = {ATL_rone, ATL_rzero};
      TYPE NONE[2] = {ATL_rnone, ATL_rzero};
   #else
      #define ONE ATL_rone
      #define NONE ATL_rnone
   #endif
   size_t sz;  /* size(A) + 2*pan(B) */
/*
 * Stop recursion when A & panel of B fit in cache
 */
   sz = ((M*M)>>1) + M*(ATL_AMM_98KB<<1);
   sz = ATL_MulBySize(sz);
   if (STOP_EARLY || sz < CacheEdge)
   {
      if (!ttrsm_ammL(AtlasUpper, AtlasTrans, diag, M, N, alpha, 
                      A, lda, B, ldb))
         return;
/*
 *    If we can't allocate any space, call serial & hope it can work
 */
      if (M < 80)
      {
         Mjoin(PATL,trsm)(AtlasLeft, AtlasUpper, AtlasTrans, diag, M, N, 
                          alpha, A, lda, B, ldb);
         return;
      }
   }
   trsmREC_LUT(STOP_EARLY, diag, Ml, N, alpha, A, lda, B, ldb);
   Mjoin(PATL,tgemm)(AtlasTrans, AtlasNoTrans, Mr, N, Ml, NONE, A10, lda,
                     B, ldb, alpha, B1, ldb);
   trsmREC_LUT(STOP_EARLY, diag, Mr, N, ONE, L11, lda, B1, ldb);
}

#ifndef TCPLX
   #undef ONE
   #undef NONE
#endif

#ifdef ATL_ARCH_XeonPHI
  #define ATL_SE 1
#else
  #define ATL_SE 0
#endif
int Mjoin(PATL,ttrsm_amm)
   (const enum ATLAS_SIDE side, const enum ATLAS_UPLO uplo,
    const enum ATLAS_TRANS TA, const enum ATLAS_DIAG diag,
    ATL_CINT M, ATL_CINT N, const SCALAR alpha,
    const TYPE *A, ATL_CINT lda, TYPE *B, ATL_CINT ldb)
{
   if (side == AtlasLeft)
   {
      if (uplo == AtlasLower && TA == AtlasNoTrans)
      {
         trsmREC_LLN(ATL_SE, diag, M, N, alpha, A, lda, B, ldb);
         return(0);
      }
      if (uplo == AtlasUpper && TA == AtlasTrans)
      {
         trsmREC_LUT(ATL_SE, diag, M, N, alpha, A, lda, B, ldb);
         return(0);
      }
   }
   return(1);
}
@ROUT bad_ttrsm
static void SolveAllRemainingRows(ATL_ttrsm_amm_t *pd, int vrank, int xctr)
{
   const int mb=pd->mb, mb0=pd->mb0, nnblks=pd->nnblks, nxblks=pd->nxblks,
      nmu=pd->nmu, blkszA=pd->blkszA, blkszB=pd->blkszC,
      panszB=pd->panszB, nmblks=pd->nmblks, Bn=pd->nb;
   TYPE *X, *C = pd->wCs + vrank*pd->blkszC;
   ammkern_t amm=pd->amm_b1;
   ablk2cmat_t blk2c=pd->blk2c;

   do
   {
      const int xblk = nxblks - xctr, i = xblk / nnblks, j = xblk - i*nnblks;
      const int nb  =(j < nnblks-1) ? Bn : pd->nbf, 
                nnu =(j < nnblks-1) ? pd->nnu : pd->nnuf;
      int k, bb;
      const TYPE *a;
/*
 *    Loop over all row panels I'm waiting on to come into B
 */
      for (bb=k=0; k < i; k++, bb += nnblks)
      {
         int ba, ibn = bb+j+nnblks;
/*
 *       Wait until B(k,j) is available, and copy A while waiting
 */
         while(!ATL_IsBitSetBV(pd->BcpyBV, bb+j))
         {
            if (!pd->AcpyDone)
               cpyAblk(pd);
            else
               ATL_thread_yield();
         }
/*
 *       If the block of A I need hasn't been copied, copy until it is
 *       We get the lock to force memory barrier on weakly-ordered caches,
 *       so that we don't waste time copying A while waiting for the BV
 *       to be updated
 */
         ba = Mcoord2tblk(nmblks, i, k);
         if (!pd->AcpyDone)
         {
            while(!pd->AcpyDone && !ATL_IsBitSetBV(pd->AcpyBV, ba))
               cpyAblk(pd);
@beginskip
            ATL_mutex_lock(pd->Acpymut);
            while(!ATL_IsBitSetBV(pd->AcpyBV, ba))
            {
               ATL_mutex_unlock(pd->Acpymut);
               cpyAblk(pd);
               ATL_mutex_lock(pd->Acpymut);
            }
            ATL_mutex_unlock(pd->Acpymut);
@endskip
         }
/*
 *       I've got both A & B I need, so time to put the solved equation
 *       subtractions into C for later updating of my RHS
 */
         if (k)
         {
            TYPE *a = pd->wA + ba*blkszA;
            TYPE *b = pd->wB + j*panszB + k*blkszB, *bn=b;
            if (ibn < nxblks-nnblks && ATL_IsBitSetBV(pd->BcpyBV, ibn))
               bn = b + blkszB;
            amm(nmu, nnu, mb, a, b, C, a+blkszA, bn, C);
         }
         else /* first row-panel of size mb0, may require special kernel */
         {
            TYPE *a = pd->wA + ba*blkszA;
            TYPE *b = pd->wB + j*panszB, *bn=b;
            if (ibn < nxblks-nnblks && ATL_IsBitSetBV(pd->BcpyBV, bb+j+nnblks))
               bn = b + blkszB;
            pd->amm_b0(nmu, nnu, pd->MB0, a, b, C, a+blkszA, bn, C);
         }
      }  /* end loop over B blocks I need to subtract from my RHS */
/* 
 *    OK, I've got all updates in C, apply them and original alpha to my RHS,
 *    then do solve with diagonal block to produce final X
 */
      X = pd->X + pd->mb0 + (i-1)*mb + pd->ldx*Bn*j;
      a = pd->A + (pd->lda+1)*(pd->mb0 + (i-1)*mb);
      blk2c(mb, nb, ATL_rone, C, pd->alpha, X, pd->ldx);
      trsmK(AtlasLeft, pd->uplo, pd->TA, pd->diag, mb, nb, ATL_rone, 
            a, pd->lda, X, pd->ldx);
/*
 *    If I'm not the last row panel, copy my final X to C for updates
 */
      if (i != nmblks-1)
      {
         TYPE *b = pd->wB + j*panszB + i*blkszB;
         pd->b2blk(mb, nb, ATL_rone, X, pd->ldx, b);
         ATL_mutex_lock(pd->Bcpymut);
         ATL_SetBitBV(pd->BcpyBV, bb+j);
         ATL_mutex_unlock(pd->Bcpymut);
      }
   }
   while ((xctr = ATL_DecAtomicCount(pd->XblkCtr)));
}

void Mjoin(PATL,DoTrsm_amm)(void *vpp, int rank, int vrank)
{
   ATL_tpool_t *pp=vpp;
   ATL_ttrsm_amm_t *pd = pp->PD;
   int xctr;
   const int nnblks=pd->nnblks, nxblks=pd->nxblks;

   xctr = ATL_DecAtomicCount(pd->XblkCtr);
   if (xctr)
   {
      unsigned int xblk = nxblks - xctr;
      unsigned int i = xblk / nnblks;
/*
 *    Doing first mb0 row panel! 
 */
      while (!i)
      {
         const unsigned int j = xblk - i*nnblks;
         const int nb = (j == nnblks-1) ? pd->nbf : pd->nb, mb0 = pd->mb0;
         TYPE *x, *b;
/*
 *       Solve X0 = inv(L00) * B0
 */
         x = pd->X + pd->ldx*j*pd->nb;
         trsmK(AtlasLeft, pd->uplo, pd->TA, pd->diag, mb0, nb, pd->alpha, 
               pd->A, pd->lda, x, pd->ldx);
/*
 *       Move X into GEMM B storage for updates, and report ready to update
 */
         if (pd->nmblks > 1)
         {
            b = pd->wB + xblk*pd->panszB;
            pd->b2blk(mb0, nb, ATL_rone, x, pd->ldx, b);
            ATL_mutex_lock(pd->Bcpymut);
            ATL_SetBitBV(pd->BcpyBV, xblk);
            ATL_mutex_unlock(pd->Bcpymut);
         }
         xctr = ATL_DecAtomicCount(pd->XblkCtr);
         xblk = nxblks - xctr;
         i = xblk / nnblks;
      }   /* end while(working on 1st row panel) */
      if (xctr)
         SolveAllRemainingRows(pd, vrank, xctr);
   }     /* end if (xctr) */
/*
 * If all work on RHS has been claimed, finish up any A copying
 */
   while (!pd->AcpyDone)
      cpyAblk(pd);
}

/*
 * This routine handles case where A is on left of B, A X = B
 */
static int ttrsm_ammL
   (const enum ATLAS_UPLO uplo, const enum ATLAS_TRANS TA, 
    const enum ATLAS_DIAG diag, ATL_CINT M, ATL_CINT N, const SCALAR alpha,
    const TYPE *A, ATL_CINT lda, TYPE *B, ATL_CINT ldb)
{
   ATL_ttrsm_amm_t pd;
   int P;
   size_t sz, szA, szB;
   void *vp;

   P = Mjoin(PATL,tGetTrsmInfo)(&pd, TA, M, N, M, alpha);
   pd.actpan = 0;
   pd.nablks = sz = ((pd.nmblks-1)*pd.nmblks)>>1;
   if (sz != pd.nablks)    /* if # of blocks overflow ints */
      return(1);              /* tell caller to recur until it doesn't */
   if (((size_t)pd.nmblks)*pd.nnblks != pd.nxblks)
      return(1);
   pd.TA = TA;
   pd.uplo = uplo;
   pd.diag = diag;
   pd.M = M;
   pd.N = N;
   pd.alpha = alpha;
   pd.lda = lda;
   pd.ldx = ldb;
   pd.A = A;
   pd.X = B;
   pd.blkszC = pd.mb * pd.nb;
   pd.blkszA = pd.mb * pd.mb;
   pd.panszB = (pd.nmblks-1) * pd.blkszC;
   szA = pd.nablks * pd.blkszA;
   szB = pd.panszB * pd.nnblks;
   sz = szA + szB;
   sz += P * pd.blkszC + pd.mu*pd.nu*pd.ku;  /* 1 C block per thread */
   sz = ATL_MulBySize(sz) + ATL_Cachelen;
   if ((sz>>20) > ATL_PTMAXMALLOC_MB)
      return(2);
   vp = malloc(sz);
   if (!vp)
      return(2);
   pd.AcpyDone = (pd.mb >= M);
   pd.wA = ATL_AlignPtr(vp);
   pd.wB = pd.wA + szA;
   pd.wCs = pd.wB + szB;
   pd.XblkCtr = ATL_SetAtomicCount(pd.nxblks);  /* w/o glob, is in-order */
   pd.AblkCtr = ATL_SetAtomicCount(pd.nablks); 
   pd.BcpyBV = ATL_NewBV(pd.nxblks-pd.nnblks);
   pd.AcpyBV = ATL_NewBV(pd.nablks);
   pd.Bcpymut = ATL_mutex_init();
   pd.Acpymut = ATL_mutex_init();
//   #define DEBUG1 
   #ifdef DEBUG1
   {
      ATL_tpool_t *pp=ATL_TP_PTR;
      if (!pp)
         pp = ATL_NewThreadPool(1, 0, NULL);
      ATL_assert(pp);
      pp->PD = &pd;
      Mjoin(PATL,DoTrsm_amm)(pp, 0, 0);
      if (pp != ATL_TP_PTR)
         ATL_FreeThreadPool(pp);
   }
   #else
      ATL_goParallel(P, Mjoin(PATL,DoTrsm_amm), NULL, &pd, NULL);
   #endif
   ATL_FreeAtomicCount(pd.XblkCtr);
   ATL_FreeAtomicCount(pd.AblkCtr);
   ATL_FreeBV(pd.BcpyBV);
   ATL_FreeBV(pd.AcpyBV);
   ATL_mutex_free(pd.Bcpymut);
   ATL_mutex_free(pd.Acpymut);
   free(vp);
   return(0);
}


@beginskip
@endskip

int Mjoin(PATL,ttrsm_amm)
   (const enum ATLAS_SIDE side, const enum ATLAS_UPLO uplo,
    const enum ATLAS_TRANS TA, const enum ATLAS_DIAG diag,
    ATL_CINT M, ATL_CINT N, const SCALAR alpha,
    const TYPE *A, ATL_CINT lda, TYPE *B, ATL_CINT ldb)
{
   if (side == AtlasLeft && uplo == AtlasLower)
      return(ttrsm_ammL(uplo, TA, diag, M, N, alpha, A, lda, B, ldb));
   return(1);
}
@ROUT ATL_tgemm_amm ATL_tammm_tNK ATL_tammm_G
#include "atlas_misc.h"
#define ATL_ESTNCTR 1
#include "atlas_tlvl3.h"
#include "atlas_bitvec.h"

@ROUT ATL_tammm_tNK
void Mjoin(PATL,DoWork_tamm_tNK)(ATL_LAUNCHSTRUCT_t *lp, void *vp)
{
   ATL_thread_t *tp = vp;
   ATL_tamm_tNK_t *pd = lp->opstruct;  /* problem definition structure */
   const unsigned int rank = tp->rank, mb=pd->mb, kb=pd->KB0, K=pd->K,
      N=pd->N, nmblks = pd->nmblks, nmbm1=nmblks-1, nmu=pd->nmu, nnu=pd->nnu,
      lda=pd->lda, ldc=pd->ldc;
   TYPE *pB, *pA, *pC, *C = pd->C;
   const TYPE *A = pd->A;
   int BCOPIED=0;
   int imtr;
   const TYPE beta = *pd->beta;
   const size_t incA = (pd->TA) ? lda : 1;
   cm2am_t a2blk = pd->a2blk;
   ablk2cmat_t blk2c = pd->blk2c;
   ammkern_t amm = pd->amm_b0;

@skip printf("%d: started tamm_K\n", rank);
   pB = pd->w;
/*
 * First guy here starts to copy B
 */
   if (ATL_DecAtomicCount(pd->BassgCtr))
   {
      pd->b2blk(K, N, *pd->alpha, pd->B, pd->ldb, pB);
/*
 *    Let waiting threads know B is ready for use
 */
      BCOPIED = ATL_DecAtomicCount(pd->BdoneCtr);
      ATL_assert(BCOPIED);
   }
   pA = pB + pd->bsz + rank*(pd->wsz);
   pA = ATL_AlignPtr(pA);
   pC = pA + mb*kb;
   pC = ATL_AlignPtr(pC);
/*
 * For first block I work on, I must await B to be copied
 */
   if (!BCOPIED)
   {
      int iblk;
      size_t ii;

      imtr = ATL_DecGlobalAtomicCount(pd->MbCtr, rank);
      if (imtr)
      {
         iblk = nmblks - imtr;
         ii = mb*iblk;
         if (iblk != nmbm1)
            a2blk(K, mb, ATL_rone, A+incA*ii, lda, pA);
         else
            a2blk(K, pd->mr, ATL_rone, A+incA*ii, lda, pA);
         while (ATL_GetAtomicCount(pd->BdoneCtr))  /* await B cpy finish */
            ATL_thread_yield();
         if (iblk != nmbm1)
         {
            amm(nmu, nnu, kb, pA, pB, pC, pA, pB, pC);
            blk2c(mb, N, ATL_rone, pC, beta, C+ii, ldc);
         }
         else
         {
            amm(pd->nmuL, nnu, kb, pA, pB, pC, pA, pB, pC);
            blk2c(pd->mr, N, ATL_rone, pC, beta, C+ii, ldc);
         }
      }
   }
/*
 * Now, B is ready, so just go to town on remaining blocks
 */
   while ((imtr = ATL_DecGlobalAtomicCount(pd->MbCtr, rank)))
   {
      const int iblk = nmblks - imtr;
      const size_t ii = mb*iblk;

      if (iblk != nmbm1)
      {
         a2blk(K, mb, ATL_rone, A+incA*ii, lda, pA);
         amm(nmu, nnu, kb, pA, pB, pC, pA, pB, pC);
         blk2c(mb, N, ATL_rone, pC, beta, C+ii, ldc);
      }
      else
      {
         a2blk(K, pd->mr, ATL_rone, A+incA*ii, lda, pA);
         amm(pd->nmuL, nnu, kb, pA, pB, pC, pA, pB, pC);
         blk2c(pd->mr, N, ATL_rone, pC, beta, C+ii, ldc);
      }
   }
}
/*
 * This routine handles the case where N <= maxNB && K <= maxKB, so B is
 * only one block.  It is particularly important for the panel factorizations
 * of both LU and QR.
 */
int Mjoin(PATL,tammm_tNK)
(
   enum ATLAS_TRANS TA,
   enum ATLAS_TRANS TB,
   ATL_CINT M,
   ATL_CINT N,
   ATL_CINT K,
   const SCALAR alpha,
   const TYPE *A,
   ATL_CINT lda,
   const TYPE *B,
   ATL_CINT ldb,
   const SCALAR beta,
   TYPE *C,
   ATL_CINT ldc
)
{
   ATL_SZT nmblks;
   amminfo_t mminfo;
   unsigned int i, mb, nb, kb, mu, nu, ku, P, mr;
   ATL_tamm_tNK_t pd;  /* problem definition structure */
   void *vp;

/*
 * Special case for tiny N&K, and large M
 */
   if (N >= ATL_AMM_MAXNB || K >= ATL_AMM_MAXKB || M < ATL_AMM_MAXMB || 
       M < Mmin(8,ATL_NTHREADS)*ATL_AMM_MAXMB)
      return(1);
   Mjoin(PATL,GetRankKInfo)(&mminfo, TA, TB, M, N, K, alpha, beta);
   pd.a2blk = mminfo.a2blk;
   pd.b2blk = mminfo.b2blk;
   pd.blk2c = mminfo.Cblk2cm;
   pd.amm_b0 = mminfo.amm_b0;
   pd.TA = (TA == AtlasTrans);
   pd.TB = (TB == AtlasTrans);
   pd.N = N;
   pd.K = K;
   pd.A = A;
   pd.B = B;
   pd.C = C;
   pd.lda = lda;
   pd.ldb = ldb;
   pd.ldc = ldc;
   pd.alpha = &alpha;
   pd.beta  = &beta;
   mu = mminfo.mu;
   nu = mminfo.nu;
   ku = mminfo.ku;
   pd.mb = mb = mminfo.mb;
   pd.nmu = mb / mu;
   pd.nnu = (N+nu-1)/nu;
   nb = pd.nnu * nu;
   kb = mminfo.kb;
   nmblks = M / mb;
   mr = M - nmblks*mb;
   if (!mr)
   {
      pd.mbL = mr = mb;
      pd.nmuL = pd.nmu;
   }
   else
   {
      nmblks++;
      pd.nmuL = (mr+mu-1)/mu; 
      pd.mbL = pd.nmuL * mu;
   }
   pd.mr = mr;
   pd.nmblks = nmblks;
   pd.KB0 = K;
   #if ATL_MAXKMAJ_RKK > 1
      if (ATL_AMMFLG_KMAJOR(mminfo.flag))
         pd.KB0 = ((K+ku-1)/ku)*ku;
   #endif
/*
 * Maximum scale is limited by NTHREADS or max number of M-blocks
 */
   P = (ATL_NTHREADS <= nmblks) ? ATL_NTHREADS : nmblks;
/*
 * We have a common B wrk of size KB0*nb, then
 * for each node, we need workspace: sz(A,C) = mb*K, K*nb, mb*N, laid out
 * in memory as A,C, then we add safety margin mu*nu*ku so advance loads don't
 * seg fault, and we add space for aligning the ptrs
 */
   pd.bsz = pd.KB0*nb;
   pd.wsz = mb*(pd.nnu*nu + pd.bsz) + 2*ATL_DivBySize(ATL_Cachelen);

   vp = malloc(ATL_MulBySize(pd.wsz*P + pd.bsz+mu*nu*ku) + ATL_Cachelen);
   if (!vp) 
      return(2);
   pd.w = ATL_AlignPtr(vp);
   pd.MbCtr = ATL_SetGlobalAtomicCount(ATL_EstNctr(nmblks, P), nmblks, 0);
   pd.BassgCtr = ATL_SetAtomicCount(1);
   pd.BdoneCtr = ATL_SetAtomicCount(1);
   #ifdef DEBUG1
   {
      ATL_LAUNCHSTRUCT_t ls;
      ATL_thread_t ts;
      ts.rank = 0;
      ts.P = 1;
      ls.opstruct = &pd;
      Mjoin(PATL,DoWork_tamm_tNK)(&ls, &ts);
   }
   #else
      ATL_goparallel(P, Mjoin(PATL,DoWork_tamm_tNK), &pd, NULL);
   #endif

   ATL_FreeAtomicCount(pd.BdoneCtr);
   ATL_FreeAtomicCount(pd.BassgCtr);
   ATL_FreeGlobalAtomicCount(pd.MbCtr);

   free(vp);
   return(0);
}
@ROUT ATL_tammm_G
static void DoCpy
(
   ATL_tgemm_ammG_t *pd,             /* problem definition */
   unsigned const int rank,          /* my (virtual) rank */
   unsigned const int dblk,          /* CpyBlk of C to compute */
   const size_t i,                   /* row index to start at */
   const size_t j,                   /* col index to start at */
   unsigned int kblk,                /* kblk to compute */
   unsigned const int mb,            /* actual sz of M w/o padding */
   unsigned const int nb             /* actual sz of N w/o padding */
)
{
   const int CPYA = dblk < pd->nmblks, CPYB = dblk < pd->nnblks;
   int kb = pd->kb, KB0=kb;
   const TYPE *A;
   const TYPE *B;
   TYPE *wA, *wB;
   size_t k;

   if (kblk)
      k = pd->kb0 + (kblk-1)*kb;
   else
   {
      k = 0;
      if (kb != pd->kb0)
      {
         kb = pd->kb0;
         KB0 = pd->KB0;
      }
   }

   if (CPYA)
   {
      A = pd->A + ((pd->TA) ? k + i*pd->lda : i + k*pd->lda);
      wA = pd->wA + kblk*pd->blkszA;
      wA += pd->panszA * ((CPYA) ? dblk : (pd->nmblks-1));
      pd->a2blk(kb, mb, pd->alpA, A, pd->lda, wA);
   }
   if (CPYB)
   {
      B = pd->B + ((pd->TB) ?  j + k*pd->ldb : k + j*pd->ldb);
      wB = pd->wB + kblk*pd->blkszB;
      wB += pd->panszB * ((CPYB) ? dblk : (pd->nnblks-1));
      pd->b2blk(kb, nb, pd->alpB, B, pd->ldb, wB);
   }
/*
 * If I just finished copying the K-panel for A or B, let folks know
 */
   if (ATL_DecGlobalAtomicCountDown((pd->KdonCtr)[dblk], rank) == 1)
   {
      ATL_mutex_lock(pd->cpmut);
      if (CPYA)
      {
         ATL_SetBitBV(pd->cpyAdBV, dblk);
         if (ATL_FindFirstUnsetBitBV(pd->cpyAdBV, 0) == -1)
            pd->cpyAdone = 1;
      }
      if (CPYB)
      {
         ATL_SetBitBV(pd->cpyBdBV, dblk);
         if (ATL_FindFirstUnsetBitBV(pd->cpyBdBV, 0) == -1)
            pd->cpyBdone = 1;
      }
      ATL_mutex_unlock(pd->cpmut);
   }
}

static void DoBlkWtCpy
(
   ATL_tgemm_ammG_t *pd,             /* problem definition */
   unsigned const int rank,          /* my (virtual) rank */
   unsigned const int dblk,          /* CpyBlk of C to compute */
   const size_t i,                   /* row index to start at */
   const size_t j,                   /* col index to start at */
   unsigned int kblk,                /* kblk to compute */
   unsigned const int mb,            /* actual sz of M w/o padding */
   unsigned const int nb,            /* actual sz of N w/o padding */
   unsigned const int nmu,           /* CEIL(mb/mu) */
   unsigned const int nnu,           /* CEIL(nb/nu) */
   ammkern_t amm,                    /* kern to use for all non-kb0 blocks */
   TYPE *wC                          /* my private C workspace */
)
{
   int kb = pd->kb, KB0=kb;
   const TYPE *A = pd->A;
   const TYPE *B = pd->B;
   TYPE *wA, *wB;
   size_t k;

   if (kblk)
      k = pd->kb0 + (kblk-1)*kb;
   else
   {
      k = 0;
      if (kb != pd->kb0)
      {
         kb = pd->kb0;
         KB0 = pd->KB0;
         if (amm == pd->amm_b0)
            amm = pd->ammK_b0;
         else
            amm = pd->ammK_b1;
      }
   }

   A += (pd->TA) ? k + i*pd->lda : i + k*pd->lda;
   B += (pd->TB) ?  j + k*pd->ldb : k + j*pd->ldb;
   wA = pd->wA + kblk*pd->blkszA + pd->panszA * dblk;
   wB = pd->wB + kblk*pd->blkszB + pd->panszB * dblk;
   pd->a2blk(kb, mb, pd->alpA, A, pd->lda, wA);
   pd->b2blk(kb, nb, pd->alpB, B, pd->ldb, wB);
/*
 * If I just finished copying the K-panel for both A & B, let folks know
 */
   if (ATL_DecGlobalAtomicCountDown((pd->KdonCtr)[dblk], rank) == 1)
   {
      ATL_mutex_lock(pd->cpmut);
      ATL_SetBitBV(pd->cpyAdBV, dblk);
      if (ATL_FindFirstUnsetBitBV(pd->cpyAdBV, 0) == -1)
         pd->cpyAdone = 1;

      ATL_SetBitBV(pd->cpyBdBV, dblk);
      if (ATL_FindFirstUnsetBitBV(pd->cpyBdBV, 0) == -1)
         pd->cpyBdone = 1;
      ATL_mutex_unlock(pd->cpmut);
   }
   amm(nmu, nnu, KB0, wA, wB, wC, wA, wB, wC);
}
/*
 * This routine computes one block of C, and writes answer back out to C
 */
static void DoBlksWtCpy
(
   ATL_tgemm_ammG_t *pd,             /* problem definition */
   unsigned const int rank,          /* my (virtual) rank */
   unsigned const int dblk,          /* CpyBlk of C to compute */
   unsigned int kctr,                /* non-zero KbegCtr */
   TYPE *wC                          /* my private C workspace */
)
{
   const int nmblks=pd->nmblks, nnblks=pd->nnblks, nkblks=pd->nkblks; 
   const int minblks=Mmin(nmblks, nnblks);
   const int mb = (dblk >= nmblks-1) ? pd->mbf : pd->mb;
   const int nb = (dblk >= nnblks-1) ? pd->nbf : pd->nb;
   const int nmu = (dblk >= nmblks-1) ? pd->nmuf : pd->nmu;
   const int nnu = (dblk >= nnblks-1) ? pd->nnuf : pd->nnu;
   size_t i, j;
   unsigned int kblk = pd->nkblks - kctr;
   TYPE *wA, *wB, *c;
   TYPE beta = ATL_rone;

   i = (dblk < nmblks) ? dblk : nmblks-1;
   j = (dblk < nnblks) ? dblk : nnblks-1;
//fprintf(stderr, "%d: (%d,%d)\n", __LINE__, i, j);
   i *= pd->mb;
   j *= pd->nb;
   if (dblk < minblks)
      DoBlkWtCpy(pd, rank, dblk, i, j, kblk, mb, nb, nmu, nnu, pd->amm_b0, wC);
   else
      DoCpy(pd, rank, dblk, i, j, kblk, mb, nb);
   while (kctr = ATL_DecGlobalAtomicCount((pd->KbegCtr)[dblk], rank))
   {
      kblk = nkblks - kctr;
      if (dblk < minblks)
         DoBlkWtCpy(pd, rank, dblk, i, j, kblk, mb, nb, nmu, nnu, 
                    pd->amm_b1, wC);
      else
         DoCpy(pd, rank, dblk, i, j, kblk, mb, nb);
   }
/*
 * Seize mutex for block of original C, and add my part out
 */
   if (dblk < minblks)
   {
      c = pd->C + j*pd->ldc + i;
      ATL_mutex_lock(pd->Cmuts[dblk]);
      if (ATL_IsBitSetBV(pd->cbetaBV, dblk))
         pd->blk2c_b1(mb, nb, pd->alpC, wC, ATL_rone, c, pd->ldc);
      else  /* I need to apply beta */
      {
         ATL_SetBitBV(pd->cbetaBV, dblk); /* other thrs shouldn't apply beta */
         pd->blk2c(mb, nb, pd->alpC, wC, pd->beta, c, pd->ldc);
      }
      ATL_mutex_unlock(pd->Cmuts[dblk]);
   }
}

static void DoCopyBlksG(ATL_tgemm_ammG_t *pd, int rank, int vrank, TYPE *wC)
{
   int NEWBLK=1;
   const int ND = pd->nMNblks;
/*
 * Keep going as long as there is copy work to be done
 */
   while (!pd->NOCPWORK)
   {
       int d=0, k;
/*
 *     Find which diagonal block to work on, and then which k blk to use
 */
       if (NEWBLK)
       {
          d = ATL_DecGlobalAtomicCount(pd->ccCtr, vrank);
          if (d)
          {
             k = ATL_DecGlobalAtomicCount((pd->KbegCtr)[ND-d], rank);
             if (!k)     /* if no more K work to do */
                d = 0;   /* can't work on this diag after all */
          }
       }
/*
 *     If all diagonal blocks currently being worked on by threads, find
 *     one that I can help with.
 */
       if (!d)
       {
          unsigned int i;
          NEWBLK = 0;
          for (i=0; i < ND; i++)
          {
             unsigned int j = (i+vrank)%ND;
             k = ATL_DecGlobalAtomicCount((pd->KbegCtr)[j], vrank);
             d = ND-j;
             if (k)
                goto FOUNDDK;
          }
          pd->NOCPWORK = 1;   /* no work left to assign */
          return;             /* so done with copy of A&B and this func */
       }
/*
 *     If I reach here, I've got a valid d & k;  and I'll call a routine
 *     that continues to grab blocks from this diag along K until all K
 *     is done; it will then write the answer back to the original C, and
 *     return to this loop to see if it can help with another diag.
 */
       FOUNDDK:
          DoBlksWtCpy(pd, vrank, ND-d, k, wC);
   }
}

/*
 * RETURNS: undone C block that has had its A and B k-panels copied or:
 *          -1: no work left to do;     -2: no work available
 * NOTE: this routine only called when A/B copying is still ongoing!
 */
static int FindCblk(const int rank, ATL_tgemm_ammG_t *pd)
{
   unsigned const int ncblks=pd->nCblks, nmblks=pd->nmblks, nnblks=pd->nnblks;
   int bb=0, ib;
   do
   {
      int i, j;
      ib = ATL_FindFirstUnsetBitBV(pd->cCblkBV, bb);
      if (ib == -1)
         return(-1);
      j = ib / nmblks;
      i = ib - j*nmblks;
      if (pd->cpyAdone || ATL_IsBitSetBV(pd->cpyAdBV, i))
      {
         if (pd->cpyBdone || ATL_IsBitSetBV(pd->cpyBdBV, j))
            return(ib);
      }
      bb = ib+1;
   }
   while(bb < ncblks);
   return(-2);
}

/*
 * RETURNS: C block that has had its A and B k-panels copied
 */
static INLINE int GetCblk(const int rank, ATL_tgemm_ammG_t *pd)
{
   const int ncblks = pd->nCblks;
   int ib;
/*
 * If we are done copying, then use the counter to get the C block to work on
 * NOTE: thread that sets cpy[A,B]done must have cpmut to avoid race cond!
 */
   if (pd->cpyAdone & pd->cpyBdone)
   {
      do
      {
         ib = ATL_DecGlobalAtomicCount(pd->cCtr, rank);
         if (!ib)
            return(-1);
         ib = ncblks - ib;
      }
      while (ATL_IsBitSetBV(pd->cCblkBV, ib));
      return(ib);
   }
/*
 * If we reach here, we must assign C work based on dep info in cpy[A,B]dBV
 */
   else
   {
      while(1)
      {
         ib = FindCblk(rank, pd);
/*
 *       If we have a candidate block, grab the mutex and make sure one is
 *       still there
 */
         if (ib >= 0)
         {
            ATL_mutex_lock(pd->cpmut);
/*
 *          If copy finished while locking, call ourselves to get fast answer
 */
            if (pd->cpyAdone & pd->cpyBdone)
            {
               ATL_mutex_unlock(pd->cpmut);
               return(GetCblk(rank, pd));
            }
/*
 *          Otherwise, see if we've still got a candidate block
 */
            ib = FindCblk(rank, pd);
            if (ib >= 0)  /* we've got a block! */
            {
               ATL_SetBitBV(pd->cCblkBV, ib);  /* claim the block */
               ATL_mutex_unlock(pd->cpmut);
               return(ib);
            }
            ATL_mutex_unlock(pd->cpmut);
         }
         if (ib == -1)
            return(-1);
/*
 *       If no work available, pause before trying again
 */
         if (ib == -2)
            ATL_thread_yield();
      }
   }
}

/*
 * Computes (i,j) block of C whose data has previously been copied
 */
static void DoCblk(ATL_tgemm_ammG_t *pd, int rank, TYPE *wC, int i, int j)
{
   const unsigned int szA=pd->blkszA, szB=pd->blkszB, nkblks=pd->nkblks,
      kb=pd->kb;
   unsigned int nmu, nnu, mb, nb, k;
   const TYPE *wA = pd->wA + i*pd->panszA;
   const TYPE *wB = pd->wB + j*pd->panszB;
   const TYPE *wAn = wA+szA, *wBn = wB+szB;
   const ammkern_t amm = pd->amm_b1;

   #ifdef DEBUG2
     /* fprintf(stderr, "%d: DoCblk, (%d,%d)\n", __LINE__, i, j); */
      ATL_assert(ATL_IsBitSetBV(pd->cpyAdBV, i));
      ATL_assert(ATL_IsBitSetBV(pd->cpyBdBV, j));
   #endif
   if (i != pd->nmblks-1)
   {
      nmu = pd->nmu;
      mb = pd->mb;
   }
   else
   {
      nmu = pd->nmuf;
      mb = pd->mbf;
   }
   if (j != pd->nnblks-1)
   {
      nnu = pd->nnu;
      nb = pd->nb;
   }
   else
   {
      nnu = pd->nnuf;
      nb = pd->nbf;
   }
   pd->ammK_b0(nmu, nnu, pd->KB0, wA, wB, wC, wAn, wBn, wC);
   for (k=1; k < nkblks; k++)
   {
      wA = wAn;
      wB = wBn;
      wAn += szA;
      wBn += szB;
      amm(nmu, nnu, kb, wA, wB, wC, wAn, wBn, wC);
   }
   pd->blk2c(mb, nb, pd->alpC, wC, pd->beta, pd->C+pd->ldc*j*pd->nb+i*pd->mb,
             pd->ldc);
}

static void DoNoCopyBlksG(ATL_tgemm_ammG_t *pd, int rank, int vrank, TYPE *wC)
{
   const unsigned int nmblks = pd->nmblks;
   int ic;
   while ((ic = GetCblk(rank, pd)) != -1)
   {
      int i, j;
      j = ic / nmblks;
      i = ic - j*nmblks;
      DoCblk(pd, vrank, wC, i, j);
   }
}

void Mjoin(PATL,DoWork_ammG)(void *vpp, int rank, int vrank)
{
   ATL_tpool_t *pp=vpp;
   ATL_tgemm_ammG_t *pd = pp->PD;
   TYPE *myC = pd->wC + vrank*pd->blkszC;
   if (!pd->NOCPWORK)
      DoCopyBlksG(pd, rank, vrank, myC);
   DoNoCopyBlksG(pd, rank, vrank, myC);
}
/*
 * This one can be used anytime C is large enough to provide parallelism.
 * It tries to copy all of A & B up front, so recursion may be needed to
 * use it for large matrices.
 * RETURNS: 0 if it did the operation, non-zero if it did not
 */
int Mjoin(PATL,tammm_G)
(
   enum ATLAS_TRANS TA,
   enum ATLAS_TRANS TB,
   ATL_CINT M,
   ATL_CINT N,
   ATL_CINT K,
   const SCALAR alpha,
   const TYPE *A,
   ATL_CINT lda,
   const TYPE *B,
   ATL_CINT ldb,
   const SCALAR beta,
   TYPE *C,
   ATL_CINT ldc
)
{
   int ncblks, nmblks, nnblks, nkblks, nMNblks, mb, nb, kb, mu, nu, ku; 
   int kb0, KB0, P, nkcnt, i, j, k;
   size_t sz, szA, szB;
   ATL_tgemm_ammG_t pd;
   amminfo_t mminfo;
   void *vp;

   nmblks = M / ATL_AMM_66MB;
   nnblks = N / ATL_AMM_66NB;
   ncblks = sz = ((size_t)nmblks) * nnblks;
   if (ncblks != sz)   /* too many blocks to safely count */
      return(1);       /* so tell caller to recur or call something else */
/*
 * Quick exit for problems too small to thread
 */
   if (ncblks < 8)
      return(Mjoin(PATL,ammm)(TA, TB, M, N, K, alpha, A, lda, B, ldb, 
                              beta, C, ldc));
   mb = Mjoin(PATL,tGetAmmmInfo)(&mminfo, Mmin(ncblks, ATL_NTHREADS), TA,
                                 TB, M, N, K, alpha, beta);
   pd.alpA = pd.alpB = pd.alpC = ATL_rone;
   if (!mb)
      pd.alpA = alpha;
   else if (mb == 1)
      pd.alpB = alpha;
   else
      pd.alpC = alpha;
   pd.beta = beta;
   pd.blk2c = mminfo.Cblk2cm;
   pd.blk2c_b1 = mminfo.Cblk2cm_b1;
   pd.a2blk = mminfo.a2blk;
   pd.b2blk = mminfo.b2blk;
   pd.mb = mb = mminfo.mb;
   pd.nb = nb = mminfo.nb;
   pd.kb = kb = mminfo.kb;
   mu = mminfo.mu;
   nu = mminfo.nu;
   ku = mminfo.ku;
   pd.nmu = mb/mu;
   pd.nnu = nb/nu;
   pd.nmblks = nmblks = (M+mb-1)/mb;
   pd.nnblks = nnblks = (N+nb-1)/nb;
   pd.nkblks = nkblks = (K+kb-1)/kb;
   pd.nCblks = sz = ((size_t)nmblks)*nnblks;
   if (pd.nCblks != sz)
      return(1);
   P = Mmin(ATL_NTHREADS,pd.nCblks);
   pd.nMNblks = nMNblks = Mmax(nmblks, nnblks);
   sz = nmblks * mb;
   pd.mbf = (sz == M) ? mb : M+mb-sz;
   sz = nnblks * nb;
   pd.nbf = (sz == N) ? nb : N+nb-sz;
   pd.nmuf = (pd.mbf+mu-1)/mu;
   pd.nnuf = (pd.nbf+nu-1)/nu;
   pd.blkszA = mb * kb;
   pd.blkszB = nb * kb;
   pd.blkszC = mb * nb;
   KB0 = kb0 = K - (K/kb)*kb;
   if (!kb0)
   {
      kb0 = KB0 = kb;
      pd.ammK_b0 = mminfo.amm_b0;
      pd.ammK_b1 = mminfo.amm_b1;
   }
   else
   {
      #if ATL_AMM_MAXKMAJ > 1
         if (ATL_AMMFLG_KMAJOR(mminfo.flag))
         {
            KB0 = ((kb0+ku-1)/ku)*ku;
            if (ATL_AMMFLG_KRUNTIME(mminfo.flag))
            {
               pd.ammK_b0 = mminfo.amm_b0;
               pd.ammK_b1 = mminfo.amm_b1;
            }
            else
            {
               pd.ammK_b0 = mminfo.amm_k1_b0;
               pd.ammK_b1 = mminfo.amm_k1_b1;
            }
         }
         else
      #endif
      {
         if (ATL_AMMFLG_KRUNTIME(mminfo.flag) && kb0 == (kb0/ku)*ku &&
             kb0 > mminfo.kbmin)
         {
            pd.ammK_b0 = mminfo.amm_b0;
            pd.ammK_b1 = mminfo.amm_b1;
         }
         else
         {
            pd.ammK_b0 = mminfo.amm_k1_b0;
            pd.ammK_b1 = mminfo.amm_k1_b1;
         }
      }
   }
   pd.kb0 = kb0;
   pd.KB0 = KB0;
   pd.amm_b0 = mminfo.amm_b0;
   pd.amm_b1 = mminfo.amm_b1;
   szA = nmblks*nkblks*pd.blkszA;
   szB = nnblks*nkblks*pd.blkszB;
   sz = nMNblks*3*sizeof(void*);    /* for K[beg,don]Ctr & Cmuts arrays */
   sz += ATL_MulBySize(pd.blkszC)*P;   /* local C wrkspc */
   sz += ATL_MulBySize(szA+szB);              /* A/B workspace */
   sz += 3*ATL_Cachelen;                      /* room for alignment */
   #ifdef ATL_PHI_SORTED
      if (pd.ncntxts > 1)
         sz += ATL_Cachelen*(pd.ncores+1);
   #endif
   if ((sz>>20) > ATL_PTMAXMALLOC_MB)
      return(2);
   vp = malloc(sz);
   if (!vp)
      return(2);
   #ifdef ATL_PHI_SORTED
   if (pd.ncntxts > 1)
   {
      pd.chkin = (volatile int*)ATL_AlignPtr(vp);
      pd.KbegCtr = (void*)(((char*)pd.chkin)+pd.ncores*ATL_Cachelen);
      for (i=0; i < pd.ncores; i++)
      {
         int *ip;
         char *cp = (char*)pd.chkin;

         cp += i*ATL_Cachelen;
         ip = (int*) cp;
         *ip = ip[1] = ip[2] = ip[3] = -2;
      }
   }
   else
   #endif
      pd.KbegCtr = vp;
   pd.cpyAdone = pd.cpyBdone = pd.NOCPWORK = 0;
   pd.TA = (TA == AtlasTrans);
   pd.TB = (TB == AtlasTrans);
   pd.panszA = nkblks*pd.blkszA;
   pd.panszB = nkblks*pd.blkszB;
   pd.ccCtr = ATL_SetGlobalAtomicCount(ATL_EstNctr(nMNblks, P),nMNblks, 0);
   pd.cCtr = ATL_SetGlobalAtomicCount(ATL_EstNctr(pd.nCblks, P),pd.nCblks, 0);
   nkcnt = ATL_EstNctr(nkblks, P);
   pd.KdonCtr = pd.KbegCtr + nMNblks;
   pd.Cmuts = pd.KdonCtr + nMNblks;
   pd.wA = (TYPE*)(pd.Cmuts+nMNblks);
   pd.wA = ATL_AlignPtr(pd.wA);
   pd.wB = pd.wA + szA;
   pd.wB = ATL_AlignPtr(pd.wB);
   pd.wC = pd.wB + szB;
   pd.wC = ATL_AlignPtr(pd.wC);
   pd.A = A;
   pd.B = B;
   pd.lda = lda;
   pd.ldb = ldb;
   pd.C = C;
   pd.ldc = ldc;
   for (i=0; i < nMNblks; i++)
   {
      pd.KbegCtr[i] = ATL_SetGlobalAtomicCount(nkcnt, nkblks, 0);
/*
 *    This blows up contention, but needed for correctness until we add
 *    ability to have last non-zero be 1!
 */
      pd.KdonCtr[i] = ATL_SetGlobalAtomicCountDown(nkcnt, nkblks);
      pd.Cmuts[i] = ATL_mutex_init();
   }
   pd.cpmut = ATL_mutex_init();
   pd.cpyAdBV = ATL_NewBV(nmblks);
   pd.cpyBdBV = ATL_NewBV(nnblks);
   pd.cCblkBV = ATL_NewBV(pd.nCblks);
   pd.cbetaBV = ATL_NewBV(nMNblks);
/*
 * Initialize cCblkBV so that all diagonal blocks are shown as already complete
 * This BV is used for assigning work to non-copy blocks.
 */
   k = Mmin(nmblks, nnblks);
   for (i=0; i < k; i++)
      ATL_SetBitBV(pd.cCblkBV, i*nmblks+i);
   k = Mmin(nmblks, nnblks);

//   #define DEBUG1
   #ifdef DEBUG1
   {
      ATL_tpool_t *pp=ATL_TP_PTR;
      if (!pp)
         pp = ATL_NewThreadPool(1, 0, NULL);
      ATL_assert(pp);
      pp->PD = &pd;
      Mjoin(PATL,DoWork_ammG)(pp, 0, 0);
      if (pp != ATL_TP_PTR)
         ATL_FreeThreadPool(pp);
   }
   #else
      ATL_goParallel(P, Mjoin(PATL,DoWork_ammG), NULL, &pd, NULL);
   #endif
/*
 * Free allocated structures and return;
 */
   ATL_FreeBV(pd.cpyAdBV);
   ATL_FreeBV(pd.cpyBdBV);
   ATL_FreeBV(pd.cCblkBV);
   ATL_FreeBV(pd.cbetaBV);
   ATL_mutex_free(pd.cpmut);
   ATL_FreeGlobalAtomicCount(pd.cCtr);
   ATL_FreeGlobalAtomicCount(pd.ccCtr);
   for (i=0; i < nMNblks; i++)
   {
      ATL_FreeGlobalAtomicCount(pd.KbegCtr[i]);
      ATL_FreeGlobalAtomicCountDown(pd.KdonCtr[i]);
      ATL_mutex_free(pd.Cmuts[i]);
   }
   free(vp);
   return(0);
}
@ROUT ATL_tgemm_amm
/*
 * recurs on any standard GEMM interface routine, passed as a function
 * pointer in amm.
 */
int Mjoin(PATL,ammm_REC)
(
   enum ATLAS_TRANS TA,
   enum ATLAS_TRANS TB,
   ATL_CINT M,
   ATL_CINT N,
   ATL_CINT K,
   const SCALAR alpha,
   const TYPE *A,
   ATL_CINT lda,
   const TYPE *B,
   ATL_CINT ldb,
   const SCALAR beta,
   TYPE *C,
   ATL_CINT ldc,
   int (*amm)(enum ATLAS_TRANS,enum ATLAS_TRANS, ATL_CINT, ATL_CINT, ATL_CINT,
              const SCALAR, const TYPE*, ATL_CINT,  const TYPE*, ATL_CINT, 
              const SCALAR, TYPE*, ATL_CINT)
)
{
   if (amm(TA, TB, M, N, K, alpha, A, lda, B, ldb, beta, C, ldc))
   {
/*
 *    Stopping criteria in case something is horribly wrong
 */
      if (K <= ATL_AMM_MAXKB && M <= ATL_AMM_MAXMB && N <= ATL_AMM_MAXNB)
         return(Mjoin(PATL,ammm)(TA, TB, M, N, K, alpha, A, lda, B, ldb, 
                                 beta, C, ldc));
/*
 *    Divide K first: it cuts space from both A & B
 */
      if (K+K >= Mmax(M,N))
      {
         const int KL = K>>1, KR = K-KL;
         ATL_assert(!Mjoin(PATL,ammm_REC)(TA, TB, M, N, KL, alpha, A, lda, 
                                          B, ldb, beta, C, ldc, amm));
         A += (TA == AtlasNoTrans) ? KL*lda : KL;
         B += (TB == AtlasNoTrans) ? KL : KL*ldb;
         return(Mjoin(PATL,ammm_REC)(TA, TB, M, N, KR, alpha, A, lda, 
                                     B, ldb, ATL_rone, C, ldc, amm));
      }
/*
 *    If M largest dim (twice K), cut it instead
 */
      else if (M >= N)
      {
         const int ML = M>>1, MR = M-ML;
         ATL_assert(!Mjoin(PATL,ammm_REC)(TA, TB, ML, N, K, alpha, A, lda, 
                                          B, ldb, beta, C, ldc, amm));
         A += (TA == AtlasNoTrans) ? ML : ML*lda;
         return(Mjoin(PATL,ammm_REC)(TA, TB, MR, N, K, alpha, A, lda, 
                                     B, ldb, beta, C+ML, ldc, amm));
      }
/*
 *    Otherwise, cut N
 */
      else
      {
         const int NL = N>>1, NR = N-NL;
         ATL_assert(!Mjoin(PATL,ammm_REC)(TA, TB, M, NL, K, alpha, A, lda, 
                                          B, ldb, beta, C, ldc, amm));
         B += (TB == AtlasNoTrans) ? NL*ldb : NL;
         return(Mjoin(PATL,ammm_REC)(TA, TB, M, NR, K, alpha, A, lda, 
                                     B, ldb, beta, C+NL*ldc, ldc, amm));
      }
   }
   return(0);
}

int Mjoin(PATL,tammm)
(
   enum ATLAS_TRANS TA,
   enum ATLAS_TRANS TB,
   ATL_CSZT M,
   ATL_CSZT N,
   ATL_CSZT K,
   const SCALAR alpha,
   const TYPE *A,
   ATL_CSZT lda,
   const TYPE *B,
   ATL_CSZT ldb,
   const SCALAR beta,
   TYPE *C,
   ATL_CSZT ldc
)
{
   if (N < ATL_AMM_MAXNB && K <= ATL_AMM_MAXKB && K > 2)
      return(Mjoin(PATL,tammm_tNK)(TA, TB, M, N, K, alpha, A, lda, B, ldb,
                                   beta, C, ldc));
   else if (M <= ATL_AMM_MAXMB && N <= ATL_AMM_MAXNB)
      return(Mjoin(PATL,tammm_tMN)(TA, TB, M, N, K, alpha, A, lda, B, ldb,
                                   beta, C, ldc));
   else if (M > ATL_AMM_MAXMB && K >= 16 && N > ATL_AMM_MAXNB)
      return(Mjoin(PATL,ammm_REC)(TA, TB, M, N, K, alpha, A, lda, B, ldb,
                                  beta, C, ldc, Mjoin(PATL,tammm_G)));
   return(1);
}
@ROUT ATL_tsyrk_amm
#include "atlas_misc.h"
#define ATL_ESTNCTR 1
#include "atlas_tlvl3.h"
#include "atlas_bitvec.h"
/* 
 * Translates C coordinate in lower triangular matrix to Cblk #.
 * nm is the number of Mblocks, 
 * (i,j) is the coordinate of the block
 */
#define Mcoord2cblk(i_, j_, nm_) ((((nm_)+(nm_)-j-1)*j)>>1 - (j_) + (i_) - 1)
/*
 * Translates block number b_ to (i,j) coordinates assuming first panel
 * has nm_ blocks in it (including diagonal block)
 */
#define Mcblk2coord(NM_, B_, I_, J_) \
{ \
   register int n_ = (NM_)-1, b_=(B_), j_; \
   for (j_=0; b_ >= n_; j_++) \
   { \
      b_ -= n_; \
      n_--; \
   } \
   (J_) = j_; \
   (I_) = j_ + b_ + 1; \
}

void Mjoin(PATL,CombSyrk_ammK)
(
   void *vp,          /* void ptr to ATL_GEMV_t struct given to threads */
   const int myrank,  /* my processor rank */
   const int hisrank  /* rank entry to be combined into mine */
)
{
   ATL_tsyrk_ammK_t *pd=vp;
   TYPE *myC, *hisC;
   const unsigned int incW = pd->kb*(pd->mb+pd->nb), N = pd->nb * pd->mb;

   myC = pd->w + pd->wsz*myrank + incW;
   hisC = pd->w + pd->wsz*hisrank + incW;
   Mjoin(PATL,axpy)(N, ATL_rone, hisC, 1, myC, 1);
}

static void DoSyrkK(unsigned int rank, ATL_tsyrk_ammK_t *pd)
{
   TYPE *wA, *wB, *wC;
   ammkern_t amm = pd->amm_b0;
   const cm2am_t a2blk=pd->a2blk, b2blk=pd->b2blk;
   const unsigned int mb=pd->mb, nb=pd->nb, kb=pd->kb, nkblks=pd->nkblks, 
      N=pd->N, nmu=pd->nmu, nnu=pd->nnu;
   const size_t mulA = (pd->TA) ? 1 : pd->lda;
   int lda=pd->lda;
   int kctr;
   wA = pd->w + pd->wsz*rank;
   wB = wA + mb*kb;
   wC = wB + nb*kb;

   while ((kctr = ATL_DecGlobalAtomicCount(pd->KbCtr, rank)))
   {
      const int kblk = nkblks - kctr;
      const TYPE *a = pd->A + ((size_t)kblk)*mulA*kb;
      if (kblk != nkblks-1)  /* normal full-kb operation */
      {
         a2blk(kb, N, ATL_rone, a, lda, wA);
         b2blk(kb, N, ATL_rone, a, lda, wB);
         amm(nmu, nnu, kb, wA, wB, wC, wA, wB, wC);
      }
      else /* last block of size kb0 */
      {
         a2blk(pd->kb0, N, ATL_rone, a, lda, wA);
         b2blk(pd->kb0, N, ATL_rone, a, lda, wB);
         if (amm == pd->amm_b0)
            pd->ammK_b0(nmu, nnu, pd->KB0, wA, wB, wC, wA, wB, wC);
         else
            pd->ammK_b1(nmu, nnu, pd->KB0, wA, wB, wC, wA, wB, wC);
      }
      amm = pd->amm_b1;
   }
/*
 * If I did no work, zero my wrkspace so I don't screw up combine!
 */
   if (amm != pd->amm_b1)
      Mjoin(PATL,zero)(mb*nb, wC, 1);
@beginskip
/*
 * If I did any work at all, must now write answer back to C
 */
   if (amm == pd->amm_b1)
   {
      TYPE *w = wC + mb*nb, *c = pd->C;
      TYPE beta=ATL_rone;
      pd->blk2c_b0(N, N, ATL_rone, wC, ATL_rzero, w, N);
/*
 *    Now, seize C mutex, and copy out only upper/lower portion
 */
      ATL_mutex_lock(pd->Cmut);
      if (!pd->BETA_APPLIED)
      {
         pd->BETA_APPLIED = 1;
         beta = *(pd->beta);
      }
      if (pd->LOWER)
      {
         int j;
         for (j=0; j < N; j++, c += pd->ldc+1, w += N+1)
             Mjoin(PATL,axpby)(N-j, *(pd->alpha), w, 1, beta, c, 1);
      }
      else
      {
         int j;
         for (j=0; j < N; j++, c += pd->ldc, w += N)
             Mjoin(PATL,axpby)(j+1, *(pd->alpha), w, 1, beta, c, 1);
      }
      ATL_mutex_unlock(pd->Cmut);
   }
@endskip
}

void Mjoin(PATL,DoWork_syrk_amm_K)(ATL_LAUNCHSTRUCT_t *lp, void *vp)
{
   ATL_thread_t *tp = vp;
   DoSyrkK(tp->rank, lp->opstruct);
}
int Mjoin(PATL,tsyrk_amm_K)
(
   const enum ATLAS_UPLO Uplo,
   const enum ATLAS_TRANS Trans,
   ATL_CINT N,
   ATL_CINT K,
   const SCALAR alpha,
   const TYPE *A,
   ATL_CINT lda,
   const SCALAR beta,
   TYPE *C,
   ATL_CINT ldc
)
{
   amminfo_t mminfo;
   ATL_tsyrk_ammK_t pd;
   ablk2cmat_t Mjoin(PATL,tGetSyammInfo_K)
      (amminfo_t *out, const int P, enum ATLAS_TRANS TA, ATL_CSZT N,ATL_CSZT K);
   int kb=ATL_AMM_MAXKB, nkb = K / ATL_AMM_MAXKB, P = ATL_NTHREADS;
   int ku, kr, mb, nb, mu, nu;
   size_t sz;
   void *vp=NULL;

   if (nkb < P)
   {
      kb = ATL_AMM_98KB;
      nkb = K / ATL_AMM_98KB;
      if (nkb < P)
      {
         nkb = K / ATL_AMM_66KB;
         kb = ATL_AMM_66KB;
      }
   }
   if (nkb < P)
   {
      if (nkb < 2)
      {
          Mjoin(PATL,syrk)(Uplo, Trans, N, K, alpha, A, lda, beta, C, ldc);
          return(0);
      }
      P = nkb;
   }
   pd.blk2c_b0 = Mjoin(PATL,tGetSyammInfo_K)(&mminfo, P, Trans, N, kb);
   kb = mminfo.kb;
   nkb = K / kb;
@skip   ATL_assert(kb == mminfo.kb);

   mu = mminfo.mu;
   nu = mminfo.nu;
   pd.nmu = (N+mu-1) / mu;
   pd.nnu = (N+nu-1) / nu;
   pd.mb = mb = pd.nmu*mu;
   pd.nb = nb = pd.nnu*nu;
   pd.kb = mminfo.kb;
   sz = ((((size_t)mb)*nb)<<1) + (mb+nb)*kb;
   pd.wsz = sz;
   sz = ATL_MulBySize(sz)*P;
   vp = malloc(sz+ATL_Cachelen);
   if (!vp)
      return(1);
   pd.w = ATL_AlignPtr(vp);
   kr = K - nkb*kb;
   pd.kb0 = pd.KB0 = kr;
   ku = mminfo.ku;
   if (!kr)
   {
      pd.kb0 = pd.KB0 = kb;
      pd.ammK_b0 = mminfo.amm_b0;
      pd.ammK_b1 = mminfo.amm_b1;
   }
   else
   {
      #if ATL_AMM_MAXKMAJ > 1
         if (ATL_AMMFLG_KMAJOR(mminfo.flag))
         {
            pd.KB0 = ((kr+ku-1)/ku)*ku;
            if (ATL_AMMFLG_KRUNTIME(mminfo.flag))
            {
               pd.ammK_b0 = mminfo.amm_b0;
               pd.ammK_b1 = mminfo.amm_b1;
            }
            else
            {
               pd.ammK_b0 = mminfo.amm_k1_b0;
               pd.ammK_b1 = mminfo.amm_k1_b1;
            }
         }
         else
      #endif
      {
         if (ATL_AMMFLG_KRUNTIME(mminfo.flag) && kr == (kr/ku)*ku &&
             kr > mminfo.kbmin)
         {
               pd.ammK_b0 = mminfo.amm_b0;
               pd.ammK_b1 = mminfo.amm_b1;
         }
         else
         {
               pd.ammK_b0 = mminfo.amm_k1_b0;
               pd.ammK_b1 = mminfo.amm_k1_b1;
         }
      }
   }
   pd.amm_b0 = mminfo.amm_b0;
   pd.amm_b1 = mminfo.amm_b1;
   pd.blk2c_b1 = mminfo.Cblk2cm;
   pd.a2blk = mminfo.a2blk;
   pd.b2blk = mminfo.b2blk;
   pd.A = A;
   pd.C = C;
   pd.alpha = &alpha;
   pd.beta = &beta;
   pd.nkblks = (kr) ? nkb+1 : nkb;
   pd.KbCtr = ATL_SetGlobalAtomicCount(ATL_EstNctr(pd.nkblks, P), pd.nkblks, 0);
   pd.Cmut = ATL_mutex_init();
   pd.BETA_APPLIED = SCALAR_IS_ONE(beta);
   pd.LOWER = (Uplo == AtlasLower);
   pd.TA = (Trans == AtlasTrans);
   pd.N = N;
   pd.lda = lda;
   pd.ldc = ldc;

//   #define DEBUG1 1
   #ifdef DEBUG1
   {
      ATL_LAUNCHSTRUCT_t ls;
      ATL_thread_t ts;
      ts.rank = 0;
      ts.P = 1;
      ls.opstruct = &pd;
      Mjoin(PATL,DoWork_syrk_amm_K)(&ls, &ts);
   }
   #else
      ATL_goparallel(P, Mjoin(PATL,DoWork_syrk_amm_K), &pd, 
                     Mjoin(PATL,CombSyrk_ammK));
   #endif
/*
 * Answer is written back to rank0's workspace, extract it & write to C
 */
   {
      TYPE *wC = pd.w+kb*(mb+nb), *w = wC + mb*nb, *c = C;
/*
 *    Put it into block-major storage in w 
 */
      pd.blk2c_b0(N, N, ATL_rone, wC, ATL_rzero, w, N);
/*
 *    Now copy out only upper or lower portion
 */
      if (pd.LOWER)
      {
         int j;
         for (j=0; j < N; j++, c += ldc+1, w += N+1)
             Mjoin(PATL,axpby)(N-j, alpha, w, 1, beta, c, 1);
      }
      else
      {
         int j;
         for (j=0; j < N; j++, c += ldc, w += N)
             Mjoin(PATL,axpby)(j+1, alpha, w, 1, beta, c, 1);
      }
   }
   free(vp);
   ATL_mutex_free(pd.Cmut);
   ATL_FreeGlobalAtomicCount(pd.KbCtr);
   return(0);
}

/*
 * On PHI, we deal out 4 Cblks at once, so that contexts on same core share
 * either A (Upper) or B (Lower)
 */
#ifdef ATL_PHI_SORTED
   #define DEBUG2 1
/*
 * RETURNS: >=0 : block to work on, -1: no work left, -2, no work avail
 */
static int FindContextChunkC(ATL_tsyrk_ammN_t *pd)
{
/*
 * While we haven't finished copying all of A, must check dependencies
 * before giving out work
 */
   if (ATL_FindFirstUnsetBitBV(pd->cpydonBV, 0) != -1)
   {
      const unsigned int ndiag=pd->ndiag, ncblks=pd->ncblks, LOWER=pd->LOWER,
                         ncntxts=pd->ncntxts;
      int ib = -1, bb=0, i, j, k, mul;

      do
      {
         ib = ATL_FindFirstUnsetBitBV(pd->cblkBV, bb);
         if (ib == -1)
            return((bb) ? -2 : -1);
         Mcblk2coord(ndiag, ib, i, j);
         k = i-j-1;

         if (ncntxts != 3)
            mul = !(k&(ncntxts-1));
         else 
            mul = (k == (k/3)*3);
         if (mul && ATL_IsBitSetBV(pd->cpydonBV, i) &&
             ATL_IsBitSetBV(pd->cpydonBV, j))
         {
            int  nl = ndiag-i;
            nl = Mmin(nl, ncntxts);
/*
 *          If only one row/col left in dir of context, then OK
 */
            if (nl == 1)
               return(ib);
/*
 *          If >1 left, must check of 2nd context's operand ready as well
 */
            else
            {
               if (ATL_IsBitSetBV(pd->cpydonBV, i+1))
               {
                  if (nl == 2)
                     return(ib);
                  if (ATL_IsBitSetBV(pd->cpydonBV, i+2))
                  {
                     if (nl == 3)
                        return(ib);
                     if (ATL_IsBitSetBV(pd->cpydonBV, i+3))
                        return(ib);
                  }
               }
            }
         }     /* end ifs checking if A/B blocks have been copied */
         bb = ib + 1;
      }
      while (bb < ncblks);
      return(-2);
   }  /* end if I still am copying A */
/*
 * If everything has been copied, then just take the first available
 * chunk.  Since we have dealt out all other blocks in chunks, we know
 * any remaining chunks are full or are partial due to N
 */
   return(ATL_FindFirstUnsetBitBV(pd->cblkBV, 0));
}

static int GetCBlkChunk(const int rank, ATL_tsyrk_ammN_t *pd)
{
   int ib, bb=0, i, j, k;
   const unsigned int ndiag=pd->ndiag, ncblks=pd->ncblks, ncntxts=pd->ncntxts;
/*
 *  We will exit loop when we run out of panels (ib=-1), or we get a valid
 *  block (ib >= 0).  If we have a valid block, we'll also have cwmut locked.
 */
   while((ib = FindContextChunkC(pd)) != -1)
   {
      if (ib == -2)
      {
         ATL_thread_yield();
         continue;
      }
/*
 *    OK, there's a potential panel, so lock the data structure and if there
 *    is still a panel to be had, take it
 */
      ATL_mutex_lock(pd->cwmut);
      ib = FindContextChunkC(pd);
      if (ib >= 0)
         break;
      ATL_mutex_unlock(pd->cwmut);
   }   /* exit loop wt valid ib & cwmut
/*
 * If we are out of C blocks, inform thread (cwmut is not locked!)
 */
   if (ib == -1)  
      return(-1);
/*
 * Otherwise, we exited loop with ib>=0 and the cwmut locked
 * Lower deals out blks from column panel, Upper from row panel;
 * Get 4 of them in a row, unless we reach end of panel; in this case
 * 1-3 contexts will be inactive during the computation of the cblk
 * to avoid pollution of the cache
 */
   Mcblk2coord(ndiag, ib, i, j);
@skip   fprintf(stderr, "%d[%d]: ib=%d, i=%d, j=%d, cBV=%x\n", rank, __LINE__, ib, i, j, pd->cblkBV[2]);
   #ifdef DEBUG2
      ATL_assert(!ATL_IsBitSetBV(pd->cblkBV,ib));
   #endif
   ATL_SetBitBV(pd->cblkBV, ib);
   if (i+1 < ndiag)
   {
      #ifdef DEBUG2
         ATL_assert(!ATL_IsBitSetBV(pd->cblkBV,ib+1));
      #endif
      ATL_SetBitBV(pd->cblkBV, ib+1);
      if (i+2 < ndiag && ncntxts > 2)
      {
         #ifdef DEBUG2
            ATL_assert(!ATL_IsBitSetBV(pd->cblkBV,ib+2));
         #endif
         ATL_SetBitBV(pd->cblkBV, ib+2);
         if (i+3 < ndiag && ncntxts > 3)
         {
            #ifdef DEBUG2
               ATL_assert(!ATL_IsBitSetBV(pd->cblkBV,ib+3));
            #endif
            ATL_SetBitBV(pd->cblkBV, ib+3);
         }
      }
   }
@skip   fprintf(stderr, "%d[%d]: ib=%d, i=%d, j=%d, cBV=%x\n", rank, __LINE__, ib, i, j, pd->cblkBV[2]);
   ATL_mutex_unlock(pd->cwmut);
   return(ib);
}
#endif

static int GetCBlk(const int rank, ATL_tsyrk_ammN_t *pd)
{
   int ib, bb=0;
   const unsigned int ndiag=pd->ndiag, ncblks=pd->ncblks;
   do
   {
      int i, j, k;
/*
 *    If all copying has finished, we just take the first available block
 */
      if (ATL_FindFirstUnsetBitBV(pd->cpydonBV, 0) == -1)
         pd->cpydone = 1;
      if (pd->cpydone)
      {
         bb = ATL_FindFirstUnsetBitBV(pd->cblkBV, 0);
         if (bb == -1)
            return(-1);
         ATL_mutex_lock(pd->cwmut);
         ib = ATL_FindFirstUnsetBitBV(pd->cblkBV, bb);
         if (ib != -1)
            ATL_SetBitBV(pd->cblkBV, ib);
         ATL_mutex_unlock(pd->cwmut);   /* unlock mutex protecting cblkBV */
         return(ib);
      }
      ib = ATL_FindFirstUnsetBitBV(pd->cblkBV, bb);
      if (ib == -1)
      {
         if (!bb)
            return(-1);
         else
            bb = 0;
         continue;
      }
      bb = ib+1;          /* don't check this again until we cycle */
      if (bb == ncblks)   /* if at end of bitvec */
         bb = 0;          /* start looking again at beginning */
/*
 *    There's an unassigned block, check if it's A & A' blks have been copied
 */
      Mcblk2coord(ndiag, ib, i, j);
//      fprintf(stderr, "%d:ib=%d, i=%d, j=%d\n", rank, ib, i, j);
      if (!ATL_IsBitSetBV(pd->cpydonBV, i))
         continue;
      if (!ATL_IsBitSetBV(pd->cpydonBV, j))
         continue;
/*
 *    Dep look good, now seize mutex and take it for real
 */
      ATL_mutex_lock(pd->cwmut);
      k = ATL_FindFirstUnsetBitBV(pd->cblkBV, ib);
/*
 *    If nobody else got the proven-good block before me, take it
 */
      if (k == ib)
      {
         ATL_SetBitBV(pd->cblkBV, ib);  /* I take it */
         ATL_mutex_unlock(pd->cwmut);   /* unlock mutex protecting cblkBV */
         return(ib);                    /* caller can compute this block */
      }
      ATL_mutex_unlock(pd->cwmut);
   }
   while(1);
   return(-1);
}

/*
 * computes (i,j) non-diagonal block of C
 */
static void DoCblk(const int rank, ATL_tsyrk_ammN_t *pd, TYPE *wC, int i, int j)
{
   const ammkern_t amm = pd->amm_b1;
   const unsigned int nkblks=pd->nkblks, bs=pd->blkszA, kb=pd->kb, NB=pd->nb;
   unsigned int nmu, nnu, mb, nb;
   const TYPE *wA, *wB, *wAn, *wBn;
   TYPE *c;
   int k;

   if (!(pd->LOWER))
   {
      k = i;
      i = j;
      j = k;
   }
   if (j != pd->ndiag-1)
   {
      nnu = pd->nnu;
      nb = pd->nb;
   }
   else
   {
      nnu = pd->nnuf;
      nb = pd->nbf;
   }
   if (i != pd->ndiag-1)
   {
      nmu = pd->nmu;
      mb = pd->nb;
   }
   else
   {
      nmu = pd->nmuf;
      mb = pd->nbf;
   }
   wA = pd->wA + i*pd->panszA;
   wB = pd->wAt + j*pd->panszA;
   wA = pd->wA + i*pd->panszA;
   wB = pd->wAt + j*pd->panszA;
   wAn = wA + bs;
   wBn = wB + bs;
   #ifdef DEBUG2
      if (!ATL_IsBitSetBV(pd->cpydonBV, i) || !ATL_IsBitSetBV(pd->cpydonBV, j))
          fprintf(stderr, "%d: ndiag=%d, i=%d, j=%d\n", rank, pd->ndiag, i, j);
      ATL_assert(ATL_IsBitSetBV(pd->cpydonBV, i));
      ATL_assert(ATL_IsBitSetBV(pd->cpydonBV, j));
   #endif
   pd->ammK(nmu, nnu, pd->KB0, wA, wB, wC, wAn, wBn, wC);
   for (k=1; k < nkblks; k++)
   {
      wA = wAn;
      wB = wBn;
      wAn += bs;
      wBn += bs;
      amm(nmu, nnu, kb, wA, wB, wC, wAn, wBn, wC);
   }
   pd->blk2c(mb, nb, *(pd->alpha), wC, *(pd->beta), 
             pd->C+ NB*(j*(size_t)(pd->ldc) + i), pd->ldc);
}

#ifdef ATL_PHI_SORTED
static void DoNonDiagChunk(const int rank, ATL_tsyrk_ammN_t *pd)
{
   const unsigned int ndiag = pd->ndiag, LOWER=pd->LOWER, ncntxts=pd->ncntxts;
   const unsigned crank = rank / pd->ncores;  /* my context rank */
   int ic;

   TYPE *wC = pd->wC + (rank+rank)*(pd->nbnb);
   volatile int *chkin;

   ic = rank - crank*pd->ncores;  /* What core am I on? */
   chkin = (volatile int*) (((volatile char *)pd->chkin)+ATL_Cachelen*ic);
/*
 * Each core waits for all its contexts to show up before doing work, so that
 * we don't grab work well in advance of all contexts arriving
 * 0 makes sure everyone reads the initial chkin[0] before proceeding, since
 * the following sync switches who writes first
 */
   if (!crank)
   {
      chkin[4] = chkin[5] = chkin[6] = -1;
      if (ncntxts == 4)
      {
         while (chkin[1] != -1 || chkin[2] != -1 || chkin[3] != -1)
            ATL_thread_yield();
         *chkin = -1;
         while (chkin[4] != -8 || chkin[5] != -8 || chkin[6] != -8)
            ATL_thread_yield();
      }
      else if (ncntxts == 3)
      {
         while (chkin[1] != -1 || chkin[2] != -1)
            ATL_thread_yield();
         *chkin = -1;
         while (chkin[4] != -8 || chkin[5] != -8)
            ATL_thread_yield();
      }
      else /* if (ncntxts == 2) */
      {
         while (chkin[1] != -1)
            ATL_thread_yield();
         *chkin = -1;
         while (chkin[4] != -8)
            ATL_thread_yield();
      }
   }
   else
   {
      ic = chkin[crank] = -1;
      while (chkin[0] != -1)
         ATL_thread_yield();
      chkin[crank+3] = -8;
   }

//   fprintf(stderr, "%d: START DoNonDiag: ndiag=%d, ncblks=%d\n", 
//           rank, ndiag, pd->ncblks);
   while (1)
   {
      int i, j;

      if (!crank)
      {
         ic = GetCBlkChunk(rank, pd);
//         fprintf(stderr, "\n%d: NEW ic=%d\n", rank, ic);
         *chkin = ic;
         if (ncntxts == 4)
         {
            while (chkin[1] != ic || chkin[2] != ic || chkin[3] != ic)
               ATL_thread_yield();
         }
         else if (ncntxts == 3)
         {
            while (chkin[1] != ic || chkin[2] != ic)
               ATL_thread_yield();
         }
         else /* if (ncntxts == 3) */
         {
            while (chkin[1] != ic)
               ATL_thread_yield();
         }
      }
      else
      {
         while (chkin[0] == ic)
            ATL_thread_yield();
         chkin[crank] = ic = chkin[0];
      }
      if (ic == -1)
         return;
      Mcblk2coord(ndiag, ic, i, j);
      #ifdef DEBUG2
         ATL_assert(ATL_IsBitSetBV(pd->cblkBV, ic));
      #endif
      i += crank;
      if (i < ndiag)
         DoCblk(rank, pd, wC, i, j);
   }
}
#endif

/*
 * For non-diagonal work, we count the number of non-diagonal blocks of C,
 * which is initially stored in the ncblks variable, which is protected
 * the cwmut mutex, which also protects the cblkBV, which is a ncblk-len BV.
 * A unset bit means that particular non-diagonal block has not yet been
 * assigned to a thread, while a 1 means it has.  The mutex also protects
 * the cpydone variable, which is set to 1 when cpydonBV has all bits set.
 * So, threads wanting to do non-diagonal work will find the first unset
 * bit in cblkBV, and then translate that to a (i,j) C block coordinate.
 * If the ith & jth bits are both set in cpydonBV (or cpydone is set), then
 * they will take that block as their own to do (in this phase, each thread
 * gets an individual block of C to do) by setting the bit in cblkBV.
 * When all bits are set in cblkBV, then all work has been dealt out, and
 * threads will exit once they say there is no more work to do.
 * The master process joins all created threads, and can delete data structures
 * safely after all joins succeed.
 */
static void DoNonDiag(const int rank, ATL_tsyrk_ammN_t *pd)
{
   const unsigned int ndiag = pd->ndiag;
   int ic;
   TYPE *wC = pd->wC + (rank+rank)*(pd->nbnb);

//   fprintf(stderr, "%d: START DoNonDiag: ndiag=%d, ncblks=%d\n", 
//           rank, ndiag, pd->ncblks);
   while ((ic = GetCBlk(rank, pd)) != -1)
   {
      int i, j;
      Mcblk2coord(ndiag, ic, i, j);
//      fprintf(stderr, "%d: ic=%d, i=%d, j=%d\n", rank, ic, i, j);
      DoCblk(rank, pd, wC, i, j);
   }
}

static void DoBlkWtCpy
(
   unsigned const int rank,   /* my thread rank */
   ATL_tsyrk_ammN_t *pd,      /* problem def structure */
   unsigned const int dblk,   /* diagonal block of C to compute */
   unsigned const int kblk,   /* kblk to compute */
   unsigned const int b,      /* actual size of block w/o padding */
   unsigned const int nmu,    /* m/mu for this C blk */
   unsigned const int nnu,    /* n/nu for this C blk */
   ammkern_t amm,             /* kern to use for all non-kb0 blocks */
   TYPE *wC                   /* my private workspace */
)
{
   const int nb=pd->nb, kb=pd->kb;
   const TYPE *A;             /* ptr to original matrix */
   TYPE *wA, *wB;             /* ptr to copy space */
   size_t i, k;
   int kk;
// fprintf(stderr, "%d: dblk=%d, kblk=%d\n", rank, dblk, kblk);
   i = dblk * nb;
   k = (kblk) ? pd->kb0 + (kblk-1) * kb : 0;
   if (pd->TA)  /* A is transposed */
   {
      A = pd->A + k + i*pd->lda;
   }
   else        /* A is Notranspose */
   {
      A = pd->A + k*pd->lda + i;
   }
   wA = pd->wA + dblk*(pd->panszA) + kblk*(pd->blkszA);
   wB = pd->wAt + dblk*(pd->panszA) + kblk*(pd->blkszA);
   if (kblk)
   {
      pd->a2blk(kb, b, ATL_rone, A, pd->lda, wA);
      pd->b2blk(kb, b, ATL_rone, A, pd->lda, wB);
/*
 *    If there are any off-diag blks awaiting the copy, signal copy of this
 *    K-panel of A/A' is complete iff I just copied the last block
 */
      if (pd->ncblks)
      {
         if (ATL_DecGlobalAtomicCountDown((pd->KdonCtr)[dblk], rank) == 1)
         {
            ATL_mutex_lock(pd->cdmut);
            ATL_SetBitBV(pd->cpydonBV, dblk);
            ATL_mutex_unlock(pd->cdmut);
         }
      }
      amm(nmu, nnu, kb, wA, wB, wC, wA, wB, wC);
   }
   else
   {
      pd->a2blk(pd->kb0, b, ATL_rone, A, pd->lda, wA);
      pd->b2blk(pd->kb0, b, ATL_rone, A, pd->lda, wB);
/*
 *    If there are any off-diag blks awaiting the copy, signal copy of this
 *    K-panel of A/A' is complete iff I just copied the last block
 */
      if (pd->ncblks)
      {
         if (ATL_DecGlobalAtomicCountDown((pd->KdonCtr)[dblk], rank) == 1)
         {
            ATL_mutex_lock(pd->cdmut);
            ATL_SetBitBV(pd->cpydonBV, dblk);
            ATL_mutex_unlock(pd->cdmut);
         }
      }
      if (amm == pd->amm_b0)   /* ammK works for beta=0, so OK */
         pd->ammK(nmu, nnu, pd->KB0, wA, wB, wC, wA, wB, wC);
/*
 *    If using ammK in middle of operation, it handles only beta=0, so do
 *    operation in extra workspace and then add result into running sum
 */
      else
      {
         pd->ammK(nmu, nnu, pd->KB0, wA, wB, wC+pd->nbnb, wA, wB, wC+pd->nbnb);
         Mjoin(PATL,axpy)(pd->nbnb, ATL_rone, wC+pd->nbnb, 1, wC, 1);
      }
   }
}

static void DoBlksWtCpy
(
   unsigned const int rank,   /* my thread rank */
   ATL_tsyrk_ammN_t *pd,      /* problem def structure */
   unsigned const int dblk,   /* diagonal block of C to compute */
   unsigned int kctr,         /* non-zero Kbeg ctr */
   TYPE *wC                   /* my private workspace */
)
{
   const int TRANS = (pd->TA == AtlasTrans);
   int kblk = pd->nkblks - kctr;
   const int b = (dblk == pd->ndiag-1) ? pd->nbf : pd->nb;
@skip   const int m = (dblk == pd->ndiag-1) ? pd->Mf : pd->nb;
@skip   const int n = (dblk == pd->ndiag-1) ? pd->Nf : pd->nb;
   const int nmu = (dblk == pd->ndiag-1) ? pd->nmuf : pd->nmu;
   const int nnu = (dblk == pd->ndiag-1) ? pd->nnuf : pd->nnu;
   TYPE *wA, *wB, *w, *c;
   TYPE beta = ATL_rone;
   DoBlkWtCpy(rank, pd, dblk, kblk, b, nmu, nnu, pd->amm_b0, wC);
   while (kctr = ATL_DecGlobalAtomicCount((pd->KbegCtr)[dblk], rank))
   {
      kblk = pd->nkblks - kctr;
      DoBlkWtCpy(rank, pd, dblk, kblk, b, nmu, nnu, pd->amm_b1, wC);
   }
/*
 * Since I'm done with this blk of C, copy it to block-major storage, and
 * scale by alpha (blk2c won't access only triangle, so must cpy first)
 */
   w = wC + pd->nbnb;
   pd->blk2d(b, b, *(pd->alpha), wC, ATL_rzero, w, b);
/*
 * Now, seize mutex for diagonal block of original C, and copy back out
 * only above/below diagonal
 */
   c = pd->C + dblk*(pd->nb)*(pd->ldc+1);
   ATL_mutex_lock(pd->Cdmuts[dblk]);
   if (!ATL_IsBitSetBV(pd->dbetaBV, dblk)) /* if I apply beta */
   {
      ATL_SetBitBV(pd->dbetaBV, dblk); /* tell rest of thrs don't apply beta */
      beta = *(pd->beta);              /* because I'm going to do it */
   }
   if (pd->LOWER)
   {
      int j;
      for (j=0; j < b; j++, c += pd->ldc+1, w += b+1)
          Mjoin(PATL,axpby)(b-j, ATL_rone, w, 1, beta, c, 1);
   }
   else
   {
      int j;
      for (j=0; j < b; j++, c += pd->ldc, w += b)
          Mjoin(PATL,axpby)(j+1, ATL_rone, w, 1, beta, c, 1);
   }
   ATL_mutex_unlock(pd->Cdmuts[dblk]);
}

/*
 * In the first phase, we work only on diagonal blocks, while copying both
 * A & A'.  For diag work, we parallelize both N & K dims so that the copy
 * is done as quickly as possible.  Threads coming in first choose differing
 * diag blks; diagonal blocks are dealt out cheaply using the dCtr global
 * counter (which starts at nnblks == ndiag).
 * Once all diagonal blocks are dealt out, new threads will start using
 * the atomic ctr array KbegCtr array to share K work for each diagonal.
 * both KbegCtr & KdonCtr are nnblk-len arrays of atomic counters.  Each
 * counter starts at nkblks.  Once the block pointed to by KbegCtr is
 * completely copied, the copying array increments the KdonCtr.  Only one
 * core per diag will get KdonCtr == 0 after doing his copy, and this
 * core will seize cdmut mutex in order to set the appropriate bit in
 * cpydonBV, which is a nnblks-length bit vector.  If the kth bit is set,
 * that means the ith row of A & jth col of A' has been copied.
 * Once a thread gets KbegCtr for a particular diag of 0, it means there's
 * no more work for this block of C, and so it will seize the appropriate
 * Cdmuts mutex which protects each diagonal block of C, and write its
 * finished contribution out to C.  The first such thread to ever seize
 * the mutex will scope dbetaBV to find this diagonal block needs beta applied;
 * while later threads will use beta=1.
 * Eventually, all diagonal work is finished, and the first processor to
 * get 0 for all dCtr & KbegCtr requests will set NODWORK=1, so later
 * threads don't have to query all the counters to know they should proceed
 * to non-diagonal work.
 */
static void DoDiag(const int rank, ATL_tsyrk_ammN_t *pd)
{
   int DIAG=1, k;
   TYPE *myC = pd->wC + (rank+rank)*(pd->nbnb);
   while (!(pd->NODWORK))
   {
       int d=0;
       
/*
 *     Find which diagonal block to work on, and then which k blk to use
 */
       if (DIAG)
       {
          d = ATL_DecGlobalAtomicCount(pd->dCtr, rank);
          if (d)
          {
             k = ATL_DecGlobalAtomicCount((pd->KbegCtr)[pd->ndiag - d], rank);
             if (!k)     /* if no more K work to do */
                d = 0;   /* can't work on this diag after all */
          }
       }
/*
 *     If all diagonal blocks currently being worked on by threads, find
 *     one that I can help with.
 */
       if (!d)
       {
          unsigned int i, n=pd->ndiag;
          DIAG = 0;
          for (i=0; i < n; i++)
          {
             unsigned int j = (i+rank)%n;
             k = ATL_DecGlobalAtomicCount((pd->KbegCtr)[j], rank);
             d = n-j;
             if (k)
                goto FOUNDDK;
          }
          pd->NODWORK = 1;    /* no work left to assign */
          return;             /* so done with copy of A/A' & this func */
       }
/*
 *     If I reach here, I've got a valid d & k;  and I'll call a routine
 *     that continues to grab blocks from this diag along K until all K
 *     is done; it will then write the answer back to the original C, and
 *     return to this loop to see if it can help with another diag.
 */
       FOUNDDK:
          DoBlksWtCpy(rank, pd, pd->ndiag-d, k, myC);
   }
}

void Mjoin(PATL,DoWork_syrk_amm)(ATL_LAUNCHSTRUCT_t *lp, void *vp)
{
   ATL_thread_t *tp = vp;
   ATL_tsyrk_ammN_t *pd = lp->opstruct;
   if (!(pd->NODWORK))
      DoDiag(tp->rank, pd);
   if (pd->ncblks)
   {
      if (ATL_FindFirstUnsetBitBV(pd->cblkBV, 0) != -1)
      {
         #ifdef ATL_PHI_SORTED
         if (pd->ncntxts > 1)
            DoNonDiagChunk(tp->rank, pd);
         else
         #endif
         DoNonDiag(tp->rank, pd);
      }
   }
}
/*
 * SYRK where all parallelism comes from blocks of N, built atop amm directly
 *    if (TA == AtlasNoTrans) 
 *       C = alpha * A*A' + beta*C
 *    else
 *       C = alpha * A'*A + beta*C
 *    C is an upper or lower symmetric NxN matrix, 
 *    A is a dense rectangular NxK (NoTrans) or KxN (Trans) matrix
 * RETURNS: 0 if operation performed, non-zero otherwise.
 *   This routine assumes it can copy all of A up-front to simplify parallelism.
 *   Will return non-zero if memory cannot be allocated, on the assumption
 *   it is called from recursive implementation that can recur until malloc
 *   succeeds.
 */
int Mjoin(PATL,tsyrk_amm_N)
(
   const enum ATLAS_UPLO Uplo,
   const enum ATLAS_TRANS Trans,
   ATL_CINT N,
   ATL_CINT K,
   const SCALAR alpha,
   const TYPE *A,
   ATL_CINT lda,
   const SCALAR beta,
   TYPE *C,
   ATL_CINT ldc
)
{
   int NB, nb, mu, nu, ku, nfdiag, ndiag, nkblks, ncnt, nkcnt, blkszA, nbf;
   int ncblks, kb, kb0, KB0, i, P;
   size_t sz, szA;
   ATL_tsyrk_ammN_t pd;
   amminfo_t mminfo;
   void *vp;
   ablk2cmat_t Mjoin(PATL,tGetSyammInfo)
      (amminfo_t *out, const int P, enum ATLAS_TRANS TA, ATL_CSZT N, 
       ATL_CSZT K, const SCALAR alpha, const SCALAR beta);

   #ifdef ATL_AMM_98NB
      ncblks = N / ATL_AMM_98NB;
   #else
      ncblks = N / ATL_AMM_MAXNB;
   #endif
   ncblks = (ncblks*(ncblks-1))>>1;
   #ifdef ATL_PHI_SORTED
      if (ncblks >= ATL_NTHREADS)
      {
         P = ATL_NTHREADS;
         pd.ncores = ATL_NTHREADS>>2;
         pd.ncntxts = 4;
      }
      else if (ncblks+8 >= 3*(ATL_NTHREADS>>2))
      {
         P = 3*(ATL_NTHREADS>>2);
         pd.ncores = ATL_NTHREADS>>2;
         pd.ncntxts = 3;
      }
      else if (ncblks+4 >= (ATL_NTHREADS>>1))
      {
         P = ATL_NTHREADS>>1;
         pd.ncores = ATL_NTHREADS>>2;
         pd.ncntxts = 2;
      }
      else
      {
         pd.ncores = P = ncblks;
         pd.ncntxts = 1;
      }
   #else
      P = (ncblks < ATL_NTHREADS) ? ncblks : ATL_NTHREADS;
   #endif
   if (P < 2)
   {
      Mjoin(PATL,syrk)(Uplo, Trans, N, K, alpha, A, lda, beta, C, ldc);
      return(0);
   }
   pd.blk2d = Mjoin(PATL,tGetSyammInfo)(&mminfo, P, Trans, N, K, alpha, beta);
   pd.blk2c = mminfo.Cblk2cm;
   pd.a2blk = mminfo.a2blk;
   pd.b2blk = mminfo.b2blk;
   nb = mminfo.nb;
   kb = mminfo.kb;
   mu = mminfo.mu;
   nu = mminfo.nu;
   ku = mminfo.ku;
   nfdiag = N/nb;
   nbf = N - nfdiag*nb;
   ndiag = (nbf) ? nfdiag+1 : nfdiag;
   ncblks = ((ndiag-1)*ndiag)>>1;
/*
 * Set up parallel data structure
 */
   nkblks = (K+kb-1)/kb;
   KB0 = kb0 = K - (K/kb)*kb;
   if (!kb0)
   {
      kb0 = KB0 = kb;
      pd.ammK = mminfo.amm_b0;
   }
   else
   {
      #if ATL_AMM_MAXKMAJ > 1
         if (ATL_AMMFLG_KMAJOR(mminfo.flag))
         {
            KB0 = ((kb0+ku-1)/ku)*ku;
            if (ATL_AMMFLG_KRUNTIME(mminfo.flag))
               pd.ammK = mminfo.amm_b0;
            else
               pd.ammK = mminfo.amm_k1_b0;
         }
         else
      #endif
      {
         pd.ammK = mminfo.amm_k1_b0;
         if (ATL_AMMFLG_KRUNTIME(mminfo.flag) && kb0 == (kb0/ku)*ku &&
             kb0 > mminfo.kbmin)
            pd.ammK = mminfo.amm_b0;
      }
   }
   pd.amm_b0 = mminfo.amm_b0;
   pd.amm_b1 = mminfo.amm_b1;
   blkszA = kb * nb;
   szA = ndiag*nkblks*blkszA;                 /* space for A & A' */
   sz = (ndiag+ndiag+ndiag)*sizeof(void*);    /* space for Ctr & mut arrays */
   sz += ATL_MulBySize(nb)*nb*2*P; /* local C wrkspc */
   sz += ATL_MulBySize(szA+szA);              /* A/At workspace */
   sz += 3*ATL_Cachelen;                      /* room for alignment */
   #ifdef ATL_PHI_SORTED
      if (pd.ncntxts > 1)
         sz += ATL_Cachelen*(pd.ncores+1);
   #endif
   if ((sz>>20) > ATL_PTMAXMALLOC_MB)       /* if I'm over malloc limit */
      return(2);                            /* return and recur on K */
   vp = malloc(sz);
   if (!vp)
      return(2);
   #ifdef ATL_PHI_SORTED
   if (pd.ncntxts > 1)
   {
      pd.chkin = (volatile int*)ATL_AlignPtr(vp);
      pd.KbegCtr = (void*)(((char*)pd.chkin)+pd.ncores*ATL_Cachelen);
      for (i=0; i < pd.ncores; i++)
      {
         int *ip;
         char *cp = (char*)pd.chkin;

         cp += i*ATL_Cachelen;
         ip = (int*) cp;
         *ip = ip[1] = ip[2] = ip[3] = -2;
      }
   }
   else
   #endif
      pd.KbegCtr = vp;
   pd.alpha = &alpha;
   pd.beta = &beta;
   pd.TA = (Trans == AtlasTrans);
   pd.LOWER = (Uplo == AtlasLower);
   pd.ndiag = ndiag;
   pd.ncblks = ncblks;
   pd.blkszA = blkszA;
   pd.panszA = nkblks*blkszA;
   pd.nkblks = nkblks;
   pd.nb = nb;
   pd.nbnb = nb*nb;
   pd.kb = kb;
   pd.kb0 = kb0;
   pd.KB0 = KB0;
   pd.nnu = nb / nu;
   pd.nmu = nb / mu;
@beginskip
   pd.mu = mu;
   pd.nu = nu;
   pd.ku = ku;
@endskip
   if (nbf)
   {
      pd.nbf = nbf;
      pd.nmuf = (nbf+mu-1)/mu;
      pd.nnuf = (nbf+nu-1)/nu;
      pd.Mf = pd.nmuf*mu;
      pd.Nf = pd.nnuf*nu;
   }
   else
   {
      pd.nbf = pd.Mf = pd.Nf = nb;
      pd.nmuf = pd.nmu;
      pd.nnuf = pd.nnu;
   }
      
   pd.cpydone = pd.NODWORK = 0;
   pd.dCtr = ATL_SetGlobalAtomicCount(ATL_EstNctr(ndiag, P), ndiag, 0);
   nkcnt = ATL_EstNctr(nkblks, P);
   pd.KdonCtr = pd.KbegCtr + ndiag;
   pd.Cdmuts = pd.KdonCtr + ndiag;
   pd.wA = (TYPE*)(pd.Cdmuts+ndiag);
   pd.wA = ATL_AlignPtr(pd.wA);
   pd.wAt = pd.wA + szA;
   pd.wAt = ATL_AlignPtr(pd.wAt);

   pd.wC = pd.wAt + szA;
   pd.wC = ATL_AlignPtr(pd.wC);
   pd.A = A;
   pd.lda = lda;
   pd.C = C;
   pd.ldc = ldc;
   for (i=0; i < ndiag; i++)
   {
      pd.KbegCtr[i] = ATL_SetGlobalAtomicCount(nkcnt, nkblks, 0);
/*
 *    This blows up contention, but needed for correctness until we add
 *    ability to have last non-zero be 1!
 */
      pd.KdonCtr[i] = ATL_SetGlobalAtomicCountDown(nkcnt, nkblks);
      pd.Cdmuts[i] = ATL_mutex_init();
   }
   pd.cdmut = ATL_mutex_init();
   pd.cwmut = ATL_mutex_init();
   pd.cpydonBV = ATL_NewBV(ndiag);
   pd.dbetaBV = ATL_NewBV(ndiag);
   pd.cblkBV = ATL_NewBV(ncblks);
   pd.cbetaBV = ATL_NewBV(ncblks);

//   #define DEBUG1
   #ifdef DEBUG1
   {
      ATL_LAUNCHSTRUCT_t ls;
      ATL_thread_t ts;
      ts.rank = 0;
      ts.P = 1;
      ls.opstruct = &pd;
      Mjoin(PATL,DoWork_syrk_amm)(&ls, &ts);
   }
   #else
      ATL_goparallel(P, Mjoin(PATL,DoWork_syrk_amm), &pd, NULL);
   #endif
   #ifdef DEBUG2
      ATL_assert(ATL_FindFirstUnsetBitBV(pd.cblkBV, 0) == -1);
   #endif

/*
 * Free allocated structures and return; 
 */
   ATL_FreeBV(pd.cpydonBV);
   ATL_FreeBV(pd.cblkBV);
   ATL_FreeBV(pd.dbetaBV);
   ATL_FreeBV(pd.cbetaBV);
   ATL_mutex_free(pd.cdmut);
   ATL_mutex_free(pd.cwmut);
   ATL_FreeGlobalAtomicCount(pd.dCtr);
   for (i=0; i < ndiag; i++)
   {
      ATL_FreeGlobalAtomicCount(pd.KbegCtr[i]);
      ATL_FreeGlobalAtomicCountDown(pd.KdonCtr[i]);
      ATL_mutex_free(pd.Cdmuts[i]);
   }
   free(vp);
   return(0);
}

void Mjoin(PATL,tsyrk_amm)
   (const enum ATLAS_UPLO Uplo, const enum ATLAS_TRANS Trans, ATL_CINT N,
    ATL_CINT K, const SCALAR alpha, const TYPE *A, ATL_CINT lda,
    const SCALAR beta, TYPE *C, ATL_CINT ldc)

{
   if (N <= Mmax(ATL_AMM_MAXMB,ATL_AMM_MAXNB))
   {
      if (!Mjoin(PATL,tsyrk_amm_K)(Uplo, Trans, N, K, alpha, A, lda, 
                                   beta, C, ldc))
         return;
   }
/*
 *  Recur on K until tsyrk_amm can allocate enough space 
 */
    if (Mjoin(PATL,tsyrk_amm_N)(Uplo, Trans, N, K, alpha, A, lda, beta, C, ldc))
    {
       unsigned int kL = K>>1, kR=K-kL;
       const TYPE *a = (Trans == AtlasNoTrans) ? A+((size_t)lda)*kL : A+kL;
/*
 *     This stopping criteria should never happen, but it's here in case
 *     we have a system where you can't malloc much of anything, where we'll
 *     just try serial
 */
       if (kL < 32 || kR < 32)
       {
           Mjoin(PATL,syrk)(Uplo, Trans, N, K, alpha, A, lda, beta, C, ldc);
           return;
       }
       Mjoin(PATL,tsyrk_amm)(Uplo, Trans, N, kL, alpha, A, lda, beta, C, ldc);
       Mjoin(PATL,tsyrk_amm)(Uplo, Trans, N, kR, alpha, a, lda, 
                             ATL_rone, C, ldc);
    }
}
@ROUT ATL_threadpool
#include "atlas_threads.h"
#include "atlas_misc.h"
#include "atlas_bitvec.h"
#ifdef ATL_USE_THREADPOOL
/*
 * These are wrapperf for original codes, which expect to get a launchstruct
 * as the problem definition, and DoWork expects to get a pointer to my entry
 * in the thread array
 */
void ATL_oldjobwrap(void *vpp, int rank, int vrank)
{
   ATL_tpool_t *pp = vpp;
   ATL_LAUNCHSTRUCT_t *lp = pp->PD;
   void *vp = pp->threads[rank].vp;

   pp->threads[rank].vp = lp;
   lp->DoWork(lp, pp->threads+rank);
   pp->threads[rank].vp = vp;
}

void ATL_oldcombwrap(void *vpp, int rank, int vrank, int vhisrank)
{
   ATL_tpool_t *pp = vpp;
   ATL_LAUNCHSTRUCT_t *lp = pp->PD;

   if (lp->DoComb)
      lp->DoComb(lp->opstruct, vrank, vhisrank);
}

/*
 * Fully dynamic combine with restriction that iam=0 gets final answer
 * NOTE: assumed that during operation, have change tp->P and tp->rank
 *       to local values.  They will be restored to true rank & P here.
 */
static void ATL_dyncomb(ATL_tpool_t *pp, const int rank, const int vrank)
{
   const unsigned int P=pp->nworkers;
   ATL_thread_t *tp = pp->threads + rank;
/*
 * Let everyone know my data is ready for combining
 * Thread 0's work is not available because some combines expect
 * thread 0 to have the answer
 */
   if (vrank)
   {
      ATL_mutex_lock(pp->combmut);
      ATL_SetBitBV(pp->combReadyBV, vrank);
      ATL_mutex_unlock(pp->combmut);
   }
/*
 * Now participate in combine until everything is done, or someone
 * combines my stuff, and thus ends my participation
 */
   do
   {
      int d;
/*
 *    If my combDone bit is set, someone has combined my results: I'm finished
 */
      if (ATL_IsBitSetBV(pp->combDoneBV, vrank))
         return;
/*
 *    Seize mutex lock and do the work if still there & I'm still in game
 */
      ATL_mutex_lock(pp->combmut);
      if (ATL_IsBitSetBV(pp->combDoneBV, vrank)) /* I'm done */
      {
         ATL_mutex_unlock(pp->combmut);
         return;
      }
      d = ATL_FindFirstSetBitBV(pp->combReadyBV, 0);
      if (d == vrank)
         d = (vrank != P-1) ?  
            ATL_FindFirstSetBitBV(pp->combReadyBV, vrank+1) : -1;
      if (d != -1)  /* work still needs to be done */
      {
         ATL_SetBitBV(pp->combDoneBV, d);        /* combined thr is done */
         ATL_UnsetBitBV(pp->combReadyBV, d);     /* I took, no longer avail */
         ATL_UnsetBitBV(pp->combReadyBV, vrank); /* I'm not avail, working */
         ATL_mutex_unlock(pp->combmut);
         pp->combf(pp, rank, vrank, d);
/*
 *       After finishing my combine, my buffer ready for other's use
 */
         if (vrank)  /* thr 0 must get answer, so never available */
         {
            ATL_mutex_lock(pp->combmut);
            ATL_SetBitBV(pp->combReadyBV, vrank);   /* now avail again */
         }
/*
 *       Check if I'm the last guy left, if so, combine is complete
 *       Only vrank=0 allowed to be last guy (0's buff has final answer)
 */
         else  /* I'm 0, so see if combine is done */
         {
            ATL_mutex_lock(pp->combmut);
            if (ATL_FindFirstUnsetBitBV(pp->combDoneBV, 1) == -1)
            {
               ATL_SetBitBV(pp->combDoneBV, vrank);    /* I'm done */
               ATL_UnsetBitBV(pp->combReadyBV, vrank); /* so not ready */
               ATL_mutex_unlock(pp->combmut);
               return;
            }
         }
      }
      else if (vrank)  /* No work to do, leave to reduce lock contention */
      {
         ATL_mutex_unlock(pp->combmut);
         return;
      }
      ATL_mutex_unlock(pp->combmut);
   }     
   while(1);   /* end combine loop */
}

/*
 * This combine is for routines that require only leftward combination,
 * so that you can add results only from vranks greater (to the right)
 * than yourself, and then only if you have already added in all the
 * ranks in between yourself and the candidate.  This type of result
 * is naturally enforced by log2 reduction and linear sum-to-zero, which
 * some of the earlier tblas assume.
 */
static void ATL_leftcomb(ATL_tpool_t *pp, const int rank, const int vrank)
{
   const unsigned P = pp->nthr;
   const unsigned int nw = pp->nworkers;
   ATL_thread_t *tp = pp->threads + rank;
/*
 * Let everyone know my data is ready for combining
 * Thread 0's work is not available because some combines expect
 * thread 0 to have the answer
 */
   if (vrank)
   {
      ATL_mutex_lock(pp->combmut);
      ATL_SetBitBV(pp->combReadyBV, vrank);
      ATL_mutex_unlock(pp->combmut);
   }
/*
 * Now participate in combine until everything is done, or someone
 * combines my stuff, and thus ends my participation
 */
   do
   {
      int i, d;
/*
 *    If my combDone bit is set, somone has combined my results, so I'm finished
 */
      if (ATL_IsBitSetBV(pp->combDoneBV, vrank))
         return;
/*
 *    If everyone to right of me is finished, there's nothing left for me to do
 */
      for (i=vrank+1; i < nw && ATL_IsBitSetBV(pp->combDoneBV, i); i++);
      if (i == nw)
         return;
/*
 *    Sieze mutex lock and do the work if I'm still in the game
 */
      ATL_mutex_lock(pp->combmut);
      if (ATL_IsBitSetBV(pp->combDoneBV, vrank)) /* I'm done */
      {
         ATL_mutex_unlock(pp->combmut);
         return;
      }
/*
 *    Take buffers from as far right as possible, as they have least to do
 */
      for (i=nw-1; i > vrank; i--)
      {
/*
 *       If target is ready, there must be no unfinished nodes between us
 */
         if (ATL_IsBitSetBV(pp->combReadyBV, i)) /* got a candidate */
         {
            int j;
            for (j=vrank+1; j < i; j++)
               if (!ATL_IsBitSetBV(pp->combDoneBV, j)) 
                  break;
            if (j == i)  /* he's a legal partner */
               break;
         }
      }
      if (i != vrank)  /* work still needs to be done */
      {
         ATL_SetBitBV(pp->combDoneBV, i);        /* he's done, can quit */
         ATL_UnsetBitBV(pp->combReadyBV, i);     /* he's not avail, done  */
         ATL_UnsetBitBV(pp->combReadyBV, vrank); /* I'm not avail, working */
         ATL_mutex_unlock(pp->combmut);
         pp->combf(pp, rank, vrank, i);
/*
 *       After finishing my combine, my buffer ready for other's use
 */
         if (vrank)    /* rank=0 gets answer, so never available */
         {
            ATL_mutex_lock(pp->combmut);
            ATL_SetBitBV(pp->combReadyBV, vrank);   /* now avail again */
         }
/*
 *       Check if I'm the last guy left, if so, combine is complete
 *       Only vrank=0 allowed to be last guy (0's buff has final answer)
 */
         else  /* I'm 0, so see if combine is done */
         {
            ATL_mutex_lock(pp->combmut);
            if (ATL_FindFirstUnsetBitBV(pp->combDoneBV, 1) == -1)
            {
               tp->rank = rank;                       /* put my thr info back */
               tp->P = P;                             /* to global values */
               ATL_SetBitBV(pp->combDoneBV, vrank);   /* I'm done */
               ATL_UnsetBitBV(pp->combReadyBV, vrank);/* so not ready */
               ATL_mutex_unlock(pp->combmut);
               return;
            }
         }                             /* end else I'm 0 */
      }                                /* end work still to be done */
      ATL_mutex_unlock(pp->combmut);
   }     
   while(1);   /* end combine loop */
}

/*
 * Do a task using thread pool pp; RETURNS: virtual rank (0...nworkers-1)
 */
int ATL_tpool_dojob
(
   ATL_tpool_t *pp,    /* thread pool to use */
   const int rank,     /* actual rank, my tp found at pp->threads+rank */
   const int CFWTHR    /* 1: worker thr called, 0: ATL_threadpool loop called */
)
{
   const int nworkers=pp->nworkers, nthr = pp->nthr, SUBSET=(nworkers != nthr);
   int vrank, iwrk;
   void *tpmut = pp->tpmut;
   ATL_thread_t *tp = pp->threads+rank;
/*
 * Enroll thread in working pool
 */
   if (CFWTHR)
      ATL_mutex_lock(tpmut);
/*
 * On PHI, we must use our actual ranks when we are exploiting contexts
 */
   #if defined(ATL_PHI_SORTED)
   if (nworkers == nthr || nworkers == (nthr>>1) || 
       nworkers == (nthr>>2)+(nthr>>1))
      vrank = rank;
   else
   #endif
   vrank = pp->wcnt;
   if (vrank >= nworkers)            /* If I don't participate in this job */
   {
      ATL_mutex_unlock(tpmut);       /* release mutex and */
      return(vrank);                 /* get out of here now */
   }
   iwrk = pp->wcnt++;        /* inc count of participating workers */
   ATL_mutex_unlock(tpmut);  /* let other threads enroll in job */
/*
 * If this was called from threadloop, I may need to wake other workers
 * Only if I'm the first worker to get here (!iwrk) & called from tl (!CFWTHR)
 */
   if (!(CFWTHR+iwrk) && ATL_TPF_ZEROWAKES(pp))
   {
   #ifdef ATL_PHI_SORTED
      if (ATL_TPF_MICSORTED(pp))
      {
         int k, nw=nworkers;
         const int p4 = (pp->nthr >> 2);

         if (nworkers < p4)
            for (k=1; k < p4; k++)
               ATL_cond_signal(pp->wcond);
         else
         {
            ATL_cond_bcast(pp->wcond);
            nw -= p4;
            if (nw < p4)
               while (nw--)
                  ATL_cond_signal(pp->wcond2);
            else
            {
               ATL_cond_bcast(pp->wcond2);
               nw -= p4;
               if (nw < p4)
                  while (nw--)
                     ATL_cond_signal(pp->wcond3);
               else
               {
                  ATL_cond_bcast(pp->wcond3);
                  nw -= p4;
                  if (nw <  p4)
                     while (nw--)
                        ATL_cond_signal(pp->wcond4);
                  else
                     ATL_cond_bcast(pp->wcond4);
               }
            }
         }
      }
      else
   #endif
      if (nthr == nworkers)
         ATL_cond_bcast(pp->wcond);
      else
      {
         int k;
         for (k=1; k < nworkers; k++)
            ATL_cond_signal(pp->wcond);
      }
   }
/*
 * Now, do work based on PD; change thread info to use virtual rank info
 */
   tp->P = nworkers;             /* make jobf think only nworkers thrs */
   tp->rank = vrank;             /* and use virtual rank */
   pp->jobf(pp, rank, vrank);    /* do the required task */
/*
 * If the calling routine has requested it, perform dynamic combine
 */
   if (pp->combf)
   {
/*
 *    Do combine using virtial ranks stored in pp->icomm
 */
      if (ATL_TPF_DYNCOMB(pp))
         ATL_dyncomb(pp, rank, vrank);
      else
         ATL_leftcomb(pp, rank, vrank);
   }     /* end combine if */
/*
 * Return thread values to their global values
 */
   tp->rank = rank;
   tp->P = nthr;

   return(vrank);
}

#ifdef ATL_POLLTPOOL
/*
 * In this version of threadpool, worker threads restart master process once
 * job is done, but threads may still be awake and polling for work.  Since
 * the master process is not bound to a thread, this may cause contention
 * with the serial portion of the algorithm (if any), and so cause a slowdown.
 * However, it might be the right thing to do on systems where condition
 * variables are very expensive, or if the number of cores is very large,
 * or if polling does not interfere with the serial process for some reason.
 */
void *ATL_threadpool(void *vp)
{
   ATL_thread_t *tp=vp;
   ATL_tpool_t *pp = ATL_TP_PTR;
   const unsigned int rank=tp->rank;
   #ifdef ATL_PHI_SORTED
      const unsigned int p4 = pp->nthr>>2;
   #endif
   void *tpmut = pp->tpmut;
@skip   unsigned int jobID=0;  /* starts at 1, so never a match */

   #ifdef DEBUG
      fprintf(stderr, "%d: WORKER START P=%d\n", rank, pp->nthr);
   #endif
/*
 * Infinite loop over tasks.  Loop has 3 phases:
 * (1) Do work if commanded, including optional combine
 * (2) Poll for a period of time bounded by ATL_POLLTIME.  During this
 *     period the worker actively pools for work.  If a new job is requested
 *     for time expires, worker gets started on next job w/o th everhead
 *     of going to sleep & waking up.  
 *     If ATL_POLLTIME <= 0, this phase is skipped.
 * (3) Sleep until master wakes up for new work
 * Have lock at top of loop.
 */
   ATL_mutex_lock(tpmut);  /* force wait until pp set up */
   while(1)
   {
      unsigned int vrank;
/*
 *    If I've got work to do
 */
      if (pp->jobf)
      {
         #ifdef DEBUG
            fprintf(stderr, "%d: start job, nwrk=%d\n", rank, pp->nworkers);
         #endif
         vrank = ATL_tpool_dojob(pp, rank, 0);
         #ifdef DEBUG
            fprintf(stderr, "%d,%d: done job\n", rank, vrank);
         #endif
         ATL_mutex_lock(tpmut); 
         #ifdef DEBUG
            fprintf(stderr, "%d,%d: got lock done job\n", rank, vrank);
         #endif
/*
 *       If I'm the last worker, signal master process we're done
 */
         if (vrank < pp->nworkers)
         {
            if (++pp->nwdone == pp->nworkers)
            {
               #ifdef DEBUG
                  fprintf(stderr, "%d,%d: SIGNAL MASTER\n", rank, vrank);
               #endif
               pp->NOWORK = pp->WORKDONE = 1;
               ATL_cond_signal(pp->mcond);
            }
         }
      }
/*
 *    If no job, scope flag to see if I should sleep, or quit
 */
      else
      {
         if (ATL_TPF_DIE(pp))
         {
            ATL_mutex_unlock(tpmut);
            return(NULL);
         }
      }
/*
 *    If we've finished so quickly that the job is still waiting on a
 *    mandatory worker, go back to job and pretend to be a new worker,
 *    as long as we are allowed to work on problem
 */
      if (pp->wcnt < pp->nworkers && vrank < pp->nworkers)
         continue;
/*
 *    Sleep for next job; nsleep only examined during startup
 */
      pp->nsleep++;  /* don't care if this rolls over */
/*
 *    Poll until time elapses or a new job is available.
 */
      if (ATL_POLLTIME > 0.0 && pp->wcnt < pp->nworkers)
      {
         double t0, t1=0.0;
         t0 = ATL_walltime();
         #ifdef DEBUG
            fprintf(stderr, "%d,%d: START POLL\n", rank, vrank);
         #endif
         while ((pp->wcnt >= pp->nworkers && t1 < ATL_POLLTIME))
         {
            ATL_mutex_unlock(tpmut);
            while ((pp->wcnt >= pp->nworkers && t1 < ATL_POLLTIME))
            {
               ATL_thread_yield();
               t1 = ATL_walltime() - t0;
            }
            ATL_mutex_lock(tpmut);
         }
         #ifdef DEBUG
            fprintf(stderr, "%d,%d: DONE  POLL %e\n", rank, vrank, t1);
         #endif
      }
      while (pp->WORKDONE || pp->wcnt >= pp->nworkers)
      {
         #ifdef DEBUG
            fprintf(stderr, "%d,%d: go to sleep\n", rank, vrank);
         #endif
         #ifndef ATL_PHI_SORTED
            ATL_cond_wait(pp->wcond, tpmut);
         #else
            if (rank < p4)
               ATL_cond_wait(pp->wcond, tpmut);
            else if (rank < p4+p4)
               ATL_cond_wait(pp->wcond2, tpmut);
            else if (rank < p4+p4+p4)
               ATL_cond_wait(pp->wcond3, tpmut);
            else
               ATL_cond_wait(pp->wcond4, tpmut);
         #endif
      }  /* end sleep loop */
   }     /* end loop over tasks */
/*
 * When a thread exits, decrement thread count
 */

   ATL_mutex_lock(tpmut);
   pp->nthr--;
   ATL_mutex_unlock(tpmut);
   #ifdef DEBUG
      fprintf(stderr, "%d: WORKER DONE\n", rank);
   #endif
   return(NULL);
}
#else
/*
 * In this version of threadpool, all worker threads go to sleep before
 * control is returned to the master process
 */
void *ATL_threadpool(void *vp)
{
   ATL_thread_t *tp=vp;
   ATL_tpool_t *pp = ATL_TP_PTR;
   const unsigned int rank=tp->rank;
   #ifdef ATL_PHI_SORTED
      const unsigned int p4 = pp->nthr>>2;
   #endif
   void *tpmut = pp->tpmut;

   #ifdef DEBUG
      fprintf(stderr, "%d: WORKER START POLL=%d, P=%d\n", rank, DOPOLL, P);
   #endif
/*
 * Infinite loop over tasks.  Loop has 2 phases:
 * (1) Do work if commanded, including optional combine
 * (2) Wait for orders - can wait by polling or condition variable
 * Have lock at top of loop.
 */
   ATL_mutex_lock(tpmut);  /* force wait until pp set up */
   while(1)
   {
      unsigned int vrank;
/*
 *    If I've got work to do
 */
      if (pp->jobf)
      {
         vrank = ATL_tpool_dojob(pp, rank, 0);
         ATL_mutex_lock(tpmut);
/*
 *       Threads go to sleep or poll for work after each job
 *       If I'm the last guy, signal master process we're done
 */
         if (vrank < pp->nworkers)
         {
            if (++pp->nwdone == pp->nworkers)
            {
               pp->WORKDONE = 1;
               ATL_cond_signal(pp->mcond);
            }
         }
      }
/*
 *    If no work, scope flag to see if I should sleep, or quit
 */
      else
      {
         if (ATL_TPF_DIE(pp))
         {
            ATL_mutex_unlock(tpmut);
            return(NULL);
         }
      }
/*
 *    If the job is still waiting on workers, go back up and pretend to be
 *    a new worker
 */
      if (pp->wcnt < pp->nworkers)
         continue;
/*
 *    Sleep for next job; nsleep only examined during startup
 */
      pp->nsleep++;  /* don't care if this rolls over */
      do
      {
         #ifdef DEBUG
            fprintf(stderr, "%d: go to sleep\n", rank);
         #endif
         #ifndef ATL_PHI_SORTED
            ATL_cond_wait(pp->wcond, tpmut);
         #else
            if (rank < p4)
               ATL_cond_wait(pp->wcond, tpmut);
            else if (rank < p4+p4)
               ATL_cond_wait(pp->wcond2, tpmut);
            else if (rank < p4+p4+p4)
               ATL_cond_wait(pp->wcond3, tpmut);
            else
               ATL_cond_wait(pp->wcond4, tpmut);
         #endif
      }
      while (pp->WORKDONE || pp->wcnt >= pp->nworkers);
   }  /* end loop over tasks */
/*
 * When a thread exits, decrement thread count
 */

   ATL_mutex_lock(tpmut);
   pp->nthr--;
   ATL_mutex_unlock(tpmut);
   #ifdef DEBUG
      fprintf(stderr, "%d: WORKER DONE\n", rank);
   #endif
   return(NULL);
}
#endif
#endif
@ROUT ATL_threadpool_misc
#define ATL_TP_DECL 1
#include "atlas_threads.h"
#include "atlas_misc.h"
#include "atlas_bitvec.h"

/*
 * This function is called only by one thread, and is unsafe if called
 * by more than one.  It is protected ty ATL_IsFirstThreadedCall, which
 * should return true for only one thread
 */
void ATL_InitThreadPoolStartup(int P, void *pd, void *extra)
{
   ATL_tpool_t *pp;

   ATL_assert(!ATL_TP_PTR);


   ATL_TP_PTR = pp = ATL_NewThreadPool(ATL_NTHREADS, 1, pd);
   ATL_mutex_lock(pp->tpmut);
   pp->nworkers = P;
   pp->PD = pd;
   pp->extra = extra;
   #ifdef ATL_PHI_SORTED
      ATL_TPF_SET_MICSORTED(pp);
   #endif

   ATL_thread_start(pp->threads, 0, 0, ATL_threadpool_launch, pp->threads);
/*
 * We leave function still holding the lock, so that master can go to sleep
 * awaiting job completion before spawned threads are able to complete
 * and thus mess up timing
 */
}

/*
 * Main way parallel jobs are spawned to workpool
 */
void ATL_goParallel
(
   const unsigned int P0,  /* # of worker threads to use on job */
   void *DoWork,           /* ptr to function doing the job */
   void *DoComb,           /* ptr to func doing combine; NULL: don't combine */
   void *PD,               /* ptr to problem definition (work queue, etc) */
   void *extra             /* extra ptr that is passed to DoWork/DoComb */
)
{
   ATL_tpool_t *pp;
   int n, *ip;
   int JUST_STARTED=0;
   unsigned int P;
/*
 * Don't wakeup/create threads if being called for serial execution: just
 * do work on master process to save overhead & ease debugging
 */
   if (P0 < 2)
   {
      if (!ATL_TP_PTR)
         ATL_TP1_PTR = ATL_NewThreadPool(1, 0, NULL);
      pp = ATL_TP1_PTR;
      pp->nworkers = 1;
      pp->PD = PD;
      pp->extra = extra;
      pp->jobf = DoWork;
      pp->combf = DoComb;
      pp->jobf(pp, 0, 0);
      return;
   }
/*
 * If thread pool not currently active, must spawn it.
 */
   if (!ATL_TP_PTR)
   {
      if (ATL_IsFirstThreadedCall())
      {
         JUST_STARTED = 1;
         ATL_InitThreadPoolStartup(Mmin(P0,ATL_NTHREADS), PD, extra);
      }
   }
   pp = ATL_TP_PTR;
   P = Mmin(P0, pp->nthr);

   if (!JUST_STARTED)
   {
      ATL_mutex_lock(pp->tpmut);
      pp->jobID++;
      pp->PD = PD;
      pp->extra = extra;
      pp->jobf = DoWork;
      pp->combf = DoComb;
      pp->nworkers = P;
      pp->nwdone = pp->wcnt = 0;
      pp->WORKDONE = pp->NOWORK = 0;
      #ifdef ATL_TP_FORCEBCAST
         #if (ATL_TP_FORCEBCAST)
            ATL_TPF_UNSET_ZEROWAKES(pp);
         #else
            ATL_TPF_SET_ZEROWAKES(pp);
         #endif
      #else
         if (P == pp->nthr)
            ATL_TPF_UNSET_ZEROWAKES(pp);
         else
            ATL_TPF_SET_ZEROWAKES(pp);
         #endif
/*
 *    Wake threads up to do the new job
 */
      if (ATL_TPF_ZEROWAKES(pp))
         ATL_cond_signal(pp->wcond);
      else
      #ifdef ATL_PHI_SORTED
      {
         ATL_cond_bcast(pp->wcond);
         if (P > ATL_NTHREADS/4)
            ATL_cond_bcast(pp->wcond2);
         if (P > ATL_NTHREADS/2)
            ATL_cond_bcast(pp->wcond3);
         if (P > (3*ATL_NTHREADS)/4)
            ATL_cond_bcast(pp->wcond4);

      }
      #else
         ATL_cond_bcast(pp->wcond);
      #endif
   }
   else /* thread pool just started up, I have tpmut */
   {
      pp->jobf = DoWork;
      pp->combf = DoComb;
   }
/*
 * If I'm doing a combine, setup combine bitvectors based on nworkers
 */
   if (DoComb)
   {
      ATL_UnsetAllBitsBV(pp->combReadyBV);
      if (P == pp->nthr)
         ATL_UnsetAllBitsBV(pp->combDoneBV);
      else if (P >= pp->nthr - P) /* participating threads outnumber idle */
      {
         int i;
         ATL_UnsetAllBitsBV(pp->combDoneBV);
         for (i=P; i < pp->nthr; i++)
            ATL_SetBitBV(pp->combDoneBV, i);
      }
      else /* idle threads outnumber participating */
      {
         int i;
         ATL_SetAllBitsBV(pp->combDoneBV);
         for (i=0; i < P; i++)
            ATL_UnsetBitBV(pp->combDoneBV, i);
      }
   }
/*
 * Threads have started, so this routine awaits completion of task
 */
   do
   {
      #ifdef DEBUG
         fprintf(stderr, "master sleeps WD=%d, P=%d, comb=%p\n",
                 pp->WORKDONE, pp->nworkers, pp->combf);
      #endif
      ATL_cond_wait(pp->mcond, pp->tpmut);
      #ifdef DEBUG
         fprintf(stderr, "master awake  WD=%d\n", pp->WORKDONE);
      #endif
   }
   while (!pp->WORKDONE);
   #ifdef DEBUG
      fprintf(stderr, "MASTER AWAKE FINAL WD=%d\n", pp->WORKDONE);
   #endif
   pp->nworkers = 0;     /* make sure spurious wakeup will go back to sleep */
/*
 * If I'm using a thread pool where all threads are known to be asleep before
 * returning, then on the first launch we must make sure they have all checked
 * in before returning (in subsequent uses of pool, master will not be
 * awakened until all workers have gone to sleep)
 */
   #ifndef ATL_POLLTPOOL
      if (JUST_STARTED && pp->nsleep < pp->nthr)
      {
         do
         {
            ATL_mutex_unlock(pp->tpmut);
            while(pp->nsleep < pp->nthr)
               ATL_thread_yield();
            ATL_mutex_lock(pp->tpmut);
         }
         while(pp->nsleep < pp->nthr);
      }
   #endif
   ATL_mutex_unlock(pp->tpmut);
}

ATL_tpool_t *ATL_NewThreadPool
(
   const int P,  /* length of thread array to allocate, 0 for don't */
   int ICOM,     /* 0: don't allocate icomm array */
   void *vp      /* what to initialize thread arrays vp to */
)
{
   ATL_tpool_t *pp;
   pp = calloc(1, sizeof(ATL_tpool_t));
   ATL_assert(pp);
   pp->jobID = 1;
   if (P)
   {
      int i;
      pp->nthr = P;
      pp->threads = malloc(P*sizeof(ATL_thread_t));
      ATL_assert(pp->threads);
      if (!vp)
         vp = pp;
      for (i=0; i < P; i++)
      {
         pp->threads[i].rank = i;
         pp->threads[i].P = P;
         pp->threads[i].affID = -1;
         pp->threads[i].vp = vp;
      }
   }
   if (ICOM)
   {
      pp->icomm = calloc(P, sizeof(int));
      ATL_assert(pp->threads);
   }
   pp->combmut = ATL_mutex_init();
   pp->tpmut = ATL_mutex_init();
   pp->mcond = ATL_cond_init();
   pp->wcond = ATL_cond_init();
   #ifdef ATL_PHI_SORTED
      pp->wcond2 = ATL_cond_init();
      pp->wcond3 = ATL_cond_init();
      pp->wcond4 = ATL_cond_init();
   #endif
   pp->combReadyBV = ATL_NewBV(P);
   pp->combDoneBV = ATL_NewBV(P);
   return(pp);
}

void ATL_FreeThreadPool(ATL_tpool_t *pp)
{
   if (!pp)
      return;
   if (pp->icomm)
      free((void*)pp->icomm);
   if (pp->threads)
      free(pp->threads);
   if (pp->combmut)
      ATL_mutex_free(pp->combmut);
   if (pp->tpmut)
      ATL_mutex_free(pp->tpmut);
   if (pp->mcond)
      ATL_cond_free(pp->mcond);
   if (pp->wcond)
      ATL_cond_free(pp->wcond);
   if (pp->combReadyBV)
      ATL_FreeBV(pp->combReadyBV);
   if (pp->combDoneBV)
      ATL_FreeBV(pp->combDoneBV);
   free(pp);
}
/*
 * NOTE:
 *    void *ATL_threadpool_launch(void *vp)
 * is defined in ATL_goparallel.c, so it can use  ATL_setmyaffinity
 */
